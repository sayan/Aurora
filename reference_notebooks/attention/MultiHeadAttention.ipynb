{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "\n",
    "In this notebook, we will implement the multihead attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        if not d_model%num_heads == 0:\n",
    "            raise ValueError(\"D_model should be a multiple of num_heads\")\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        print(d_model/num_heads)\n",
    "        self.head_embed_dim = int(d_model/num_heads)\n",
    "        print(self.head_embed_dim)\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_out = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Dimension of x is: {x.shape}\") ## shape of x should be (batch_size, seq_len, d_model)\\\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        q_proj = self.W_q(x) ## shape (batch_size, seq_len, d_model)\n",
    "        k_proj = self.W_k(x) ## shape (batch_size, seq_len, d_model)\n",
    "        v_proj = self.W_v(x) ## shape (batch_size, seq_len, d_model)\n",
    "        #out_proj = self.W_out(x) ## shape (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Muliheads should be of shape (num_heads,batch_size, seq_len, d_model/num_heads)        \n",
    "        multi_q_proj = q_proj.view(self.num_heads, batch_size, seq_len, self.head_embed_dim)\n",
    "        multi_k_proj = k_proj.view(self.num_heads, batch_size, seq_len, self.head_embed_dim)\n",
    "        multi_v_proj = v_proj.view(self.num_heads, batch_size, seq_len, self.head_embed_dim)\n",
    "\n",
    "        logits = multi_q_proj @ multi_k_proj.transpose(-2,-1) # (num_heads,batch_size, seq_len, seq_len)\n",
    "        logits_scaled = logits/math.sqrt(float(d_model/self.num_heads)) # (num_heads,batch_size, seq_len, seq_len)\n",
    "\n",
    "        head_attention_probs = torch.softmax(logits_scaled, dim=-1) # (num_heads,batch_size, seq_len, seq_len)\n",
    "        head_attention_scores =     head_attention_probs @   multi_v_proj ## (num_heads,batch_size, seq_len, d_model/num_heads)\n",
    "        attention_score_permuted =   head_attention_scores.permute(1,2,0,3)   ## (batch_size, seq_len, num_heads,d_model/num_heads)\n",
    "        attention_scores = attention_score_permuted.reshape(batch_size, seq_len, d_model) \n",
    "        out = self.W_out(attention_scores)\n",
    "        print(f\"Output DImension: {out.shape}\")\n",
    "        return(out)\n",
    "\n",
    "\n",
    "multiHeadAttention = MultiHeadAttention(d_model=12, num_heads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((12,5,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x is: torch.Size([12, 5, 12])\n",
      "Output DImension: torch.Size([12, 5, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0664,  0.1414, -0.0788,  0.2497, -0.2256,  0.0281,  0.0019,\n",
       "          -0.1521,  0.0098, -0.0464,  0.0029,  0.0747],\n",
       "         [-0.0731,  0.1391, -0.0748,  0.2339, -0.2150,  0.0314,  0.0183,\n",
       "          -0.1485,  0.0056, -0.0447,  0.0154,  0.0677],\n",
       "         [-0.0717,  0.1358, -0.0773,  0.2362, -0.2151,  0.0306,  0.0108,\n",
       "          -0.1434,  0.0072, -0.0412,  0.0092,  0.0712],\n",
       "         [-0.0696,  0.1381, -0.0798,  0.2450, -0.2192,  0.0336,  0.0134,\n",
       "          -0.1523,  0.0096, -0.0507,  0.0064,  0.0714],\n",
       "         [-0.0679,  0.1427, -0.0710,  0.2347, -0.2217,  0.0239,  0.0059,\n",
       "          -0.1497,  0.0040, -0.0364,  0.0155,  0.0702]],\n",
       "\n",
       "        [[-0.1322,  0.0795, -0.0826,  0.2003, -0.1383,  0.1013, -0.0308,\n",
       "          -0.0939,  0.0773, -0.1021, -0.0484,  0.0929],\n",
       "         [-0.1484,  0.0716, -0.0866,  0.1858, -0.1265,  0.1116, -0.0233,\n",
       "          -0.0700,  0.0860, -0.1117, -0.0543,  0.0981],\n",
       "         [-0.1461,  0.0758, -0.0852,  0.1784, -0.1112,  0.1174, -0.0191,\n",
       "          -0.0740,  0.0728, -0.1109, -0.0550,  0.0952],\n",
       "         [-0.1380,  0.0781, -0.0841,  0.1971, -0.1360,  0.0980, -0.0209,\n",
       "          -0.0824,  0.0745, -0.1015, -0.0485,  0.0943],\n",
       "         [-0.1569,  0.0678, -0.0887,  0.1818, -0.1226,  0.1170, -0.0210,\n",
       "          -0.0615,  0.0924, -0.1178, -0.0568,  0.0999]],\n",
       "\n",
       "        [[-0.1403,  0.0693, -0.1876,  0.1935, -0.1026,  0.1619, -0.0868,\n",
       "           0.0122,  0.0633, -0.1183, -0.1322,  0.1150],\n",
       "         [-0.1278,  0.0762, -0.1687,  0.2023, -0.1155,  0.1420, -0.0932,\n",
       "          -0.0117,  0.0678, -0.1165, -0.1268,  0.1084],\n",
       "         [-0.1294,  0.0708, -0.1731,  0.1997, -0.1054,  0.1437, -0.0984,\n",
       "          -0.0015,  0.0657, -0.1118, -0.1363,  0.1124],\n",
       "         [-0.1454,  0.0740, -0.1862,  0.1995, -0.1058,  0.1598, -0.0864,\n",
       "           0.0058,  0.0687, -0.1243, -0.1303,  0.1116],\n",
       "         [-0.1335,  0.0748, -0.1715,  0.2005, -0.1136,  0.1440, -0.0887,\n",
       "          -0.0048,  0.0688, -0.1191, -0.1269,  0.1090]],\n",
       "\n",
       "        [[-0.0772,  0.1448, -0.1035,  0.2188, -0.1933,  0.0554, -0.0708,\n",
       "          -0.0955,  0.0419, -0.0759, -0.0452,  0.0742],\n",
       "         [-0.0925,  0.1452, -0.0943,  0.1998, -0.1735,  0.0629, -0.0696,\n",
       "          -0.0848,  0.0404, -0.0802, -0.0469,  0.0768],\n",
       "         [-0.0833,  0.1453, -0.0954,  0.2112, -0.1858,  0.0547, -0.0702,\n",
       "          -0.0955,  0.0406, -0.0752, -0.0440,  0.0765],\n",
       "         [-0.0801,  0.1394, -0.1101,  0.2132, -0.1854,  0.0644, -0.0719,\n",
       "          -0.0871,  0.0414, -0.0765, -0.0482,  0.0754],\n",
       "         [-0.0883,  0.1413, -0.0900,  0.2008, -0.1811,  0.0582, -0.0584,\n",
       "          -0.0893,  0.0418, -0.0802, -0.0392,  0.0741]],\n",
       "\n",
       "        [[-0.1162,  0.0723, -0.0107,  0.0948, -0.0919,  0.1261, -0.0791,\n",
       "          -0.0267,  0.0787, -0.1207, -0.0820,  0.0911],\n",
       "         [-0.1201,  0.0695, -0.0239,  0.0964, -0.0737,  0.1315, -0.0983,\n",
       "          -0.0192,  0.0754, -0.1140, -0.0971,  0.0949],\n",
       "         [-0.1417,  0.0615, -0.0171,  0.0834, -0.0523,  0.1465, -0.0755,\n",
       "          -0.0078,  0.0811, -0.1337, -0.1023,  0.0983],\n",
       "         [-0.1195,  0.0765, -0.0066,  0.0977, -0.0936,  0.1282, -0.0831,\n",
       "          -0.0299,  0.0848, -0.1285, -0.0849,  0.0912],\n",
       "         [-0.1302,  0.0502, -0.0339,  0.0830, -0.0469,  0.1476, -0.0865,\n",
       "          -0.0019,  0.0734, -0.1153, -0.1059,  0.0996]],\n",
       "\n",
       "        [[-0.2194,  0.0207, -0.1833,  0.1580, -0.0800,  0.1868, -0.0275,\n",
       "          -0.0203,  0.1096, -0.1400, -0.0676,  0.1169],\n",
       "         [-0.2080,  0.0230, -0.1747,  0.1640, -0.0891,  0.1812, -0.0352,\n",
       "          -0.0402,  0.1093, -0.1351, -0.0634,  0.1152],\n",
       "         [-0.2111,  0.0105, -0.1838,  0.1661, -0.0786,  0.1874, -0.0260,\n",
       "          -0.0376,  0.1097, -0.1398, -0.0674,  0.1155],\n",
       "         [-0.2203,  0.0230, -0.1870,  0.1512, -0.0783,  0.1968, -0.0344,\n",
       "          -0.0095,  0.1096, -0.1401, -0.0704,  0.1183],\n",
       "         [-0.1996,  0.0163, -0.1810,  0.1608, -0.0910,  0.1915, -0.0383,\n",
       "          -0.0360,  0.1094, -0.1326, -0.0651,  0.1159]],\n",
       "\n",
       "        [[-0.0594,  0.0807, -0.0389,  0.2155, -0.1704, -0.0009, -0.0421,\n",
       "          -0.0275,  0.0162, -0.0621, -0.1062,  0.1241],\n",
       "         [-0.0620,  0.0822, -0.0382,  0.2076, -0.1688, -0.0027, -0.0325,\n",
       "          -0.0212,  0.0123, -0.0603, -0.0990,  0.1213],\n",
       "         [-0.0610,  0.0838, -0.0412,  0.2211, -0.1770,  0.0104, -0.0442,\n",
       "          -0.0414,  0.0211, -0.0674, -0.0994,  0.1205],\n",
       "         [-0.0593,  0.0799, -0.0413,  0.2176, -0.1699,  0.0020, -0.0430,\n",
       "          -0.0299,  0.0169, -0.0630, -0.1071,  0.1243],\n",
       "         [-0.0650,  0.0842, -0.0390,  0.2142, -0.1729,  0.0052, -0.0395,\n",
       "          -0.0302,  0.0195, -0.0669, -0.0999,  0.1217]],\n",
       "\n",
       "        [[-0.1436,  0.0834, -0.0327,  0.1175, -0.1020,  0.1337, -0.0465,\n",
       "           0.0208,  0.0814, -0.1374, -0.0818,  0.0861],\n",
       "         [-0.1543,  0.0866, -0.0251,  0.1007, -0.1014,  0.1254, -0.0449,\n",
       "           0.0384,  0.0914, -0.1477, -0.0832,  0.0925],\n",
       "         [-0.1491,  0.0819, -0.0423,  0.1233, -0.0969,  0.1396, -0.0267,\n",
       "           0.0191,  0.0783, -0.1470, -0.0791,  0.0821],\n",
       "         [-0.1431,  0.0902, -0.0291,  0.1227, -0.1102,  0.1245, -0.0360,\n",
       "           0.0194,  0.0776, -0.1414, -0.0776,  0.0839],\n",
       "         [-0.1489,  0.0904, -0.0208,  0.1107, -0.1097,  0.1230, -0.0437,\n",
       "           0.0252,  0.0865, -0.1455, -0.0788,  0.0900]],\n",
       "\n",
       "        [[-0.1560,  0.1085, -0.2346,  0.2673, -0.1278,  0.1920, -0.0131,\n",
       "          -0.0074,  0.0742, -0.2188, -0.1525,  0.1193],\n",
       "         [-0.1403,  0.1151, -0.2289,  0.2715, -0.1407,  0.1759, -0.0343,\n",
       "          -0.0152,  0.0640, -0.1965, -0.1498,  0.1213],\n",
       "         [-0.1476,  0.0950, -0.2345,  0.2837, -0.1451,  0.1847, -0.0148,\n",
       "          -0.0235,  0.0835, -0.2202, -0.1549,  0.1324],\n",
       "         [-0.1565,  0.1126, -0.2339,  0.2644, -0.1282,  0.1930, -0.0216,\n",
       "          -0.0057,  0.0738, -0.2153, -0.1516,  0.1189],\n",
       "         [-0.1442,  0.1129, -0.2250,  0.2760, -0.1450,  0.1770, -0.0264,\n",
       "          -0.0252,  0.0715, -0.2059, -0.1451,  0.1199]],\n",
       "\n",
       "        [[-0.1349,  0.1044, -0.0331,  0.2048, -0.1768,  0.1342, -0.1375,\n",
       "          -0.1998,  0.1058, -0.1016, -0.0471,  0.1273],\n",
       "         [-0.1470,  0.1221, -0.0141,  0.1897, -0.1577,  0.1306, -0.1356,\n",
       "          -0.2002,  0.0945, -0.1079, -0.0500,  0.1262],\n",
       "         [-0.1280,  0.1362, -0.0081,  0.2125, -0.1969,  0.1288, -0.1487,\n",
       "          -0.2288,  0.1124, -0.1221, -0.0417,  0.1157],\n",
       "         [-0.1367,  0.1118, -0.0315,  0.2056, -0.1707,  0.1365, -0.1450,\n",
       "          -0.2076,  0.1027, -0.1020, -0.0506,  0.1282],\n",
       "         [-0.1495,  0.1293, -0.0010,  0.1916, -0.1728,  0.1137, -0.1258,\n",
       "          -0.1964,  0.0987, -0.1163, -0.0438,  0.1239]],\n",
       "\n",
       "        [[-0.1325,  0.0273, -0.1491,  0.1633, -0.0555,  0.1363,  0.0695,\n",
       "           0.1224,  0.0402, -0.1709, -0.1655,  0.1324],\n",
       "         [-0.1267,  0.0137, -0.1553,  0.1585, -0.0401,  0.1483,  0.0574,\n",
       "           0.1191,  0.0433, -0.1679, -0.1737,  0.1312],\n",
       "         [-0.1245,  0.0224, -0.1515,  0.1672, -0.0429,  0.1510,  0.0525,\n",
       "           0.1032,  0.0380, -0.1665, -0.1695,  0.1272],\n",
       "         [-0.1158,  0.0299, -0.1448,  0.1721, -0.0555,  0.1380,  0.0468,\n",
       "           0.0925,  0.0293, -0.1540, -0.1609,  0.1269],\n",
       "         [-0.1243,  0.0150, -0.1546,  0.1621, -0.0411,  0.1484,  0.0595,\n",
       "           0.1118,  0.0400, -0.1674, -0.1715,  0.1300]],\n",
       "\n",
       "        [[-0.2574, -0.0118, -0.2083,  0.2232, -0.1041,  0.2317,  0.0255,\n",
       "          -0.0316,  0.1441, -0.1749, -0.0698,  0.1295],\n",
       "         [-0.2455, -0.0327, -0.2051,  0.2313, -0.1105,  0.2225,  0.0349,\n",
       "          -0.0359,  0.1426, -0.1628, -0.0706,  0.1353],\n",
       "         [-0.2358, -0.0442, -0.2125,  0.2333, -0.1140,  0.2292,  0.0372,\n",
       "          -0.0436,  0.1461, -0.1614, -0.0646,  0.1306],\n",
       "         [-0.2714, -0.0061, -0.2071,  0.2136, -0.1024,  0.2358,  0.0197,\n",
       "          -0.0215,  0.1524, -0.1824, -0.0696,  0.1324],\n",
       "         [-0.2524, -0.0265, -0.2049,  0.2285, -0.1141,  0.2221,  0.0336,\n",
       "          -0.0352,  0.1475, -0.1681, -0.0671,  0.1363]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiHeadAttention(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by num_heads.\")\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # dimension per head\n",
    "\n",
    "        # Linear layers for Q, K, V (with bias for illustration)\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        # Final linear layer to recombine heads\n",
    "        self.W_out = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        # Optional dropout on the attention probabilities\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x:    (batch_size, seq_len, d_model)\n",
    "        mask: (batch_size, 1, 1, seq_len) or (batch_size, 1, seq_len, seq_len)\n",
    "              depending on need. Typically 1/True for keep, 0/False for masked.\n",
    "        \"\"\"\n",
    "        B, L, _ = x.size()  # (Batch, Seq_len, d_model)\n",
    "\n",
    "        # 1) Linear projections: Q, K, V\n",
    "        Q = self.W_q(x)  # (B, L, d_model)\n",
    "        K = self.W_k(x)  # (B, L, d_model)\n",
    "        V = self.W_v(x)  # (B, L, d_model)\n",
    "\n",
    "        # 2) Reshape to split heads: (B, L, num_heads, d_k)\n",
    "        Q = Q.view(B, L, self.num_heads, self.d_k)\n",
    "        K = K.view(B, L, self.num_heads, self.d_k)\n",
    "        V = V.view(B, L, self.num_heads, self.d_k)\n",
    "\n",
    "        # 3) Permute to get (B, num_heads, L, d_k)\n",
    "        Q = Q.permute(0, 2, 1, 3)\n",
    "        K = K.permute(0, 2, 1, 3)\n",
    "        V = V.permute(0, 2, 1, 3)\n",
    "\n",
    "        # 4) Scaled dot product attention\n",
    "        #    Q @ K^T => (B, H, L, d_k) x (B, H, d_k, L) = (B, H, L, L)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 5) Optional masking\n",
    "        if mask is not None:\n",
    "            # mask == 0 => fill with -inf to exclude\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)  # (B, H, L, L)\n",
    "        attn_probs = self.dropout(attn_probs)        # dropout on the attention map\n",
    "\n",
    "        # 6) Compute final attention outputs\n",
    "        #    (B, H, L, L) x (B, H, L, d_k) = (B, H, L, d_k)\n",
    "        context = torch.matmul(attn_probs, V)\n",
    "\n",
    "        # 7) Permute back to (B, L, H, d_k), then flatten\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()  # (B, L, H, d_k)\n",
    "        context = context.view(B, L, self.d_model)          # (B, L, d_model)\n",
    "\n",
    "        # 8) Final linear to recombine all heads\n",
    "        out = self.W_out(context)                           # (B, L, d_model)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, you *can* technically do without calling `.contiguous()`, especially if nothing else in your code relies on having a strictly contiguous layout. However, **the safest practice** when you perform a `permute()` and then immediately call `.view()` (or `reshape()`) is to use `.contiguous()` first. \n",
    "\n",
    "Here’s why:\n",
    "\n",
    "- **`permute()`** reorders tensor dimensions **without** rearranging them in memory. This often results in a “non-contiguous” tensor (i.e. the data no longer has a simple, row-major memory layout).\n",
    "- **`view()`** (unlike `reshape()`) strictly requires that the underlying tensor storage is contiguous. If it’s not, `view()` will throw an error (unless you use `reshape()`, which will try to handle it but may still internally call `.contiguous()` anyway).\n",
    "\n",
    "By calling `.contiguous()`, you ensure the tensor is laid out in memory in a contiguous block in the new order of dimensions, making the subsequent `view()` safe and predictable. \n",
    "\n",
    "So while it may *sometimes* work without `.contiguous()`, the recommended approach is to do:\n",
    "\n",
    "```python\n",
    "context = context.permute(0, 2, 1, 3).contiguous()\n",
    "context = context.view(B, L, self.d_model)\n",
    "```\n",
    "\n",
    "to avoid any surprises if later operations demand a contiguous layout or if PyTorch changes behavior in future versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddddmultiHeadAttention = MultiHeadAttention(d_model=12, num_heads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiHeadAttention = MultiHeadAttention(d_model=12, num_heads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.4925e-02, -8.2597e-02,  1.5392e-01,  2.9184e-01, -1.6251e-01,\n",
       "          -3.3844e-01,  2.2570e-01,  2.2068e-01, -5.2972e-01,  2.3687e-02,\n",
       "          -3.6595e-02,  2.0736e-01],\n",
       "         [ 8.9961e-03, -3.9324e-03,  5.6516e-02,  2.2119e-01, -1.8616e-01,\n",
       "          -3.8412e-01,  2.3201e-01,  1.5972e-01, -4.4214e-01,  4.6280e-02,\n",
       "          -1.3124e-01,  1.8051e-01],\n",
       "         [ 2.3962e-02, -5.5031e-02,  1.2626e-01,  3.0347e-01, -1.5344e-01,\n",
       "          -3.8098e-01,  2.4902e-01,  2.3129e-01, -5.3142e-01,  3.2017e-02,\n",
       "          -7.9054e-02,  2.1008e-01],\n",
       "         [-3.9417e-03, -3.0084e-02,  9.9333e-02,  2.4949e-01, -1.3996e-01,\n",
       "          -4.1857e-01,  1.7047e-01,  1.7855e-01, -4.5039e-01,  3.2127e-02,\n",
       "          -1.1767e-01,  1.9613e-01],\n",
       "         [ 3.0660e-02, -4.4391e-02,  1.2024e-01,  3.2198e-01, -1.6135e-01,\n",
       "          -4.1303e-01,  1.7937e-01,  2.2037e-01, -4.8638e-01,  3.0240e-02,\n",
       "          -1.1032e-01,  2.1702e-01]],\n",
       "\n",
       "        [[ 1.4514e-01, -1.5398e-01,  1.5875e-01,  2.3907e-01, -1.9391e-01,\n",
       "          -2.6616e-01,  1.4522e-01,  4.8300e-02, -4.6396e-01, -4.9895e-02,\n",
       "          -1.3841e-02,  2.4716e-01],\n",
       "         [ 8.9179e-02, -7.6565e-02,  9.8018e-02,  3.0531e-01, -1.6014e-01,\n",
       "          -3.5387e-01,  1.3576e-01,  9.0601e-02, -4.6826e-01, -4.8566e-02,\n",
       "          -1.2108e-01,  3.0423e-01],\n",
       "         [ 5.3231e-02, -1.0078e-01,  1.1684e-01,  2.9909e-01, -1.9993e-01,\n",
       "          -3.2158e-01,  2.1925e-01,  1.5137e-01, -5.0878e-01, -2.1415e-02,\n",
       "          -8.6021e-02,  2.9467e-01],\n",
       "         [ 4.5341e-02, -7.6389e-02,  1.1303e-01,  3.7175e-01, -1.6797e-01,\n",
       "          -3.7540e-01,  1.8563e-01,  1.7900e-01, -5.3132e-01, -2.2576e-02,\n",
       "          -1.5003e-01,  3.5220e-01],\n",
       "         [ 9.5030e-02, -1.1893e-01,  1.4181e-01,  3.4046e-01, -1.7639e-01,\n",
       "          -3.2660e-01,  1.9135e-01,  1.4223e-01, -5.3181e-01, -4.0729e-02,\n",
       "          -7.7763e-02,  3.0823e-01]],\n",
       "\n",
       "        [[ 1.7732e-02, -1.1969e-01,  1.9855e-01,  3.3337e-01, -2.2741e-01,\n",
       "          -3.3822e-01,  1.9649e-01,  2.9699e-01, -5.1664e-01,  2.2115e-02,\n",
       "          -7.1329e-02,  2.2232e-01],\n",
       "         [-6.2607e-02, -3.8571e-02,  1.3223e-01,  3.2459e-01, -2.0991e-01,\n",
       "          -4.4213e-01,  1.1812e-01,  2.7689e-01, -4.3564e-01,  5.1487e-02,\n",
       "          -2.1827e-01,  2.6520e-01],\n",
       "         [ 7.5647e-03, -1.1835e-01,  2.0664e-01,  3.5894e-01, -2.3354e-01,\n",
       "          -3.5455e-01,  2.0478e-01,  3.1403e-01, -5.2340e-01,  2.4557e-02,\n",
       "          -9.7169e-02,  2.0169e-01],\n",
       "         [-7.5747e-02, -4.0052e-02,  1.3893e-01,  3.0314e-01, -2.2298e-01,\n",
       "          -3.5733e-01,  2.2089e-01,  3.3897e-01, -5.0368e-01,  8.7016e-02,\n",
       "          -1.5092e-01,  2.3521e-01],\n",
       "         [-4.6472e-02, -6.8066e-02,  1.6541e-01,  3.7550e-01, -2.2356e-01,\n",
       "          -4.0104e-01,  1.8911e-01,  3.4732e-01, -5.1324e-01,  4.7600e-02,\n",
       "          -1.6578e-01,  2.7529e-01]],\n",
       "\n",
       "        [[ 2.2224e-02, -1.4192e-01,  2.0116e-01,  1.5068e-01, -1.2545e-01,\n",
       "          -2.5777e-01,  2.2671e-01,  2.2680e-01, -5.4257e-01,  2.4334e-03,\n",
       "           5.2585e-02,  1.7808e-01],\n",
       "         [ 3.1786e-02, -1.2384e-01,  2.0035e-01,  1.9707e-01, -1.3661e-01,\n",
       "          -2.8137e-01,  1.7989e-01,  2.2273e-01, -5.2814e-01,  3.0392e-02,\n",
       "          -1.0339e-02,  1.9509e-01],\n",
       "         [-1.1416e-01, -5.1649e-03,  1.1360e-01,  2.2416e-01, -1.7144e-01,\n",
       "          -3.9516e-01,  8.6599e-02,  2.9347e-01, -4.1872e-01,  9.8062e-02,\n",
       "          -1.9691e-01,  2.8958e-01],\n",
       "         [-1.1953e-01, -1.6094e-02,  1.3676e-01,  2.1875e-01, -1.4376e-01,\n",
       "          -3.8795e-01,  1.0667e-01,  3.2706e-01, -4.5326e-01,  9.6174e-02,\n",
       "          -1.6353e-01,  2.8169e-01],\n",
       "         [ 2.7882e-02, -1.0687e-01,  1.8908e-01,  1.6069e-01, -1.6305e-01,\n",
       "          -2.1693e-01,  1.9111e-01,  2.2754e-01, -5.1651e-01,  3.3836e-02,\n",
       "          -4.6820e-02,  1.9550e-01]],\n",
       "\n",
       "        [[ 1.6822e-02, -9.2530e-02,  1.5716e-01,  2.3185e-01, -1.8818e-01,\n",
       "          -3.8330e-01,  1.3063e-01,  2.1064e-01, -4.3637e-01,  1.9725e-02,\n",
       "          -7.9007e-02,  2.0278e-01],\n",
       "         [ 4.0423e-02, -1.0742e-01,  1.6185e-01,  1.9762e-01, -1.6927e-01,\n",
       "          -3.4044e-01,  1.3642e-01,  1.8389e-01, -4.4743e-01, -8.9696e-03,\n",
       "          -5.0313e-02,  1.8700e-01],\n",
       "         [-4.0564e-02, -4.4409e-02,  1.1161e-01,  2.2528e-01, -1.9732e-01,\n",
       "          -4.4200e-01,  7.7443e-02,  1.9324e-01, -3.7565e-01,  4.3576e-02,\n",
       "          -1.8899e-01,  2.4529e-01],\n",
       "         [ 2.6369e-02, -1.2225e-01,  1.9114e-01,  1.8645e-01, -1.6008e-01,\n",
       "          -2.8471e-01,  2.0980e-01,  2.4453e-01, -5.1347e-01,  7.1511e-03,\n",
       "           4.8983e-03,  1.5684e-01],\n",
       "         [-4.2376e-02, -8.7202e-02,  1.6090e-01,  2.2761e-01, -1.8381e-01,\n",
       "          -3.7176e-01,  2.0319e-01,  2.9867e-01, -4.8926e-01,  3.2737e-02,\n",
       "          -7.0282e-02,  2.2038e-01]],\n",
       "\n",
       "        [[-3.8507e-04, -5.2261e-02,  8.8323e-02,  1.9383e-01, -1.6237e-01,\n",
       "          -3.5282e-01,  9.7710e-02,  2.0304e-01, -4.0271e-01, -3.5959e-03,\n",
       "          -9.8602e-03,  2.4513e-01],\n",
       "         [-5.6490e-02, -3.7473e-02,  8.7315e-02,  1.1236e-01, -1.5430e-01,\n",
       "          -2.5848e-01,  1.9819e-01,  2.4158e-01, -4.5387e-01,  5.3920e-02,\n",
       "           3.4859e-02,  1.8082e-01],\n",
       "         [-6.9901e-02, -3.4317e-02,  8.9283e-02,  1.7147e-01, -1.8591e-01,\n",
       "          -3.1463e-01,  1.7975e-01,  2.8443e-01, -4.3780e-01,  4.5273e-02,\n",
       "          -1.9092e-02,  2.2117e-01],\n",
       "         [ 2.1802e-02, -8.4143e-02,  1.2738e-01,  2.0002e-01, -1.7261e-01,\n",
       "          -2.9146e-01,  1.5000e-01,  2.4518e-01, -4.5336e-01, -1.2373e-02,\n",
       "           4.3863e-02,  2.1343e-01],\n",
       "         [-5.7058e-02, -5.3858e-02,  1.0434e-01,  2.0548e-01, -1.9752e-01,\n",
       "          -3.3278e-01,  1.7260e-01,  2.9353e-01, -4.4134e-01,  1.2994e-02,\n",
       "          -3.1360e-02,  2.3303e-01]],\n",
       "\n",
       "        [[ 4.5841e-02, -9.8615e-02,  1.2422e-01,  2.8969e-01, -1.3603e-01,\n",
       "          -3.9971e-01,  1.6954e-01,  1.4094e-01, -4.9212e-01, -3.3230e-03,\n",
       "          -2.8063e-02,  2.7797e-01],\n",
       "         [ 1.1348e-01, -1.2480e-01,  1.3495e-01,  3.2712e-01, -1.4776e-01,\n",
       "          -4.3524e-01,  8.1028e-02,  9.6100e-02, -4.3937e-01, -4.0643e-02,\n",
       "           5.5850e-03,  3.1083e-01],\n",
       "         [ 1.4000e-01, -1.5581e-01,  1.5913e-01,  2.4585e-01, -1.6794e-01,\n",
       "          -3.0595e-01,  1.6468e-01,  9.4598e-02, -4.7793e-01, -5.0303e-02,\n",
       "           6.8015e-02,  2.3187e-01],\n",
       "         [ 7.3080e-02, -9.7289e-02,  9.5209e-02,  2.5527e-01, -1.8563e-01,\n",
       "          -3.9719e-01,  1.1239e-01,  1.0388e-01, -4.0062e-01, -3.2108e-02,\n",
       "          -2.5285e-03,  2.6576e-01],\n",
       "         [ 5.0809e-02, -8.8581e-02,  9.2524e-02,  2.7938e-01, -1.7785e-01,\n",
       "          -4.2853e-01,  1.1484e-01,  1.2153e-01, -4.1030e-01, -2.8942e-02,\n",
       "          -3.0192e-02,  2.7467e-01]],\n",
       "\n",
       "        [[-6.4863e-03, -1.5474e-01,  1.8903e-01,  2.3753e-01, -1.8200e-01,\n",
       "          -3.3496e-01,  2.2709e-01,  2.2329e-01, -5.1211e-01, -3.6266e-02,\n",
       "          -2.8261e-02,  1.7759e-01],\n",
       "         [ 6.2346e-03, -1.4737e-01,  1.8761e-01,  2.7531e-01, -1.8875e-01,\n",
       "          -3.9389e-01,  1.4051e-01,  2.1181e-01, -4.5670e-01, -4.7198e-02,\n",
       "          -5.9906e-02,  1.9554e-01],\n",
       "         [ 2.4038e-02, -1.6660e-01,  2.0740e-01,  2.9571e-01, -1.7750e-01,\n",
       "          -3.6984e-01,  1.9330e-01,  2.3531e-01, -5.1904e-01, -5.1934e-02,\n",
       "          -3.5165e-02,  2.0479e-01],\n",
       "         [ 3.2804e-02, -8.0804e-02,  1.0132e-01,  1.2635e-01, -1.9078e-01,\n",
       "          -3.2063e-01,  1.4557e-01,  7.0715e-02, -3.8905e-01, -1.2410e-02,\n",
       "          -1.0569e-01,  1.5558e-01],\n",
       "         [-5.2675e-03, -1.3058e-01,  1.6630e-01,  2.0533e-01, -1.8788e-01,\n",
       "          -3.2176e-01,  2.2934e-01,  2.2965e-01, -4.9457e-01, -1.5470e-02,\n",
       "          -9.7140e-03,  1.5994e-01]],\n",
       "\n",
       "        [[ 3.4336e-02, -6.1461e-02,  1.1800e-01,  1.8318e-01, -1.3723e-01,\n",
       "          -3.1281e-01,  1.8006e-01,  1.7158e-01, -4.8011e-01,  3.7916e-02,\n",
       "          -2.6132e-02,  2.2209e-01],\n",
       "         [ 1.5570e-03, -3.2207e-02,  9.5252e-02,  2.1506e-01, -1.2162e-01,\n",
       "          -3.6707e-01,  1.7128e-01,  1.9275e-01, -4.8088e-01,  4.7214e-02,\n",
       "          -6.8817e-02,  2.6072e-01],\n",
       "         [-1.2305e-02, -1.5782e-02,  7.4347e-02,  1.6930e-01, -1.3262e-01,\n",
       "          -3.4136e-01,  1.9381e-01,  1.9399e-01, -4.6794e-01,  5.7352e-02,\n",
       "          -5.7482e-02,  2.3152e-01],\n",
       "         [-1.9508e-04, -5.7015e-02,  1.0863e-01,  1.6664e-01, -1.5362e-01,\n",
       "          -3.4109e-01,  1.2714e-01,  1.5614e-01, -4.2543e-01,  5.0392e-02,\n",
       "          -5.3491e-02,  2.3166e-01],\n",
       "         [-3.3096e-02, -1.2029e-02,  4.1819e-02,  1.0730e-01, -1.6380e-01,\n",
       "          -3.2046e-01,  2.3130e-01,  1.7937e-01, -4.3364e-01,  3.9006e-02,\n",
       "          -2.9993e-02,  2.0758e-01]],\n",
       "\n",
       "        [[ 3.1716e-02, -2.5427e-03,  5.1896e-02,  2.3229e-01, -1.4535e-01,\n",
       "          -3.0143e-01,  1.9425e-01,  2.0718e-01, -4.8411e-01,  7.4070e-03,\n",
       "          -5.0028e-02,  2.3965e-01],\n",
       "         [ 7.4501e-02, -4.5557e-02,  8.4731e-02,  2.2038e-01, -1.4828e-01,\n",
       "          -2.7727e-01,  2.0241e-01,  1.7732e-01, -4.9181e-01, -8.6266e-03,\n",
       "           1.3871e-02,  1.8984e-01],\n",
       "         [ 8.8421e-03,  8.3244e-03,  3.2463e-02,  1.7541e-01, -1.1822e-01,\n",
       "          -3.3218e-01,  1.1978e-01,  1.5020e-01, -4.1910e-01, -4.9822e-03,\n",
       "          -7.1635e-02,  2.3194e-01],\n",
       "         [ 4.7511e-02, -3.7873e-02,  7.5320e-02,  2.0209e-01, -1.6674e-01,\n",
       "          -2.5082e-01,  2.1610e-01,  1.8356e-01, -4.8555e-01, -1.2527e-02,\n",
       "          -2.5233e-02,  2.0810e-01],\n",
       "         [ 4.8966e-03,  2.0935e-02,  2.6740e-02,  1.6327e-01, -1.5838e-01,\n",
       "          -2.5759e-01,  2.3118e-01,  2.0846e-01, -4.7196e-01,  3.2174e-02,\n",
       "          -4.0713e-02,  1.9633e-01]],\n",
       "\n",
       "        [[ 2.1032e-03, -8.1072e-02,  1.0351e-01,  1.8339e-01, -1.3012e-01,\n",
       "          -2.8472e-01,  1.6434e-01,  2.1580e-01, -4.8204e-01, -2.6739e-02,\n",
       "           6.3939e-02,  2.5370e-01],\n",
       "         [-2.2008e-02,  1.1880e-03,  3.9887e-02,  2.0094e-01, -1.7224e-01,\n",
       "          -3.6057e-01,  8.4811e-02,  2.0971e-01, -3.8246e-01,  2.3205e-02,\n",
       "          -6.1343e-02,  3.0605e-01],\n",
       "         [-4.5375e-02, -3.7363e-02,  5.3171e-02,  1.4183e-01, -1.8265e-01,\n",
       "          -2.8378e-01,  1.8385e-01,  2.6205e-01, -4.3385e-01, -1.1666e-02,\n",
       "           3.3508e-02,  2.6722e-01],\n",
       "         [ 1.9753e-03, -2.3759e-02,  6.4524e-02,  2.3968e-01, -1.4875e-01,\n",
       "          -3.7347e-01,  8.2103e-02,  1.9524e-01, -4.2288e-01,  4.6390e-03,\n",
       "          -8.5115e-02,  3.3621e-01],\n",
       "         [-4.8245e-02, -5.2107e-02,  8.9437e-02,  1.8839e-01, -1.5039e-01,\n",
       "          -2.9481e-01,  1.6727e-01,  2.5853e-01, -4.7328e-01,  2.4646e-03,\n",
       "          -2.4439e-03,  2.8593e-01]],\n",
       "\n",
       "        [[ 7.1194e-03, -2.0681e-02,  4.2816e-02,  2.3382e-01, -1.0975e-01,\n",
       "          -4.6478e-01,  3.8966e-02, -1.7488e-03, -3.7331e-01, -3.0733e-03,\n",
       "          -2.2527e-01,  3.4281e-01],\n",
       "         [ 2.1870e-02, -9.0817e-02,  1.0559e-01,  2.3077e-01, -1.1894e-01,\n",
       "          -3.6534e-01,  1.5713e-01,  7.8577e-02, -4.8101e-01, -3.1787e-02,\n",
       "          -1.1444e-01,  3.2023e-01],\n",
       "         [ 2.9297e-02, -9.7626e-02,  1.2144e-01,  2.9308e-01, -1.0015e-01,\n",
       "          -3.8365e-01,  1.6471e-01,  1.1267e-01, -5.3095e-01, -2.5746e-02,\n",
       "          -1.1252e-01,  3.5118e-01],\n",
       "         [-4.6741e-03, -7.0167e-02,  9.9319e-02,  2.2086e-01, -9.2111e-02,\n",
       "          -3.3413e-01,  2.0053e-01,  1.0370e-01, -5.3177e-01,  1.4444e-02,\n",
       "          -1.0684e-01,  3.0884e-01],\n",
       "         [ 8.7317e-02, -1.5253e-01,  1.6565e-01,  2.5837e-01, -1.1331e-01,\n",
       "          -3.1718e-01,  1.8007e-01,  7.2473e-02, -5.3817e-01, -4.8120e-02,\n",
       "          -3.7252e-02,  2.7695e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiHeadAttention(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aurora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
