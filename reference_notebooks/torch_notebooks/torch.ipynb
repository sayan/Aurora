{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install and Import the Necessary Libraries**  \n",
    "   We’ll need to install the following (if you haven’t already):  \n",
    "   - `datasets` (Hugging Face’s library for datasets)  \n",
    "   - `transformers` (Hugging Face’s library for models, tokenizers, etc.)  \n",
    "   - `torch` (PyTorch)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a Dataset using Hugging Face `datasets`**  \n",
    "The `datasets` library provides a convenient `load_dataset` function to load many popular NLP datasets.  \n",
    "- For example, let’s load the [IMDb dataset](https://huggingface.co/datasets/imdb), which is a sentiment classification dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")  # This returns a dictionary-like object with 'train' and 'test' splits\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Choose and Load a Model & Tokenizer**  \n",
    "   Using Hugging Face Transformers, you often load a pre-trained model and corresponding tokenizer. For example, we can load a pretrained [BERT-base-uncased](https://huggingface.co/bert-base-uncased):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the Dataset with the Tokenizer**  \n",
    "   You must tokenize the raw texts so that they become valid model inputs.  \n",
    "   - We typically apply the tokenizer on each text sample.  \n",
    "   - The tokenizer will return a dictionary with (by default) `input_ids` and `attention_mask`.  \n",
    "   - We will also keep the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84eb98c7ceb1403ca72a43b0bf7e2483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef86777f13b46d58aeaf7b2cbb5e854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac20934fd6648f2a0a8fa9dba14834a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128, add_special_tokens=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Dataset Splits to PyTorch-friendly Format**  \n",
    "   The Hugging Face `Dataset` object can be converted to PyTorch tensors using `set_format`, or you can create a custom `Dataset` subclass. A common approach is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([1, 0, 1, 1, 1, 0, 0, 0]),\n",
       " 'input_ids': tensor([[  101,  2040,  2081,  ..., 10334,  1005,   102],\n",
       "         [  101,  1045,  2428,  ...,  2007,  1996,   102],\n",
       "         [  101,  2025,  2069,  ...,  1997,  1037,   102],\n",
       "         ...,\n",
       "         [  101,  1999,  1037,  ...,  1996, 11967,   102],\n",
       "         [  101,  2008,  1005,  ...,  2003,  2204,   102],\n",
       "         [  101,  2079,  2025,  ...,  2009,  2172,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "labels = batch[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] john boo ##rman ' s 1998 the general was hailed as a major comeback , though it ' s hard to see why on the evidence of the film itself . one of three films made that year about famed northern irish criminal martin ca ##hill ( alongside ordinary decent criminal and vicious circles ) , it has an abundance of incident and style ( the film was shot in colour but released in b & w scope in some territories ) but makes absolutely no impact and just goes on forever . with a main character who threatens witnesses , car bombs doctors , causes a hundred people to lose their jobs , tries to buy off the sexually abused daughter of one of his [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokenizer.convert_ids_to_tokens(input_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-2.8016e-01, -9.4245e-03, -1.4533e-01,  ..., -3.6636e-01,\n",
       "           5.4831e-01,  3.8091e-01],\n",
       "         [ 2.0655e-01, -8.2792e-03,  1.4541e-01,  ...,  9.8328e-02,\n",
       "          -2.3967e-01, -6.6597e-01],\n",
       "         [-1.9836e-01, -2.0848e-01,  1.9919e-01,  ...,  2.6854e-01,\n",
       "           6.3531e-02,  3.0949e-01],\n",
       "         ...,\n",
       "         [-5.5118e-01, -5.0483e-01,  2.4049e-01,  ...,  7.3778e-02,\n",
       "          -4.2784e-01,  1.5624e-01],\n",
       "         [ 1.5739e-01,  2.1491e-01,  9.3533e-01,  ...,  3.7672e-01,\n",
       "           3.5113e-01, -2.8569e-01],\n",
       "         [ 3.4331e-01,  5.3125e-01, -1.0440e-01,  ...,  9.4862e-02,\n",
       "          -7.2419e-01, -3.1751e-01]],\n",
       "\n",
       "        [[-4.0850e-02, -1.0721e-01,  6.6465e-01,  ..., -1.8922e-01,\n",
       "           2.3761e-01,  4.4487e-01],\n",
       "         [-4.6778e-04,  2.0396e-01, -7.4568e-01,  ...,  3.4672e-01,\n",
       "           1.0196e+00,  2.9987e-01],\n",
       "         [ 1.5680e-01, -4.6012e-01,  4.9192e-01,  ...,  8.5190e-01,\n",
       "           7.3390e-01,  2.0683e-01],\n",
       "         ...,\n",
       "         [-1.3377e-01,  1.7616e-01,  8.0579e-01,  ...,  3.6117e-01,\n",
       "           5.1337e-02,  2.8036e-01],\n",
       "         [-2.8837e-02,  6.5854e-01,  9.0483e-01,  ..., -3.7758e-03,\n",
       "          -2.1905e-01,  3.7845e-01],\n",
       "         [ 4.3368e-01,  4.2909e-01,  8.1627e-01,  ...,  4.9171e-01,\n",
       "          -1.4993e-01,  1.2836e-01]],\n",
       "\n",
       "        [[-4.1488e-02, -1.6118e-01, -1.6201e-01,  ...,  3.0815e-02,\n",
       "           5.9683e-01,  5.6732e-01],\n",
       "         [ 5.0723e-01,  1.5277e-01, -4.8062e-01,  ..., -1.9181e-01,\n",
       "           5.9773e-01,  1.8439e-01],\n",
       "         [ 3.7759e-01,  2.5014e-03,  8.3181e-01,  ...,  2.4245e-02,\n",
       "           5.1752e-01,  2.7747e-02],\n",
       "         ...,\n",
       "         [ 7.8852e-02,  2.2283e-01,  4.9331e-01,  ...,  4.6576e-01,\n",
       "          -2.6109e-01,  8.2141e-02],\n",
       "         [-2.9462e-02,  8.4463e-02, -6.1981e-01,  ...,  3.3610e-01,\n",
       "           4.3621e-01,  4.5590e-01],\n",
       "         [ 2.1297e-01, -7.1524e-02,  1.6271e-01,  ...,  5.6642e-01,\n",
       "           4.4007e-01, -2.8618e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.8510e-02, -3.5433e-01,  1.6375e-02,  ...,  4.8507e-02,\n",
       "           3.8402e-01,  5.4358e-01],\n",
       "         [ 5.3346e-01, -2.5213e-01, -4.3692e-01,  ...,  2.7760e-01,\n",
       "           9.0233e-01,  6.1771e-01],\n",
       "         [-8.8071e-02, -1.4687e-01,  4.2692e-02,  ...,  2.6979e-01,\n",
       "           4.3573e-01, -2.8112e-01],\n",
       "         ...,\n",
       "         [-4.4236e-01, -6.3049e-01,  6.3537e-01,  ...,  3.9385e-01,\n",
       "           6.8319e-01, -3.8765e-01],\n",
       "         [ 2.3340e-01, -5.5081e-01,  6.6285e-01,  ..., -1.3923e-01,\n",
       "           8.5637e-01,  8.7962e-01],\n",
       "         [ 3.6445e-01,  3.1115e-01,  3.7209e-01,  ...,  3.6373e-01,\n",
       "           3.6569e-01,  2.1829e-02]],\n",
       "\n",
       "        [[-4.1220e-01, -1.7757e-01, -8.6654e-01,  ..., -4.9628e-02,\n",
       "           5.3417e-01,  7.8576e-01],\n",
       "         [-5.1001e-01, -4.1628e-01, -3.4238e-01,  ..., -4.4837e-01,\n",
       "           1.4218e+00,  2.9961e-01],\n",
       "         [-5.0389e-01, -8.6570e-01, -4.4827e-01,  ..., -2.3522e-01,\n",
       "           1.1074e-01,  3.0090e-01],\n",
       "         ...,\n",
       "         [-3.9854e-01,  1.3096e-01, -1.2555e-01,  ..., -4.4263e-01,\n",
       "           1.9537e-01,  5.6056e-01],\n",
       "         [-1.2555e+00, -7.6293e-01, -1.9722e-01,  ..., -3.6274e-01,\n",
       "          -1.2403e-01,  5.8047e-01],\n",
       "         [ 1.9558e-01,  2.5101e-01, -3.9875e-01,  ...,  6.2577e-02,\n",
       "           3.9047e-01, -8.3072e-02]],\n",
       "\n",
       "        [[ 2.4111e-01, -3.9075e-01,  2.6047e-01,  ..., -9.2849e-02,\n",
       "           6.5320e-01,  4.7269e-01],\n",
       "         [-4.6909e-01, -4.1937e-01,  4.0967e-01,  ..., -3.0898e-01,\n",
       "           3.3354e-01,  8.1549e-01],\n",
       "         [ 6.8228e-01, -6.5264e-01,  4.6485e-01,  ..., -5.6572e-01,\n",
       "           5.2039e-01,  1.9008e-01],\n",
       "         ...,\n",
       "         [-5.7951e-02,  2.4562e-01,  7.8575e-01,  ..., -7.2857e-01,\n",
       "           4.9964e-01,  6.0760e-01],\n",
       "         [ 1.9593e-01, -3.9634e-01, -1.2980e-01,  ..., -2.9073e-01,\n",
       "           3.5029e-02,  8.1570e-01],\n",
       "         [ 5.9133e-01,  9.7575e-02,  7.5772e-01,  ...,  3.6428e-01,\n",
       "           1.6639e-01, -2.9358e-01]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6838, -0.5086, -0.7862,  ..., -0.7158, -0.5240,  0.6480],\n",
       "        [-0.4405, -0.3965, -0.8321,  ..., -0.4746, -0.6434,  0.6674],\n",
       "        [-0.6445, -0.3619, -0.9518,  ..., -0.9111, -0.5601,  0.6947],\n",
       "        ...,\n",
       "        [-0.7283, -0.4015, -0.9039,  ..., -0.7623, -0.6097,  0.7152],\n",
       "        [-0.4905, -0.5653, -0.9493,  ..., -0.9111, -0.6367,  0.5145],\n",
       "        [-0.7214, -0.4367, -0.9517,  ..., -0.9015, -0.5038,  0.8298]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(2,10,(3,1,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 8, 3, 6, 2, 5, 5, 4, 9, 4, 7, 5, 8, 4, 6, 8],\n",
       "        [3, 5, 2, 9, 5, 3, 7, 9, 6, 7, 8, 6, 8, 3, 5, 9],\n",
       "        [7, 5, 8, 9, 7, 9, 8, 3, 7, 9, 4, 2, 7, 3, 4, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aurora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
