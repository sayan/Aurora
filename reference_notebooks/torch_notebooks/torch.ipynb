{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install and Import the Necessary Libraries**  \n",
    "   We’ll need to install the following (if you haven’t already):  \n",
    "   - `datasets` (Hugging Face’s library for datasets)  \n",
    "   - `transformers` (Hugging Face’s library for models, tokenizers, etc.)  \n",
    "   - `torch` (PyTorch)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a Dataset using Hugging Face `datasets`**  \n",
    "The `datasets` library provides a convenient `load_dataset` function to load many popular NLP datasets.  \n",
    "- For example, let’s load the [IMDb dataset](https://huggingface.co/datasets/imdb), which is a sentiment classification dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9715c3de5fa04f3094eb92560f15ec2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5bbf009a80446fa2d48a8763563b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9c8de2b2a1485d87a51b2cb88bdf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f4d868e53347fbb491faab89afd4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772b8dd123f84eb6bd5f8e2a4e113602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1f445cf76a44b69d22b48ea8479407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224db9322aa14c7ab4c866998ba33c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")  # This returns a dictionary-like object with 'train' and 'test' splits\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Choose and Load a Model & Tokenizer**  \n",
    "   Using Hugging Face Transformers, you often load a pre-trained model and corresponding tokenizer. For example, we can load a pretrained [BERT-base-uncased](https://huggingface.co/bert-base-uncased):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82432fa99e44179a188ecd6d627b669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3862d2bbab4548de9b52bc6846cce53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f570d793d822428a9dd44d7583c6d4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b831e46e542b47bc9d488cadf8049abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4c14286dea447a8089e5c8c1524c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the Dataset with the Tokenizer**  \n",
    "   You must tokenize the raw texts so that they become valid model inputs.  \n",
    "   - We typically apply the tokenizer on each text sample.  \n",
    "   - The tokenizer will return a dictionary with (by default) `input_ids` and `attention_mask`.  \n",
    "   - We will also keep the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274c1a7c941742b0a83b20aa0a0f25f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82478bed5d974ebaa7e6419f045d4c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5307df79c64fc987812589d1257aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128, add_special_tokens=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Dataset Splits to PyTorch-friendly Format**  \n",
    "   The Hugging Face `Dataset` object can be converted to PyTorch tensors using `set_format`, or you can create a custom `Dataset` subclass. A common approach is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([1, 1, 1, 0, 0, 0, 1, 0]),\n",
       " 'input_ids': tensor([[  101,  3434, 17266,  ...,  1011,  4526,   102],\n",
       "         [  101,  2023,  3315,  ...,  1037,  2261,   102],\n",
       "         [  101,  2044,  2652,  ...,  2298,  2000,   102],\n",
       "         ...,\n",
       "         [  101,  2004,  1045,  ...,  3494,  1997,   102],\n",
       "         [  101,  2023,  2143,  ...,  9364,  1999,   102],\n",
       "         [  101,  4717, 15247,  ...,  2039,  2108,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "labels = batch[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] spoil ##er alert ! this movie , zero day , gives an inside to the lives of two students , andre and calvin , who feel resentment and hatred for anyone and anything associated with there school . < br / > < br / > they go on a series of self - thought out \" missions \" all leading up to the huge mission , which is zero day . zero days contents are not specified until the middle to the end of the movie . the viewer knows its serious and filled with hate but is never quite sure until the end . < br / > < br / > now we all know , if the movie is based on the [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokenizer.convert_ids_to_tokens(input_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.6437e-01, -3.0119e-01,  2.4606e-01,  ..., -4.5059e-02,\n",
       "           6.1141e-01,  2.2898e-01],\n",
       "         [ 3.7196e-01, -7.8483e-01,  4.1921e-01,  ...,  4.4211e-01,\n",
       "           8.3071e-01, -4.8639e-01],\n",
       "         [-1.4368e-01, -9.0404e-01,  2.6135e-01,  ...,  3.3414e-01,\n",
       "           6.1729e-01, -2.3585e-01],\n",
       "         ...,\n",
       "         [ 1.4112e-01, -3.6525e-01, -6.6771e-03,  ...,  1.4221e-02,\n",
       "           4.0865e-01,  2.6404e-01],\n",
       "         [-2.9771e-01, -9.0166e-01, -3.2657e-01,  ...,  4.7729e-03,\n",
       "           1.8783e-01,  1.1907e-02],\n",
       "         [-2.3642e-01, -8.4104e-03,  4.1910e-01,  ...,  4.3990e-01,\n",
       "           3.1462e-01, -6.3929e-01]],\n",
       "\n",
       "        [[ 1.3358e-01, -1.8097e-03,  3.0410e-01,  ..., -7.3996e-02,\n",
       "           4.4114e-01,  4.3563e-01],\n",
       "         [-6.2384e-01,  5.5745e-01,  1.2868e-01,  ..., -1.1573e-01,\n",
       "           7.2936e-01,  2.7176e-01],\n",
       "         [-2.4140e-01,  2.7660e-01,  1.0382e+00,  ..., -3.3970e-01,\n",
       "           3.0663e-01, -8.6786e-01],\n",
       "         ...,\n",
       "         [-3.6322e-01,  2.8960e-01,  1.1743e-01,  ...,  2.9693e-01,\n",
       "           3.2098e-01,  2.3647e-01],\n",
       "         [ 1.3179e-01,  7.4849e-02, -1.1862e-01,  ...,  1.0637e-01,\n",
       "           5.2468e-01, -5.2503e-02],\n",
       "         [ 3.4325e-01,  7.0191e-01,  9.5661e-01,  ...,  2.7280e-01,\n",
       "           1.9449e-01, -9.3411e-02]],\n",
       "\n",
       "        [[ 4.4966e-04, -1.9572e-02, -6.1938e-03,  ..., -1.2267e-02,\n",
       "           6.6790e-01,  4.6454e-01],\n",
       "         [ 1.0224e+00,  1.7293e-01,  3.5127e-01,  ...,  4.4160e-01,\n",
       "           6.8392e-01,  5.9598e-01],\n",
       "         [ 1.4517e-01, -5.2433e-01,  1.9118e-01,  ...,  2.0146e-01,\n",
       "           5.1974e-01, -2.1892e-01],\n",
       "         ...,\n",
       "         [-1.2450e-01,  5.0589e-01, -4.7956e-02,  ..., -2.5353e-01,\n",
       "           9.9746e-01,  1.7983e-01],\n",
       "         [-7.9919e-02,  2.1256e-01, -5.4640e-01,  ...,  2.3023e-01,\n",
       "           1.1067e+00, -3.7160e-01],\n",
       "         [ 3.1534e-01,  6.8886e-01,  4.5854e-01,  ...,  4.6542e-01,\n",
       "          -2.5421e-01, -2.2360e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.1307e-01, -1.9942e-01, -7.9780e-01,  ..., -1.7690e-01,\n",
       "           8.2880e-01,  6.2695e-01],\n",
       "         [-2.2271e-01, -2.3881e-01, -4.4524e-01,  ...,  2.6857e-01,\n",
       "           8.1809e-01,  2.2104e-01],\n",
       "         [-6.0765e-02, -1.5354e-01, -1.7991e-01,  ..., -2.0978e-01,\n",
       "           9.0461e-01,  5.3175e-01],\n",
       "         ...,\n",
       "         [ 3.0161e-02, -1.1315e+00, -9.6752e-01,  ..., -2.9116e-02,\n",
       "           5.6811e-01,  3.5301e-01],\n",
       "         [-6.9454e-01, -9.0581e-01, -7.3208e-01,  ..., -7.4174e-03,\n",
       "           6.2369e-01, -7.3259e-03],\n",
       "         [ 4.5868e-01,  4.7903e-01, -5.4532e-02,  ...,  3.7724e-01,\n",
       "          -2.3741e-01, -3.2045e-01]],\n",
       "\n",
       "        [[-3.3375e-01, -2.6336e-01, -3.1687e-01,  ..., -1.0334e-01,\n",
       "           7.7289e-01,  7.6856e-01],\n",
       "         [-2.5416e-01,  6.3386e-02, -3.2256e-01,  ..., -1.2298e-01,\n",
       "           1.0011e+00,  5.0927e-01],\n",
       "         [-8.9914e-01,  3.8721e-01,  9.3810e-03,  ..., -3.3894e-01,\n",
       "          -2.4102e-01,  8.8343e-01],\n",
       "         ...,\n",
       "         [-1.5663e-01,  3.2544e-01,  1.9641e-01,  ...,  3.9291e-01,\n",
       "           7.6635e-01,  4.1750e-01],\n",
       "         [ 2.3389e-01,  2.7692e-02, -2.1155e-01,  ..., -6.1809e-02,\n",
       "           5.4376e-01,  9.5649e-02],\n",
       "         [ 3.6142e-01,  4.9444e-01,  3.3756e-02,  ...,  1.4340e-01,\n",
       "          -5.4905e-01, -2.1213e-01]],\n",
       "\n",
       "        [[-1.8126e-01, -2.9224e-02, -1.5167e-01,  ..., -1.6194e-01,\n",
       "           4.2979e-01,  4.4466e-01],\n",
       "         [ 7.1640e-01,  3.7559e-01, -5.7331e-02,  ..., -6.4509e-02,\n",
       "           9.1094e-01,  1.7400e-01],\n",
       "         [-2.4158e-01, -2.6741e-01,  6.5956e-01,  ..., -3.2136e-01,\n",
       "           2.3958e-01,  1.2224e+00],\n",
       "         ...,\n",
       "         [-1.1201e-01, -3.2762e-01, -3.5520e-01,  ..., -3.1346e-01,\n",
       "           8.0794e-01,  3.5029e-01],\n",
       "         [-5.8564e-01, -7.0438e-01, -4.8885e-01,  ..., -5.4252e-01,\n",
       "           5.0139e-01,  2.6551e-01],\n",
       "         [ 4.8584e-01,  7.2542e-01, -5.5948e-02,  ...,  4.4144e-01,\n",
       "          -1.0869e-01, -2.1120e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6955, -0.4147, -0.8022,  ..., -0.5277, -0.5904,  0.5495],\n",
       "        [-0.5500, -0.2968, -0.8654,  ..., -0.6690, -0.3859,  0.6182],\n",
       "        [-0.2856, -0.0942,  0.6522,  ...,  0.5116, -0.3229,  0.5157],\n",
       "        ...,\n",
       "        [-0.6662, -0.5577, -0.9822,  ..., -0.9519, -0.6035,  0.4911],\n",
       "        [-0.7758, -0.3712, -0.8968,  ..., -0.9202, -0.5774,  0.7475],\n",
       "        [-0.6692, -0.4358, -0.7656,  ..., -0.6429, -0.5552,  0.7806]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
