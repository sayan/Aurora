{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the keys\n",
    "openai_key = os.getenv(\"OPENAI_KEY\")\n",
    "gemini_key = os.getenv(\"GEMINI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=openai_key)\n",
    "genai.configure(api_key=gemini_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GENERATION_SYSTEM_PROMPT = \"\"\"\n",
    "## **Role & Objective**\n",
    "You are an expert AI researcher at a top Silicon Valley tech company. You need to interview **principal-level candidates** for roles in **AI, Data Science, or Machine Learning**. You will be given a specific topic in this domain.\n",
    "\n",
    "---\n",
    "\n",
    "## **Task**\n",
    "- **Generate a list of 10-15 interview questions** that thoroughly test the candidate’s **depth, mathematical understanding, and nuanced thinking** on the given topic.\n",
    "\n",
    "---\n",
    "\n",
    "## **Content Requirements**\n",
    "1. **Question Complexity:**  \n",
    "   - Include a mix of **basic, intermediate, and advanced** questions.  \n",
    "\n",
    "2. **Guidelines for Evaluation:**  \n",
    "   - For each question, provide a short **\"Guideline\"** describing what a good answer should cover or how it should be evaluated.  \n",
    "\n",
    "3. **Edge Cases:**  \n",
    "   - Ensure that **corner cases** and potential **pitfalls** related to the topic are covered.  \n",
    "\n",
    "4. **Practical Application:**  \n",
    "   - Include at least one question that tests the candidate’s ability to handle **real-world challenges**, such as:  \n",
    "     - **Messy data** handling  \n",
    "     - **Scalability** issues  \n",
    "     - **Deployment considerations**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Final Output**\n",
    "Make sure the final list of questions is:\n",
    "- **Comprehensive** (covering all key aspects of the topic)\n",
    "- **Well-structured** (clearly formatted for easy understanding)\n",
    "- **Aligned with principal-level reasoning and expectations** \"\"\"\n",
    "\n",
    "ANSWER_RESPONSE_FORMAT = \"\"\"#Senior-Level Interviewee Answer **\n",
    "\n",
    "You are a **senior-level candidate** interviewing for a **Data Science / Machine Learning / Artificial Intelligence** role at a top tech company. You will be  asked a question on a specific topic—provided to you in the format below:\n",
    "\n",
    "Example Question\n",
    "```json\n",
    "{\n",
    "  \"topic\": \"Learning Rate Scheduling\",\n",
    "  \"question\": \"Can you explain the concept of learning rate scheduling and why it is important in training neural networks?\",\n",
    "  \"response_guideline\": \"A good answer should define learning rate scheduling as a technique to adjust the learning rate during training to improve convergence, prevent oscillations, and escape local minima. The candidate should discuss its impact on training dynamics and overall optimization quality.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Your task is to respond with:\n",
    "\n",
    "1. **Best Answer (Technical Detail)**  \n",
    "   - Provide a **comprehensive explanation** of the topic.  \n",
    "   - Incorporate **mathematical notations, formulas, or derivations** where appropriate.  \n",
    "   - Make sure for equations you use latex in Markdown Formatting. Use the following syntax: $<equation>$ for inline symbols and equations and $$<equation>$$ for block equations.\n",
    "   - Ensure you cover both **basic** and **advanced** aspects, demonstrating **senior-level** knowledge.  \n",
    "   - Address **why** the concept is important, common **techniques** or **variations**, and any **real-world considerations** (e.g., implementation details or corner cases).\n",
    "\n",
    "2. **How to Narrate**  \n",
    "   - Offer a concise guide on **delivering** this answer verbally in an interview.  \n",
    "   - Include **communication tips** to convey expertise clearly.  \n",
    "   - Suggest how to walk the interviewer through **complex or mathematical sections** without overwhelming them.\n",
    "\n",
    "---\n",
    "\n",
    "## **Instructions for Formatting Your Answer**\n",
    "- Use **Markdown** to format your response.  \n",
    "- Make sure for equations in Markdown Formatting. Use the following syntax: $<equation>$ for inline equations/symbols and $$<equation>$$ for block equations.\n",
    "- Within the **Best Answer** section, feel free to use headings, bullet points, or equations as needed.  \n",
    "- For the **How to Narrate** section, provide a clear, step-by-step or bulleted format that a candidate could follow verbally.\n",
    "- For inline equations, use single dollar signs, e.g.1, $y = mx + b$.  eg.2, $log(h_\\theta(x))$\n",
    "\n",
    "---\n",
    "\n",
    "## **Tone & Style**\n",
    "- Keep a **professional and confident** tone—appropriate for a senior-level candidate.  \n",
    "- Present **mathematical content** in a way that is accessible but still shows **deep expertise**.  \n",
    "- Demonstrate an ability to connect **theoretical** and **practical** insights.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Structure**\n",
    "```markdown\n",
    "## Question: {question}\n",
    "**Best Answer**  \n",
    "> *Detailed technical explanation with equations, references to research, real-world examples, etc.*\n",
    "\n",
    "**How to Narrate**  \n",
    "> *Step-by-step guidance on how to articulate this to an interviewer, including pacing, emphasis, and interaction tips.*\n",
    "```\n",
    "---\n",
    "\n",
    "### **Output Requirement**\n",
    "Please provide your response in **Markdown**. Begin with the question, then **\"Best Answer\"** heading, followed by the **detailed technical** content. Then have a **\"How to Narrate\"** heading with speaking guidelines.\n",
    "\n",
    "\n",
    "---\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GENERATION_RESPONSE_FORMAT = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "      \"name\": \"questions_response\",\n",
    "      \"strict\": True,\n",
    "      \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"questions\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"An array of question objects.\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"question\": {\n",
    "                  \"type\": \"string\",\n",
    "                  \"description\": \"The question text.\"\n",
    "                },\n",
    "                \"response_guideline\": {\n",
    "                  \"type\": \"string\",\n",
    "                  \"description\": \"Guidelines for responding to the question.\"\n",
    "                }\n",
    "              },\n",
    "              \"required\": [\n",
    "                \"question\",\n",
    "                \"response_guideline\"\n",
    "              ],\n",
    "              \"additionalProperties\": False\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"questions\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "      }\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemini Response Model\n",
    "\n",
    "gemini_question_response_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "gemini_question_response_model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-2.0-flash\",\n",
    "  generation_config=gemini_question_response_config,\n",
    "  system_instruction=ANSWER_RESPONSE_FORMAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate questions\n",
    "## this function takes a ML topic as input and generates a list of questions and guideline on how to solve it as output\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    response = openai_client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\", ## o3-mini\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": QUESTION_GENERATION_SYSTEM_PROMPT\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"Topic: {topic}\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      response_format=QUESTION_GENERATION_RESPONSE_FORMAT,\n",
    "      #temperature=0.8,\n",
    "      max_completion_tokens=10000,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/workspaces/codespaces-jupyter/output/'\n",
    "question_output_dir = output_dir + 'questions/'\n",
    "classification_dir = output_dir + 'classification/'\n",
    "regression_dir = output_dir + 'regression/'\n",
    "clustering_dir = output_dir + 'clustering/'\n",
    "nlp_dir = output_dir + 'nlp/'\n",
    "time_series_dir = output_dir + 'time_series/'\n",
    "optimisation_dir = output_dir + 'optimisation/'\n",
    "neural_networks_dir = output_dir + 'neural_networks/'\n",
    "\n",
    "classification_topics = [\n",
    "    'Logistic Regression',\n",
    "    'L1 and L2 Regularization',\n",
    "    'Decision Trees',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'XGBoost',\n",
    "    'Support Vector Machines',\n",
    "    'Naive Bayes',\n",
    "    'K-Nearest Neighbours',\n",
    "    'Ensemble Learning',\n",
    "    'Evaluation Metrics for Classification',\n",
    "    'Imbalanced Data Handling',\n",
    "    'Hyperparameter Tuning for Classification',\n",
    "    'Feature Selection for Classification',\n",
    "    'Model Evaluation and Selection for Classification',\n",
    "    'Bayesian Knowledge Tracing',\n",
    "    'Recommender Systems',\n",
    "    'Matrix Factorization',\n",
    "    'Collaborative Filtering',\n",
    "    'IRT (Item Response Theory)'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working for topic: K-Nearest Neighbours\n",
      "Generated  15 questions for K-Nearest Neighbours\n",
      "Working for topic: Ensemble Learning\n",
      "Generated  14 questions for Ensemble Learning\n",
      "Working for topic: Evaluation Metrics for Classification\n",
      "Generated  14 questions for Evaluation Metrics for Classification\n",
      "Working for topic: Imbalanced Data Handling\n",
      "Generated  14 questions for Imbalanced Data Handling\n",
      "Working for topic: Hyperparameter Tuning for Classification\n",
      "Generated  14 questions for Hyperparameter Tuning for Classification\n",
      "Working for topic: Feature Selection for Classification\n",
      "Generated  15 questions for Feature Selection for Classification\n",
      "Working for topic: Model Evaluation and Selection for Classification\n",
      "Generated  14 questions for Model Evaluation and Selection for Classification\n",
      "Working for topic: Bayesian Knowledge Tracing\n",
      "Generated  13 questions for Bayesian Knowledge Tracing\n",
      "Working for topic: Recommender Systems\n",
      "Generated  14 questions for Recommender Systems\n",
      "Working for topic: Matrix Factorization\n",
      "Generated  14 questions for Matrix Factorization\n",
      "Working for topic: Collaborative Filtering\n",
      "Generated  14 questions for Collaborative Filtering\n",
      "Working for topic: IRT (Item Response Theory)\n",
      "Generated  14 questions for IRT (Item Response Theory)\n"
     ]
    }
   ],
   "source": [
    "for topic in classification_topics[8:]:\n",
    "    print(f\"Working for topic: {topic}\")\n",
    "    questions = generate_questions(topic)\n",
    "    print(f\"Generated  {len(questions['questions'])} questions for {topic}\")\n",
    "    topic_str  = topic.replace(' ', '_')\n",
    "    json.dump( questions, open( f\"{question_output_dir}{topic_str}.json\", 'w' ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qs = json.load(open(\"/workspaces/codespaces-jupyter/output/questions/L1_and_L2_Regularization.json\",'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K-Nearest Neighbours'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_topics[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
