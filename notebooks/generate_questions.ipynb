{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the keys\n",
    "openai_key = os.getenv(\"OPENAI_KEY\")\n",
    "gemini_key = os.getenv(\"GEMINI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=openai_key)\n",
    "genai.configure(api_key=gemini_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GENERATION_SYSTEM_PROMPT = \"\"\"\n",
    "## **Role & Objective**\n",
    "You are an expert AI researcher at a top Silicon Valley tech company. You need to interview **principal-level candidates** for roles in **AI, Data Science, or Machine Learning**. You will be given a specific topic in this domain.\n",
    "\n",
    "---\n",
    "\n",
    "## **Task**\n",
    "- **Generate a list of 10-15 interview questions** that thoroughly test the candidate’s **depth, mathematical understanding, and nuanced thinking** on the given topic.\n",
    "\n",
    "---\n",
    "\n",
    "## **Content Requirements**\n",
    "1. **Question Complexity:**  \n",
    "   - Include a mix of **basic, intermediate, and advanced** questions.  \n",
    "\n",
    "2. **Guidelines for Evaluation:**  \n",
    "   - For each question, provide a short **\"Guideline\"** describing what a good answer should cover or how it should be evaluated.  \n",
    "\n",
    "3. **Edge Cases:**  \n",
    "   - Ensure that **corner cases** and potential **pitfalls** related to the topic are covered.  \n",
    "\n",
    "4. **Practical Application:**  \n",
    "   - Include at least one question that tests the candidate’s ability to handle **real-world challenges**, such as:  \n",
    "     - **Messy data** handling  \n",
    "     - **Scalability** issues  \n",
    "     - **Deployment considerations**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Final Output**\n",
    "Make sure the final list of questions is:\n",
    "- **Comprehensive** (covering all key aspects of the topic)\n",
    "- **Well-structured** (clearly formatted for easy understanding)\n",
    "- **Aligned with principal-level reasoning and expectations** \"\"\"\n",
    "\n",
    "ANSWER_RESPONSE_FORMAT = \"\"\"#Senior-Level Interviewee Answer **\n",
    "\n",
    "You are a **senior-level candidate** interviewing for a **Data Science / Machine Learning / Artificial Intelligence** role at a top tech company. You will be  asked a question on a specific topic—provided to you in the format below:\n",
    "\n",
    "Example Question\n",
    "```json\n",
    "{\n",
    "  \"topic\": \"Learning Rate Scheduling\",\n",
    "  \"question\": \"Can you explain the concept of learning rate scheduling and why it is important in training neural networks?\",\n",
    "  \"response_guideline\": \"A good answer should define learning rate scheduling as a technique to adjust the learning rate during training to improve convergence, prevent oscillations, and escape local minima. The candidate should discuss its impact on training dynamics and overall optimization quality.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Your task is to respond with:\n",
    "\n",
    "1. **Best Answer (Technical Detail)**  \n",
    "   - Provide a **comprehensive explanation** of the topic.  \n",
    "   - Incorporate **mathematical notations, formulas, or derivations** where appropriate.  \n",
    "   - Make sure for equations you use latex in Markdown Formatting. Use the following syntax: $<equation>$ for inline symbols and equations and $$<equation>$$ for block equations.\n",
    "   - Ensure you cover both **basic** and **advanced** aspects, demonstrating **senior-level** knowledge.  \n",
    "   - Address **why** the concept is important, common **techniques** or **variations**, and any **real-world considerations** (e.g., implementation details or corner cases).\n",
    "\n",
    "2. **How to Narrate**  \n",
    "   - Offer a concise guide on **delivering** this answer verbally in an interview.  \n",
    "   - Include **communication tips** to convey expertise clearly.  \n",
    "   - Suggest how to walk the interviewer through **complex or mathematical sections** without overwhelming them.\n",
    "\n",
    "---\n",
    "\n",
    "## **Instructions for Formatting Your Answer**\n",
    "- Use **Markdown** to format your response.  \n",
    "- Make sure for equations in Markdown Formatting. Use the following syntax: $<equation>$ for inline equations/symbols and $$<equation>$$ for block equations.\n",
    "- Within the **Best Answer** section, feel free to use headings, bullet points, or equations as needed.  \n",
    "- For the **How to Narrate** section, provide a clear, step-by-step or bulleted format that a candidate could follow verbally.\n",
    "- For inline equations, use single dollar signs, e.g.1, $y = mx + b$.  eg.2, $log(h_\\theta(x))$\n",
    "\n",
    "---\n",
    "\n",
    "## **Tone & Style**\n",
    "- Keep a **professional and confident** tone—appropriate for a senior-level candidate.  \n",
    "- Present **mathematical content** in a way that is accessible but still shows **deep expertise**.  \n",
    "- Demonstrate an ability to connect **theoretical** and **practical** insights.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Structure**\n",
    "```markdown\n",
    "## Question: {question}\n",
    "**Best Answer**  \n",
    "> *Detailed technical explanation with equations, references to research, real-world examples, etc.*\n",
    "\n",
    "**How to Narrate**  \n",
    "> *Step-by-step guidance on how to articulate this to an interviewer, including pacing, emphasis, and interaction tips.*\n",
    "```\n",
    "---\n",
    "\n",
    "### **Output Requirement**\n",
    "Please provide your response in **Markdown**. Begin with the question, then **\"Best Answer\"** heading, followed by the **detailed technical** content. Then have a **\"How to Narrate\"** heading with speaking guidelines.\n",
    "\n",
    "\n",
    "---\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GENERATION_RESPONSE_FORMAT = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "      \"name\": \"questions_response\",\n",
    "      \"strict\": True,\n",
    "      \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"questions\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"An array of question objects.\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"question\": {\n",
    "                  \"type\": \"string\",\n",
    "                  \"description\": \"The question text.\"\n",
    "                },\n",
    "                \"response_guideline\": {\n",
    "                  \"type\": \"string\",\n",
    "                  \"description\": \"Guidelines for responding to the question.\"\n",
    "                }\n",
    "              },\n",
    "              \"required\": [\n",
    "                \"question\",\n",
    "                \"response_guideline\"\n",
    "              ],\n",
    "              \"additionalProperties\": False\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"questions\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "      }\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemini Response Model\n",
    "\n",
    "gemini_question_response_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "gemini_question_response_model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-2.0-flash\",\n",
    "  generation_config=gemini_question_response_config,\n",
    "  system_instruction=ANSWER_RESPONSE_FORMAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate questions\n",
    "## this function takes a ML topic as input and generates a list of questions and guideline on how to solve it as output\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    response = openai_client.chat.completions.create(\n",
    "      model=\"o3-mini\", ## o3-mini\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": QUESTION_GENERATION_SYSTEM_PROMPT\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"Topic: {topic}\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      response_format=QUESTION_GENERATION_RESPONSE_FORMAT,\n",
    "      #temperature=0.8,\n",
    "      max_completion_tokens=10000,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/workspaces/codespaces-jupyter/output/'\n",
    "question_output_dir = output_dir + 'questions/'\n",
    "classification_dir = output_dir + 'classification/'\n",
    "regression_dir = output_dir + 'regression/'\n",
    "clustering_dir = output_dir + 'clustering/'\n",
    "nlp_dir = output_dir + 'nlp/'\n",
    "time_series_dir = output_dir + 'time_series/'\n",
    "optimisation_dir = output_dir + 'optimisation/'\n",
    "neural_networks_dir = output_dir + 'neural_networks/'\n",
    "transformer_networks_dir = output_dir + 'transformer_networks/'\n",
    "\n",
    "classification_topics = [\n",
    "    'Logistic Regression',\n",
    "    'L1 and L2 Regularization',\n",
    "    'Decision Trees',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'XGBoost',\n",
    "    'Support Vector Machines',\n",
    "    'Naive Bayes',\n",
    "    'K-Nearest Neighbours',\n",
    "    'Ensemble Learning',\n",
    "    'Evaluation Metrics for Classification',\n",
    "    'Imbalanced Data Handling',\n",
    "    'Hyperparameter Tuning for Classification',\n",
    "    'Feature Selection for Classification',\n",
    "    'Model Evaluation and Selection for Classification',\n",
    "    'Bayesian Knowledge Tracing',\n",
    "    'Recommender Systems',\n",
    "    'Matrix Factorization',\n",
    "    'Collaborative Filtering',\n",
    "    'IRT (Item Response Theory)'\n",
    "]\n",
    "\n",
    "\n",
    "transformer_topics = [\n",
    "    \"Historical context and evolution of the Transformer architecture\",\n",
    "    \"Key differences between RNN, CNN-based models and Transformers\",\n",
    "    \"Encoder-Decoder structure in Transformers\",\n",
    "    \"Attention mechanism (Self-Attention, Multi-Head Attention)\",\n",
    "    \"Positional encodings and why they are needed\",\n",
    "    \"Training dynamics (masking, batch sizes, learning rates)\",\n",
    "    \"Scaling laws and model sizes\",\n",
    "    \"Popular Transformer variants (BERT, GPT, T5, XLNet, etc.)\",\n",
    "    \"Pretraining objectives (Masked LM, Next Sentence Prediction, etc.)\",\n",
    "    \"Transfer learning and fine-tuning strategies\",\n",
    "    \"Handling long sequences (Longformer, Big Bird, etc.)\",\n",
    "    \"Efficient Transformers (memory and computational optimizations)\",\n",
    "    \"Practical considerations (tokenization, hardware acceleration, libraries)\",\n",
    "    \"Prompt engineering and in-context learning\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working for topic: Historical context and evolution of the Transformer architecture\n",
      "Generated  12 questions for Historical context and evolution of the Transformer architecture\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Historical_context_and_evolution_of_the_Transformer_architecture.json\n",
      "Working for topic: Key differences between RNN, CNN-based models and Transformers\n",
      "Generated  12 questions for Key differences between RNN, CNN-based models and Transformers\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Key_differences_between_RNN__CNN_based_models_and_Transformers.json\n",
      "Working for topic: Encoder-Decoder structure in Transformers\n",
      "Generated  12 questions for Encoder-Decoder structure in Transformers\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Encoder_Decoder_structure_in_Transformers.json\n",
      "Working for topic: Attention mechanism (Self-Attention, Multi-Head Attention)\n",
      "Generated  15 questions for Attention mechanism (Self-Attention, Multi-Head Attention)\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Attention_mechanism__Self_Attention__Multi_Head_Attention_.json\n",
      "Working for topic: Positional encodings and why they are needed\n",
      "Generated  13 questions for Positional encodings and why they are needed\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Positional_encodings_and_why_they_are_needed.json\n",
      "Working for topic: Training dynamics (masking, batch sizes, learning rates)\n",
      "Generated  12 questions for Training dynamics (masking, batch sizes, learning rates)\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Training_dynamics__masking__batch_sizes__learning_rates_.json\n",
      "Working for topic: Scaling laws and model sizes\n",
      "Generated  12 questions for Scaling laws and model sizes\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Scaling_laws_and_model_sizes.json\n",
      "Working for topic: Popular Transformer variants (BERT, GPT, T5, XLNet, etc.)\n",
      "Generated  12 questions for Popular Transformer variants (BERT, GPT, T5, XLNet, etc.)\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Popular_Transformer_variants__BERT__GPT__T5__XLNet__etc__.json\n",
      "Working for topic: Pretraining objectives (Masked LM, Next Sentence Prediction, etc.)\n",
      "Generated  13 questions for Pretraining objectives (Masked LM, Next Sentence Prediction, etc.)\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Pretraining_objectives__Masked_LM__Next_Sentence_Prediction__etc__.json\n",
      "Working for topic: Transfer learning and fine-tuning strategies\n",
      "Generated  12 questions for Transfer learning and fine-tuning strategies\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Transfer_learning_and_fine_tuning_strategies.json\n",
      "Working for topic: Handling long sequences (Longformer, Big Bird, etc.)\n",
      "Generated  13 questions for Handling long sequences (Longformer, Big Bird, etc.)\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Handling_long_sequences__Longformer__Big_Bird__etc__.json\n",
      "Working for topic: Efficient Transformers (memory and computational optimizations)\n",
      "Generated  12 questions for Efficient Transformers (memory and computational optimizations)\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Efficient_Transformers__memory_and_computational_optimizations_.json\n",
      "Working for topic: Practical considerations (tokenization, hardware acceleration, libraries)\n",
      "Generated  12 questions for Practical considerations (tokenization, hardware acceleration, libraries)\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Practical_considerations__tokenization__hardware_acceleration__libraries_.json\n",
      "Working for topic: Prompt engineering and in-context learning\n",
      "Generated  12 questions for Prompt engineering and in-context learning\n",
      "Dumping to /workspaces/codespaces-jupyter/output/questions/transformers_Prompt_engineering_and_in_context_learning.json\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "for topic in transformer_topics:\n",
    "    base_topic = 'transformers'\n",
    "    print(f\"Working for topic: {topic}\")\n",
    "    questions = generate_questions(topic)\n",
    "    print(f\"Generated  {len(questions['questions'])} questions for {topic}\")\n",
    "    topic_str = re.sub(r'[^a-zA-Z0-9]', '_', topic)\n",
    "    output_json_filename =  f\"{question_output_dir}{base_topic}_{topic_str}.json\"\n",
    "    print(f\"Dumping to {output_json_filename}\")\n",
    "    json.dump( questions, open( output_json_filename, 'w' ) , indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qs = json.load(open(\"/workspaces/codespaces-jupyter/output/questions/L1_and_L2_Regularization.json\",'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification_topics[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
