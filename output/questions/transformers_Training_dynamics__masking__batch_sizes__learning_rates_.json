{
    "questions": [
        {
            "question": "1. Can you explain the role of masking in training deep learning models, particularly in sequence-based tasks?",
            "response_guideline": "A good answer should cover how masking prevents the model from attending to irrelevant or padded tokens, the impact on loss and gradient calculations, and examples like language models (transformers) or attention mechanisms in RNNs."
        },
        {
            "question": "2. How do different batch sizes influence the convergence dynamics of training neural networks?",
            "response_guideline": "The candidate should discuss the trade-offs between small and large batch sizes, effects on gradient noise and variance, convergence speed, generalization ability, and the impact on computational efficiency and memory usage."
        },
        {
            "question": "3. Describe the relationship between learning rate and batch size. How might one modify the learning rate when changing the batch size?",
            "response_guideline": "Look for discussion on the linear scaling rule, how larger batches might warrant higher learning rates, possible effects on model stability, and references to empirical observations or papers (e.g., Goyal et al.) that address this interplay."
        },
        {
            "question": "4. What are common learning rate scheduling techniques, and how do they impact the training dynamics over time?",
            "response_guideline": "A strong answer should mention techniques such as step decay, exponential decay, cosine annealing, and cyclical learning rates. It should also explain how these methods help in navigating local minima and adjusting the convergence speed."
        },
        {
            "question": "5. In your experience, what are the risks or pitfalls of an improperly chosen learning rate, and how can you diagnose these issues during training?",
            "response_guideline": "The response should address signs of divergence, oscillations, or slow convergence, discuss techniques like learning rate finder, and include diagnostics such as loss curves, gradient norms, or validation performance."
        },
        {
            "question": "6. Masking isn't just used in sequence models. Can you discuss any non-obvious scenarios where dynamic masking might be useful during training and why?",
            "response_guideline": "A good answer might touch on cases like dropout variants, selective backpropagation, or masking corrupt labels in unsupervised or semi-supervised setups, explaining how masking can improve robustness or reduce noise."
        },
        {
            "question": "7. How do you handle edge cases in batch preparation when dealing with highly variable sequence lengths or missing tokens?",
            "response_guideline": "The candidate should discuss strategies for padding, bucketing (grouping sequences by similar lengths), dynamic batching methods, and the impact of masking on ensuring that padded values do not bias the learning process."
        },
        {
            "question": "8. Explain how learning rate warm-up strategies function and why they might be particularly beneficial in certain training scenarios.",
            "response_guideline": "An excellent answer would detail the process of gradually increasing the learning rate during initial training phases, referencing issues such as unstable updates early in training, and connecting this to architectures like transformers or very deep networks."
        },
        {
            "question": "9. Suppose you are tasked with deploying a model trained on large-scale data using noisy and unstructured inputs. How would you adapt your training dynamics (batch size, learning rate, and masking strategies) to accommodate real-world challenges?",
            "response_guideline": "The candidate should demonstrate practical experience by discussing approaches for robust data preprocessing, handling variability, possibly using adaptive learning rate methods or robust optimizer choices, and ensuring that masking or filtering techniques are applied to mitigate the effects of noise and missing data."
        },
        {
            "question": "10. In the context of distributed training, what challenges might arise related to batch size and learning rate adjustments, and how would you address them?",
            "response_guideline": "Expect discussion on synchronization issues, differences in effective batch size per device, variance in gradient estimates, and solutions like gradient accumulation or adjusting learning rates across multiple nodes to account for effective batch scaling."
        },
        {
            "question": "11. Describe a scenario where you observed or suspect an issue with the training dynamics due to improper masking. How would you debug and resolve such an issue?",
            "response_guideline": "Look for systematic diagnostic steps, such as verifying mask generation logic, checking tensor shapes and loss propagation, inspecting model outputs for edge cases, and proposing modifications or experiments to isolate the problem."
        },
        {
            "question": "12. Can you elaborate on how the interplay between masking, batch sizes, and learning rates might influence model generalization and overfitting?",
            "response_guideline": "The candidate should articulate how these parameters interact to affect model regularization: for example, how larger batch sizes might reduce noise leading to potential overfitting, or how strict masking can reduce model bias but might lose context, and the mitigating role of learning rate adjustments in promoting generalization."
        }
    ]
}