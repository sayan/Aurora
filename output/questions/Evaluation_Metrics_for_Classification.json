{"questions": [{"question": "What is the difference between precision and recall? Why is it important to consider both metrics in the evaluation of a classification model?", "response_guideline": "A strong answer should clearly define precision and recall, provide their mathematical formulas, and explain their significance in different contexts, such as imbalanced datasets. The candidate should discuss how relying solely on accuracy can be misleading."}, {"question": "Can you explain the concept of the F1 score and when it should be used as an evaluation metric?", "response_guideline": "The candidate should provide the definition of the F1 score, its formula, and state that it is the harmonic mean of precision and recall. They should discuss its appropriateness in scenarios with imbalanced classes."}, {"question": "What are the ROC curve and AUC? How do they help assess the performance of a classifier?", "response_guideline": "The candidate should explain what a ROC curve represents and how it is generated. They should describe what the AUC (Area Under Curve) indicates about model performance, highlighting that higher AUC values indicate better overall model ability across various thresholds."}, {"question": "What is the significance of the confusion matrix in classification? Can you list and explain the terms it contains?", "response_guideline": "A perfect answer would describe the confusion matrix and detail its components: True Positives, True Negatives, False Positives, and False Negatives. The candidate should relate these terms back to various evaluation metrics."}, {"question": "Describe how you would evaluate a model's performance on an imbalanced dataset. What metrics would you prioritize?", "response_guideline": "The candidate should recognize the challenges imbalanced datasets pose and mention metrics like precision, recall, F1 score, and the ROC-AUC curve. They should also discuss methods to balance datasets, such as resampling."}, {"question": "How would you approach the problem of hyperparameter tuning in the context of classification evaluation metrics?", "response_guideline": "A competent response should explain the need for tuning hyperparameters to improve model performance. The candidate should mention methods such as grid search, random search, or Bayesian optimization, and how their evaluation would be based on chosen metrics."}, {"question": "What are some common pitfalls in using classification metrics, and how can they be mitigated?", "response_guideline": "The candidate should identify pitfalls like class imbalance bias, overfitting to specific metrics, or misinterpretation of metrics. They should discuss approaches to mitigate these issues, such as utilizing stratified cross-validation or using multiple metrics."}, {"question": "How would you handle a situation where a model has a high accuracy but low precision and recall? What insights can you draw from this scenario?", "response_guideline": "A quality answer will discuss how high accuracy could be misleading in imbalanced datasets and emphasize the need for a deeper analysis of precision and recall. The candidate should propose possible adjustments or alternative models."}, {"question": "Can you discuss the importance of comparing multiple models using a consistent set of evaluation metrics? Why is variability across metrics a concern?", "response_guideline": "The candidate should cover the importance of consistency in evaluating models to ensure fair comparisons. They should mention how variability can lead to confusion in decision making and potentially flawed model selection."}, {"question": "Describe a real-world scenario where you had to choose between multiple evaluation metrics for a classification model. What considerations influenced your decision?", "response_guideline": "The candidate should provide a specific example, discussing the problem domain and how different metrics provided insights. They should explain the contextual considerations (e.g., business implications, error costs) that guided their metric choice."}, {"question": "Explain how you would implement evaluation metrics in a model validation pipeline. What steps would you take?", "response_guideline": "The answer should include steps like splitting the dataset into training and validation sets, determining which metrics to use, computing these metrics during validation, and summarizing the evaluation findings systematically."}, {"question": "What are the limitations of using accuracy as an evaluation metric for classification problems, particularly in certain practical applications?", "response_guideline": "A comprehensive answer would mention scenarios like imbalanced classes where accuracy fails to capture model performance. The candidate should also discuss why accuracy is not informative by itself, perhaps giving examples from domains like fraud detection or medical diagnosis."}, {"question": "What performance evaluation metric would you choose for an online recommendation system, and why?", "response_guideline": "Candidates should discuss metrics like precision@k, recall@k, or AUC, and explain the rationale based on the system's goals (e.g., user engagement, relevance). They should reflect on the need for different metrics based on user behavior expectations."}, {"question": "How can cross-validation help in evaluating classification models? Explain any inherent limitations of cross-validation as well.", "response_guideline": "A strong response will outline the role of cross-validation in providing a more robust estimate of model performance while noting limitations such as computational cost and potential data leakage."}]}