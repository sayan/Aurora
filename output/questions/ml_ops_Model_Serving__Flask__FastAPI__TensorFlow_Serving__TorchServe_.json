{
    "questions": [
        {
            "question": "1. Compare and contrast Flask and FastAPI when used for model serving. What are the key architectural differences, and how do these differences affect performance, scalability, and ease of integration with machine learning models?",
            "response_guideline": "A strong answer should discuss the synchronous vs. asynchronous natures, performance differences (e.g., FastAPI\u2019s support for async operations and automatic validation), ease of integration with Python type hinting, middleware support, and real-world use cases. The candidate should mention potential scalability challenges with Flask in production and FastAPI\u2019s advantages with modern async frameworks."
        },
        {
            "question": "2. TensorFlow Serving and TorchServe are popular frameworks for model deployment. How do their designs differ in terms of supporting their respective frameworks (TensorFlow and PyTorch)? Identify potential advantages and limitations each one presents when handling model versioning and scaling.",
            "response_guideline": "The candidate should cover particulars such as TensorFlow Serving\u2019s gRPC and RESTful API support, native integration with TensorFlow models, versioning capability, and GPU support, compared to TorchServe\u2019s custom handler capabilities, model archiving, and its suitability for PyTorch ecosystems. Discussion about limitations, such as less maturity in TorchServe or integration challenges, is expected."
        },
        {
            "question": "3. Imagine you need to deploy a real-time model with high throughput requirements using TensorFlow Serving. Describe an overall deployment architecture, including strategies for scaling, model versioning, monitoring, and failover. How would you mitigate potential bottlenecks?",
            "response_guideline": "A robust answer should outline a multi-tier architecture incorporating load balancing, container orchestration (e.g., Kubernetes), auto-scaling, and observability (logging, monitoring, metrics). Discussion should include strategies such as canary deployments, blue-green deployments for versioning transitions, and the handling of peak loads. The candidate should also consider network latency and bottlenecks."
        },
        {
            "question": "4. In a production setting, updating a model without downtime is critical. What pitfalls might you encounter when updating a model served by frameworks like TensorFlow Serving or TorchServe, and what strategies would you implement to ensure a smooth, zero-downtime rollout?",
            "response_guideline": "An effective answer should reference challenges like maintaining consistent API responses, handling in-flight requests during updates, potential compatibility issues between model versions, and the need for rollback mechanisms. The candidate should suggest approaches such as canary deployments, versioned APIs, and blue-green or A/B testing strategies."
        },
        {
            "question": "5. When serving a model via a Flask-based API, how would you handle the ingestion of messy or unstructured input data, and what pre-processing steps would you incorporate to ensure data integrity and reliability?",
            "response_guideline": "A good answer should include discussion on input validation, error handling, and data sanitization. The candidate should address techniques like schema validation (potentially using libraries like Pydantic, even though Flask does not natively support it), logging and anomaly detection, and contingencies for missing or inconsistent data in a real-world scenario."
        },
        {
            "question": "6. TorchServe allows for customization in model serving workflows. Explain the process of deploying a PyTorch model with TorchServe, detailing how you would integrate custom pre-processing and post-processing logic within the serving pipeline.",
            "response_guideline": "A complete answer should describe the process of model archiving (MAR files), configuring handlers, and implementing custom inference handlers that incorporate pre- and post-processing steps. The candidate should mention the importance of performance considerations, error handling, and ensuring that modifications do not break the overall serving pipeline."
        },
        {
            "question": "7. Security is crucial when exposing model serving endpoints. What security considerations should be taken into account when deploying model-serving applications using these frameworks, and what measures would you implement to safeguard against potential vulnerabilities?",
            "response_guideline": "A comprehensive answer should address endpoint authentication, authorization, rate limiting, data encryption in transit, vulnerability to injection attacks, and potential misconfigurations. The candidate should suggest the use of API gateways, regular security audits, container security practices, and compliance with standards when deploying both Flask/FastAPI and specialized serving systems like TensorFlow Serving and TorchServe."
        }
    ]
}