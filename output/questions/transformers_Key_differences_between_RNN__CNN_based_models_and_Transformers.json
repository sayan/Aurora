{
    "questions": [
        {
            "question": "1. Can you briefly explain the core architectural differences between RNNs, CNN-based models, and Transformers?",
            "response_guideline": "A good answer should cover the sequential processing of RNNs with inherent recurrence, the spatial invariance and local connectivity of CNNs, and the parallel self-attention mechanism in Transformers that dispenses with recurrence. It should also mention how these differences affect their ability to model dependencies in data."
        },
        {
            "question": "2. How do RNNs, CNNs, and Transformers handle long-range dependencies, and what are the potential pitfalls of each approach?",
            "response_guideline": "The answer should discuss the vanishing/exploding gradient problems in RNNs, the limited receptive fields in CNNs (and possible methods like dilated convolutions to alleviate them), and the capacity of Transformers to capture global context through self-attention while possibly incurring quadratic computational costs. It should also mention techniques to mitigate such issues (e.g., LSTM/GRU for RNNs, stacking layers or positional embeddings for Transformers)."
        },
        {
            "question": "3. Mathematically, how do the convolution operation in CNNs, recurrence in RNNs, and self-attention mechanisms in Transformers differ in terms of complexity and operation?",
            "response_guideline": "A strong answer should describe the local convolution filters in CNNs (usually O(n*k) per layer), the recurrent step-by-step update in RNNs (with sequential dependency leading to difficulties in parallelization), and the O(n^2) complexity of the self-attention operation in Transformers. It should also discuss the mathematical formulations behind each and the trade-offs between them."
        },
        {
            "question": "4. Explain the concept of 'inductive bias' in the context of these three architectures. How does each model\u2019s inductive bias influence its performance on different tasks?",
            "response_guideline": "The candidate should explain how RNNs are biased toward sequential data, CNNs emphasize locality and translation invariance, and Transformers use attention to allow flexible, context-dependent interactions. They should discuss how these biases can be advantageous or limiting based on the nature of the data and tasks (e.g., image recognition for CNNs, language modeling for Transformers)."
        },
        {
            "question": "5. In practical terms, how would you handle variable-length inputs across RNNs, CNNs, and Transformers, and what are the pitfalls associated with each?",
            "response_guideline": "The answer should mention mechanisms like padding and truncation for RNNs, fixed-size receptive fields and strides in CNNs, and the use of padding masks in Transformers. Emphasis should be placed on handling edge cases and ensuring that padding does not introduce artifacts, as well as the computational implications of these approaches."
        },
        {
            "question": "6. How do positional encodings in Transformers compare with the inherent sequential nature of RNNs and the local structure exploited by CNNs?",
            "response_guideline": "A quality response would explain that Transformers require explicit positional encodings to capture order since they process input in parallel, while RNNs naturally incorporate sequence order and CNNs capture local patterns but must rely on stacking layers (or using dilated convolutions) to encode wider context. The candidate should also critique the benefits and limitations."
        },
        {
            "question": "7. Discuss the training challenges associated with each of these models. How do issues like vanishing gradients, overfitting, or computational costs manifest in RNNs, CNNs, and Transformers?",
            "response_guideline": "A comprehensive answer should include details about vanishing gradients in RNNs (and methods like LSTM/GRU), overfitting risks in CNNs with extensive parameterization if not regularized, and the memory/computational challenges in Transformers due to self-attention's quadratic scaling. The answer should also consider strategies used to mitigate these challenges."
        },
        {
            "question": "8. Describe a scenario involving messy or noisy data where one of these architectures might fail, and propose a solution or hybrid approach to overcome the challenge.",
            "response_guideline": "The candidate should propose a specific real-world problem (e.g., time series forecasting with RNNs in noisy environments) and identify weaknesses such as error propagation. They should suggest possible hybrid solutions like combining CNN layers for feature extraction with Transformers for capturing long-range dependencies, or pre-processing techniques to mitigate data noise, demonstrating practical insight."
        },
        {
            "question": "9. How do these architectures differ in terms of scalability and deployment considerations, particularly in real-time systems?",
            "response_guideline": "A good answer should evaluate the scalability benefits of Transformers due to parallelizable computations versus the sequential dependencies in RNNs that can lead to latency in real-time deployments. It should also discuss how CNNs are often hardware-efficient due to localized operations, and bring up trade-offs like model size, throughput, and memory constraints during deployment."
        },
        {
            "question": "10. Can you provide an example where you might combine elements of CNNs, RNNs, and Transformers in a single model? What would be the advantages and potential issues of such a hybrid model?",
            "response_guideline": "The candidate should propose a combination architecture (for example, using CNNs for initial feature extraction, RNNs for encoding temporal information, and Transformers for capturing global dependencies). They should discuss the complementary strengths, challenges in integration, increased complexity, and potential benefits in terms of performance on complex tasks."
        },
        {
            "question": "11. How does the attention mechanism in Transformers help in interpretability of model predictions, and how does this compare to the interpretability challenges faced with RNNs and CNNs?",
            "response_guideline": "The answer should detail how attention weights can sometimes provide insights into which parts of the input are influential in a decision, contrasted with the black-box nature of RNN hidden states and the feature abstraction in CNN filters. Discussion should include limitations, such as potential misinterpretations of attention scores and cases where they might not fully explain model behavior."
        },
        {
            "question": "12. What recent innovations or modifications in any of these model families have significantly improved their performance on tasks requiring a deep understanding of context?",
            "response_guideline": "A strong answer might mention improvements like Transformer variants (e.g., efficient attention mechanisms, sparse attention), architectural improvements in RNNs (like better gating mechanisms), or novel CNN architectures designed for sequence analysis. The candidate should discuss why these improvements are effective and any trade-offs involved."
        }
    ]
}