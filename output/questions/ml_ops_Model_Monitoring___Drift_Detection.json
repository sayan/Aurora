{
    "questions": [
        {
            "question": "1. Can you explain the difference between data drift and concept drift? How would each impact model performance in a deployed environment?",
            "response_guideline": "A strong answer should define data drift as changes in the input data distribution and concept drift as changes in the relationship between inputs and target outputs. The response should explain potential impacts on metrics and model reliability, and discuss scenarios where drift might occur along with possible mitigation strategies."
        },
        {
            "question": "2. What key metrics and methods would you employ to monitor a model's performance over time in production? Discuss both statistical and business-relevant indicators.",
            "response_guideline": "The candidate should mention performance metrics such as accuracy, precision, recall, AUC, etc., and also discuss the importance of monitoring data distributions, prediction confidence, and error rates. They should relate these metrics to business KPIs and include practical aspects like alert thresholds, logging, and dashboards for continuous monitoring."
        },
        {
            "question": "3. Describe a methodology for detecting drift in incoming data distributions using statistical tests. For instance, how would you apply tests like the Kolmogorov-Smirnov test and what precautions would you take concerning sample size or false alarms?",
            "response_guideline": "A good answer will detail the use of statistical hypothesis tests like the Kolmogorov-Smirnov test for continuous variables, emphasizing proper selection of baseline distributions and the need for sufficient sample sizes. The answer should also cover issues such as the risk of false positives/negatives, multiple testing corrections, and the importance of understanding the context of the detected drift."
        },
        {
            "question": "4. Suppose your deployed model shows signs of performance degradation due to drift. How would you design an automated system to detect and respond to this drift, including triggering retraining or model rollback mechanisms? Consider deployment challenges in your answer.",
            "response_guideline": "The candidate should describe an end-to-end pipeline that includes: real-time monitoring, drift detection alerts, automated retraining pipelines, evaluation and validation of the new model, staged deployment (e.g., canary releases), and rollbacks. The answer should demonstrate an understanding of integrating monitoring tools into CI/CD pipelines, and handling edge cases like false drift detection or temporary fluctuations."
        },
        {
            "question": "5. In scenarios involving messy, streaming data, how would you approach real-time drift detection? What challenges might arise, and what strategies could you use to address data quality issues and ensure scalability?",
            "response_guideline": "A comprehensive answer should mention techniques for incremental or streaming-based drift detection, such as window-based statistics, online algorithms, and robust estimators. The response should include potential issues like noise, latency, and outlier sensitivity, and suggest strategies like data cleaning, smoothing, or adaptive window sizes. Scalability considerations such as computational overhead and resource management in a streaming context should also be discussed."
        }
    ]
}