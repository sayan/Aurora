{
    "questions": [
        {
            "question": "1. What are the key performance metrics commonly used for classification and regression models in production, and what are the trade-offs associated with each metric?",
            "response_guideline": "A strong answer should cover metrics such as accuracy, precision, recall, F1 score, ROC AUC for classification and mean squared error, mean absolute error for regression. The candidate should discuss the limitations of each metric, for instance how accuracy can be misleading in imbalanced datasets, and should reference the need to select metrics that align well with the business objectives and real-world impact."
        },
        {
            "question": "2. How do you monitor and maintain model performance once a model is deployed in production? Specifically, what methods would you use to detect concept drift or performance degradation?",
            "response_guideline": "The candidate should mention techniques such as setting up continuous monitoring systems, statistical tests to detect distributional changes, and performance alerts. They might also discuss using techniques like rolling windows for evaluation, A/B testing, champion/challenger models, and the importance of having a feedback loop for updating metrics when notion drift is detected."
        },
        {
            "question": "3. Explain the mathematical relationship between precision, recall, and the F1 score. Under what production scenarios might you choose to optimize for one metric over the others?",
            "response_guideline": "A good answer would derive the formula for the F1 score as the harmonic mean of precision and recall, showing how these metrics are interdependent. The candidate should discuss scenarios where, for instance, high precision is prioritized (like in fraud detection to avoid false positives) versus cases where high recall is critical (such as in medical screening to minimize false negatives)."
        },
        {
            "question": "4. In production, real-world data is often messy and may not follow the same distribution as the training data. What potential pitfalls could arise when interpreting conventional performance metrics under these circumstances, and how would you adjust your evaluation strategy?",
            "response_guideline": "The answer should acknowledge issues such as data noise, label errors, and distribution shifts leading to biases in metrics. The candidate should suggest strategies like implementing robust data preprocessing pipelines, recalibrating models, using techniques such as domain adaptation, boosting metric robustness through confidence intervals, or even employing unsupervised methods to detect anomalies."
        },
        {
            "question": "5. How would you design a scalable and robust system to track and report model performance metrics in real-time for production-level machine learning models? What challenges do you anticipate in such a system?",
            "response_guideline": "A comprehensive answer should cover architectural considerations like using streaming data platforms (e.g., Kafka, Flink), real-time dashboards, and batch processing for back-filling metrics. The candidate should be aware of challenges such as latency, data ingestion variability, computation overhead, and ensuring data integrity and consistency under high load, while also discussing how to minimize performance impact on the production system."
        },
        {
            "question": "6. If the production data distribution shifts significantly from what was seen during training, how might standard performance metrics fail to accurately reflect a model's effectiveness? What alternative strategies might be employed to address this challenge?",
            "response_guideline": "The candidate should mention that conventional metrics may no longer be reliable indicators due to concept drift, stale benchmarks, or an evolving data landscape. They should discuss strategies like periodic model retraining, recalibrating thresholds, monitoring with additional metrics (e.g., population stability index or PSI), and incorporating domain-specific performance indicators to capture the true effectiveness of the model over time."
        }
    ]
}