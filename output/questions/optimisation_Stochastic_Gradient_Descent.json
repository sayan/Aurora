{
    "questions": [
        {
            "question": "1. Can you explain the core idea behind Stochastic Gradient Descent (SGD) and outline the main differences between SGD and Batch Gradient Descent?",
            "response_guideline": "A strong answer should start by describing the concept of updating parameters using only one (or a few) samples at a time, explaining the trade-offs in terms of computational cost versus variance in the updates. The candidate should also contrast this with Batch Gradient Descent, discussing convergence properties, computational expense in large datasets, and how noise in gradient estimates can sometimes help escape local minima."
        },
        {
            "question": "2. How do the choice of learning rate and batch size affect the convergence properties of SGD? What strategies would you recommend for tuning these hyperparameters?",
            "response_guideline": "Look for an explanation that covers the impact of a large vs. small learning rate, including potential divergence or slow convergence, and how batch size controls the variance of gradient estimates. The response should mention methods such as learning rate decay, adaptive learning rates (e.g., AdaGrad, RMSProp), and the use of mini-batching to balance computational efficiency with gradient variance. Mention any mathematical intuition behind convergence rates and stability considerations."
        },
        {
            "question": "3. Derive, at a high level, the expectation and variance of the gradient estimate in SGD. How do these statistical properties influence the convergence behavior of the algorithm?",
            "response_guideline": "A competent candidate should derive that the stochastic gradient is an unbiased estimator of the full gradient, while its variance is determined by the variability of the data samples. They should discuss how a high variance can slow down convergence or cause oscillations, and identify the trade-offs in reducing the variance by perhaps increasing the batch size. Mathematical reasoning and an understanding of these properties in the context of convergence analysis are essential."
        },
        {
            "question": "4. Discuss the role of momentum in SGD. How do classical momentum and Nesterov Accelerated Gradient differ, and in what scenarios might one be preferred over the other?",
            "response_guideline": "An ideal answer would explain that momentum helps smooth out noisy updates and accelerates convergence by accumulating past gradients. The candidate should describe classical momentum, which uses a running average of past gradients, and Nesterov accelerated gradient, which anticipates the future position of the parameters before applying the correction. They should also mention scenarios or properties of the loss landscape (e.g., smooth vs. oscillatory) that may favor one approach over the other."
        },
        {
            "question": "5. In a real-world setting with high-dimensional, noisy, and potentially imbalanced data, how would you adapt or extend traditional SGD to handle issues such as scaling, robustness, and convergence reliability?",
            "response_guideline": "Look for references to strategies such as data preprocessing, normalization, or batch normalization to handle noise and imbalance. The candidate could mention using adaptive learning rate methods (like Adam, Adagrad, or RMSProp) to better cope with high-dimensional data, techniques for variance reduction, and distributed SGD for scalability. They should also address monitoring convergence in practice and potential algorithmic modifications to improve robustness."
        },
        {
            "question": "6. What common pitfalls might one encounter when using SGD, such as dealing with local minima, saddle points, or unstable gradients? What techniques or modifications can be applied to mitigate these issues?",
            "response_guideline": "The answer should include awareness of local minima and saddle points, and how SGD's inherent noise sometimes helps, but can also hinder the progress. Look for discussion on techniques like learning rate scheduling, momentum, gradient clipping, and the use of advanced optimization variants. The candidate should also address potential pitfalls in real-world deployment, including dealing with non-stationary data distributions and proper initialization strategies."
        }
    ]
}