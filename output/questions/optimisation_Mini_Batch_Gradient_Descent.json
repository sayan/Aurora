{
    "questions": [
        {
            "question": "1. Explain the differences between full batch gradient descent, stochastic gradient descent (SGD), and mini-batch gradient descent. What are the trade-offs of using mini-batch gradient descent in terms of convergence speed, computational efficiency, and gradient noise?",
            "response_guideline": "A strong answer should compare the three methods, discussing computational cost (full batch is expensive, SGD is fast but noisy, mini-batch finds a balance), variance of gradient estimates, hardware parallelism benefits, and how mini-batch size affects the overall convergence behavior. Look for mention of noise regularization effects and mini-batch variance."
        },
        {
            "question": "2. How does the choice of mini-batch size influence the convergence properties and stability of the optimization process? Include a discussion on the mathematical implications such as variance reduction and estimation bias.",
            "response_guideline": "The candidate should explain that smaller batches lead to higher gradient variance (introducing noise which can help escape local optima) while larger batches provide a more accurate gradient estimate but at the cost of computational resources. Expect discussion on the trade-off between bias and variance, law of large numbers, and potential impacts on learning rate selection and convergence behavior."
        },
        {
            "question": "3. Derive or outline the implementation of mini-batch gradient descent when combined with momentum. What potential pitfalls can arise in non-convex optimization scenarios and how might these be mitigated?",
            "response_guideline": "A good answer should detail the momentum update rule along with mini-batch updates\u2014explaining how the moving average of gradients is maintained. The candidate should discuss potential issues such as overshooting, sensitivity to mini-batch noise in non-convex settings, and strategies like learning rate decay or adaptive momentum tuning to mitigate these issues. Mathematical clarity in the derivation and pitfalls identification is key."
        },
        {
            "question": "4. In a real-world scenario where the dataset is very large and stored on disk (or a distributed system) with messy, unstructured data, how would you efficiently implement mini-batch gradient descent? Consider data pipeline design, scalability, and deployment.",
            "response_guideline": "The candidate should illustrate an approach to load and preprocess data in a streaming or batched manner (e.g., using data generators or frameworks like TensorFlow Data API). They should discuss handling data cleaning on the fly, minimizing I/O overhead, and ensuring that mini-batches are representative despite messiness. Discussion of parallelism, use of distributed computing frameworks, and real-world deployment challenges is expected."
        },
        {
            "question": "5. What are some challenges when using extremely small mini-batch sizes (e.g., 1 or 2 samples) in training deep neural networks, particularly in the context of noisy gradients? How might you address these challenges in practice?",
            "response_guideline": "A good answer should address that very small batch sizes can introduce significant gradient noise, leading to unstable training and slow convergence. The candidate should discuss techniques such as gradient averaging, use of learning rate adjustments, batch normalization, or adaptive optimizers (e.g., Adam) to counteract this issue. Consideration of trade-offs between noise-induced exploration and instability is important."
        }
    ]
}