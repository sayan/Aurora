{
    "questions": [
        {
            "question": "1. What is the intuition behind Masked Language Modeling (MLM) in pretraining, and why is it particularly effective for learning contextualized representations?",
            "response_guideline": "A strong answer will describe the concept of randomly masking tokens in input sequences, forcing the model to predict missing words using context. It should include discussion on how MLM fosters deep bidirectional understanding and mention trade-offs such as the difficulty in modeling long-range dependencies."
        },
        {
            "question": "2. Can you explain the Next Sentence Prediction (NSP) objective used in earlier transformer models, and point out its potential limitations in certain applications?",
            "response_guideline": "The candidate should explain how NSP predicts the sequential relationship between sentences and discuss scenarios where this task might not capture true discourse (e.g., in languages or domains with ambiguous sentence boundaries). Pitfalls such as insufficient signal for long-range context or overfitting to shallow patterns should be mentioned."
        },
        {
            "question": "3. How does MLM differ from Causal or Autoregressive Language Modeling in terms of training objectives and downstream performance?",
            "response_guideline": "A good answer compares the non-autoregressive nature of MLM with the sequential prediction used in autoregressive models, highlighting strengths and limitations of each approach in capturing bidirectional versus unidirectional context. It should address implications on generation tasks and representation learning quality."
        },
        {
            "question": "4. Discuss the mathematical formulation of the masked language modeling objective. How is the loss computed over the masked tokens, and why is this formulation effective?",
            "response_guideline": "The ideal response should lay out the loss function typically used (e.g., cross-entropy loss computed only on the masked tokens), mention model probabilities, and describe how the sum over the masked positions is performed. An understanding of backpropagation and efficiency in handling variable masking is also useful."
        },
        {
            "question": "5. Random masking can introduce inconsistencies during training. What are some of the challenges associated with random mask selection, and what strategies can be employed to mitigate these effects?",
            "response_guideline": "A robust answer will identify issues such as the potential for the model to overfit certain patterns or ignore unmasked contexts. It should discuss techniques like dynamic masking, increased mask randomness per epoch, and alternative sampling strategies to reduce bias and improve training stability."
        },
        {
            "question": "6. Newer models sometimes replace NSP with objectives like sentence order prediction (SOP). Why might the SOP objective be preferred over NSP in some contexts?",
            "response_guideline": "An effective answer should compare NSP and SOP, explaining that SOP might better capture discourse coherence by focusing on the order of sentences rather than just their association. Mention potential benefits in capturing finer-grained inter-sentence dependencies and any empirical performance improvements."
        },
        {
            "question": "7. Pretraining objectives used during training are sometimes not well-aligned with the tasks encountered during fine-tuning. How would you address this mismatch, particularly in the context of MLM?",
            "response_guideline": "Look for answers discussing the training-inference mismatch due to masking. A good candidate might suggest solutions like dynamic masking during fine-tuning, data augmentation, or adaptive pretraining strategies that better align pretraining tasks with downstream objectives."
        },
        {
            "question": "8. How would you adapt pretraining strategies, including MLM and NSP, when dealing with extremely long documents or contexts that exceed typical transformer input lengths?",
            "response_guideline": "A sound answer would discuss strategies such as chunking or sliding windows, hierarchical modeling, or using memory-augmented architectures to handle longer contexts. Considerations about maintaining dependencies and ensuring computational efficiency should be highlighted."
        },
        {
            "question": "9. In settings with noisy or domain-specific text (e.g., medical records or informal social media), what modifications to pretraining objectives would you consider to ensure robust performance?",
            "response_guideline": "The answer should explore domain adaptation techniques such as fine-tuning on cleaned or augmented data, adjusting masking strategies to account for domain-specific vocabulary, and possibly incorporating denoising objectives. Addressing robustness and handling data heterogeneity is key."
        },
        {
            "question": "10. Scalability is a major challenge in pretraining large transformer models. Can you discuss the challenges associated with scaling pretraining objectives like MLM, and what distributed training techniques might be employed?",
            "response_guideline": "The candidate should mention both computational and communication bottlenecks (e.g., handling massive datasets, large batch sizes, and gradient synchronization). Expected answers include discussion about data parallelism, model parallelism, pipeline parallelism, and efficient gradient aggregation."
        },
        {
            "question": "11. How do the design choices in masking strategy (e.g., fixed mask probability versus adaptive masking) affect the learning dynamics and convergence of a model during pretraining?",
            "response_guideline": "A strong answer should touch upon how different masking strategies can influence the difficulty of the learning task, potentially speeding up or slowing down convergence. The candidate might discuss empirical observations or theoretical reasoning behind adaptive masking techniques."
        },
        {
            "question": "12. Can you design an alternative pretraining objective that addresses one of the drawbacks of existing objectives like MLM or NSP? Describe your proposed objective and the trade-offs involved.",
            "response_guideline": "The answer should showcase creativity and deep understanding. Expect a clear explanation of a novel objective, discussion of its theoretical benefits over existing methods, and an analysis of possible pitfalls or trade-offs (e.g., increased computational requirements or new forms of bias)."
        },
        {
            "question": "13. In real-world deployment of models pretrained with these objectives, how would you handle the challenge of unexpected or messy input data, particularly in the context of masking mismatches or corrupted sequences?",
            "response_guideline": "This answer should include strategies for data cleaning, robust error handling, and possibly online learning or fine-tuning post deployment. Emphasis should be placed on ensuring that the pretraining objectives still translate into effective performance when confronted with non-ideal data conditions."
        }
    ]
}