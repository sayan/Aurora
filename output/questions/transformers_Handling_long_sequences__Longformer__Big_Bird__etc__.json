{
    "questions": [
        {
            "question": "1. Can you explain the primary challenges associated with handling long sequences in transformer-based architectures, particularly focusing on the quadratic complexity of self-attention?",
            "response_guideline": "A good answer should detail how standard transformers compute attention with quadratic complexity, leading to memory and computational inefficiencies as sequence length increases. The candidate should mention trade-offs and the need for sparse methods, and briefly introduce alternatives like Longformer and Big Bird as solutions."
        },
        {
            "question": "2. How do sparse attention mechanisms in models like Longformer and Big Bird mitigate the computational challenges of long sequences?",
            "response_guideline": "The answer should cover different sparse attention strategies such as local windowed attention, global attention, and random attention. It should compare how these mechanisms reduce computation and maintain contextual awareness, highlighting trade-offs in complexity and coverage."
        },
        {
            "question": "3. Can you discuss the key differences between Longformer and Big Bird in terms of their attention mechanisms and scalability?",
            "response_guideline": "A comprehensive answer should outline the specific attention patterns implemented by each model. For example, mention how Longformer uses a combination of local windowed attention and global tokens, while Big Bird combines random, sparse, and global attention, and discuss how these approaches impact performance and scalability."
        },
        {
            "question": "4. Describe the potential pitfalls or edge cases that might arise when applying sparse attention methods to datasets with long sequences. How would you diagnose and address these?",
            "response_guideline": "Look for identification of issues such as loss of long-distance dependencies, difficulty in capturing global context, and potential for introducing bias due to fixed attention patterns. The candidate should discuss diagnostic techniques (e.g., ablation studies, attention visualization) and propose mitigation strategies such as dynamic attention adjustments or hybrid models."
        },
        {
            "question": "5. In practical applications, data is often messy and sequences might have highly variable lengths. How would you design a preprocessing pipeline for a model like Big Bird to handle such real-world challenges?",
            "response_guideline": "The answer should cover strategies for handling variable-length sequences, including padding, truncation, or segmentation. Discussion should include normalization, anomaly detection in sequence lengths, batching strategies, and possibly adaptive attention masks to ensure robustness during training and inference."
        },
        {
            "question": "6. Could you mathematically detail how the computational complexity changes when using sparse attention compared to full attention in transformers?",
            "response_guideline": "Expect a clear articulation of the reduction from O(n^2) complexity for full self-attention to something significantly lower (e.g., O(n) or O(n log n)) for sparse alternatives. The answer should include a mathematical breakdown, possibly referencing the attention mask structure and its impact on computation."
        },
        {
            "question": "7. How might the choice of positional encodings differ or need modification when working with long sequences in models like Longformer and Big Bird?",
            "response_guideline": "A well-rounded answer would discuss the limitations of standard sinusoidal or learned embeddings when applied to long contexts, and mention alternate strategies such as relative positional encodings or modifications that can better capture long-range dependencies. Mention any trade-offs in model performance or generalization."
        },
        {
            "question": "8. Describe a scenario where you might prefer using a model designed for long sequences over a standard transformer. What factors would influence your decision?",
            "response_guideline": "Look for an answer that identifies scenarios such as document-level summarization, legal document analysis, or genomic data processing. The answer should weigh factors like sequence length, memory constraints, latency, and the need for capturing long dependencies, while considering trade-offs in model complexity."
        },
        {
            "question": "9. How do models like Longformer and Big Bird handle the challenge of retaining global context while using sparse attention? Provide an example of how global tokens are integrated.",
            "response_guideline": "The answer should explain the concept of global tokens (or global attention) and how certain tokens are allowed to attend to all others and vice versa. An example should illustrate the mechanism, such as marking certain positions (e.g., CLS token or keyword markers) to ensure global context is captured, discussing how this benefits tasks like classification or summarization."
        },
        {
            "question": "10. What are some deployment considerations when using models like Longformer or Big Bird in a production environment, particularly with respect to latency and hardware requirements?",
            "response_guideline": "Candidates should discuss challenges related to inference speed with long sequences, memory footprint, batching strategies, and potential need for customized hardware acceleration or model quantization. They should also address how to manage real-time versus batch processing scenarios and robustness to variable input sizes."
        },
        {
            "question": "11. Discuss how attention visualization tools can assist in debugging or improving models that handle long sequences. What specific indicators would you look for?",
            "response_guideline": "Expect the answer to mention tools and techniques for visualizing attention maps. The candidate should discuss what to look for in these visualizations (e.g., whether key tokens receive adequate attention, identifying patterns in misallocation, and understanding drop-off in long-range dependencies) and how these insights can guide model improvements."
        },
        {
            "question": "12. Explain how you would approach an experiment to compare the performance of a traditional transformer, Longformer, and Big Bird on a long-document classification task. What metrics and evaluation techniques would you employ?",
            "response_guideline": "The answer should include designing controlled experiments with consistent hyperparameters, discussing evaluation metrics like accuracy, F1, and possibly memory/latency metrics. It should also consider qualitative evaluation of attention patterns and error analysis, ensuring that the comparisons are fair and comprehensive."
        },
        {
            "question": "13. How would you integrate domain-specific knowledge into a long-sequence model? For example, adjusting tokenization strategies or attention patterns when processing specialized texts such as legal or medical documents.",
            "response_guideline": "A strong answer should discuss customizing the tokenization process (handling jargon, abbreviations, or complex terminology) and possibly modifying attention mechanisms (e.g., via custom global tokens) to incorporate domain-specific signals. It should also cover how to fine-tune pre-trained models on a domain-specific corpus to improve performance."
        }
    ]
}