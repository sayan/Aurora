{
    "questions": [
        {
            "question": "1. Can you describe the overall architecture of the Encoder-Decoder Transformer? What are the primary responsibilities of the encoder and the decoder in this setup?",
            "response_guideline": "A good answer should include a high-level overview of the Transformer model, detailing that the encoder processes the input sequence into a contextual representation and the decoder uses this representation along with previously generated tokens to produce the output sequence. Mention key modules such as multi-head self-attention, encoder-decoder attention, feed-forward layers, and the use of positional encodings."
        },
        {
            "question": "2. What role does multi-head self-attention play in both the encoder and decoder? How does the masked self-attention in the decoder differ from that in the encoder?",
            "response_guideline": "The answer should cover that multi-head self-attention allows the model to focus on different positions of the input sequence simultaneously. For the decoder, the masked self-attention ensures that predictions for a given position only depend on the known outputs at previous positions, preventing information leakage during training. Candidates should explain the mathematical mechanism and its importance for autoregressive generation."
        },
        {
            "question": "3. How does the Encoder-Decoder Transformer manage variable-length input and output sequences? What is the importance of positional encoding in this context?",
            "response_guideline": "An ideal response should mention that the attention mechanism and positional encoding enable the Transformer to deal with sequences of varying lengths by incorporating sequence order information. The candidate should articulate how sine/cosine positional encodings (or learned positional embeddings) are added to token embeddings to inform the model about the position of a token in the sequence."
        },
        {
            "question": "4. Explain the use of residual connections (skip connections) and layer normalization within the architecture. Are there differences in how these mechanisms are applied in the encoder versus the decoder?",
            "response_guideline": "The candidate should describe that residual connections help ease training by mitigating the vanishing gradient problem, while layer normalization standardizes the inputs to sub-layers. They should note any subtle differences between the encoder and decoder implementations where, for instance, additional normalization may be applied after the combined attention layers in the decoder."
        },
        {
            "question": "5. Provide a mathematical explanation of the attention mechanism in Transformers. Specifically, detail how the queries, keys, and values interact in both the encoder and decoder modules.",
            "response_guideline": "An excellent answer would include the mathematical formula for scaled dot-product attention (Attention(Q, K, V) = softmax((QK^T)/\u221a(d_k))V) and explain how queries, keys, and values are derived from the input embeddings. The candidate should discuss how this mechanism is integrated into both the self-attention in the encoder and masked self-attention and encoder-decoder attention in the decoder."
        },
        {
            "question": "6. What masking strategies are implemented in the Transformer\u2019s architecture, and why are these masks necessary for effective decoder functioning?",
            "response_guideline": "A complete answer should cover the two primary types of masks: padding masks (used in both encoder and decoder to ignore padded tokens) and look-ahead (or causal) masks (used in the decoder for autoregressive prediction). The candidate should explain how these masks prevent information leakage and assist in proper handling of variable-length sequences."
        },
        {
            "question": "7. How does the encoder-decoder structure assist in tasks like machine translation compared to simpler architectures? What unique challenges does it pose in training and inference?",
            "response_guideline": "The candidate should discuss that the encoder-decoder architecture allows the model to explicitly learn a mapping between source and target languages, capturing complex relationships and dependencies. Highlight potential challenges such as exposure bias during training, handling long-range dependencies, and balance between encoding source context and generating coherent target sequences."
        },
        {
            "question": "8. Consider a real-world deployment scenario, such as translating documents in a low-resource language. What strategies might you adopt to handle noisy or messy data, and how would you ensure scalability and low latency?",
            "response_guideline": "A strong response should cover data preprocessing techniques (e.g., normalization, cleaning), transfer learning and fine-tuning on specific domain data, and model optimization strategies such as quantization, distillation, or efficient serving infrastructures. The candidate should discuss strategies for scalability (like distributed inference) and robustness measures to manage noisy real-world inputs."
        },
        {
            "question": "9. How can the standard Encoder-Decoder Transformer architecture be adapted for tasks beyond sequence-to-sequence, such as summarization or question answering?",
            "response_guideline": "The answer should note that while the architecture was originally designed for machine translation, modifications like task-specific pre-training, alterations in the attention mechanisms, or incorporation of additional modules (e.g., pointer networks for summarization) allow it to be applied to various domains. The candidate should also mention fine-tuning strategies and handling domain-specific context."
        },
        {
            "question": "10. Discuss the trade-offs between scaling the depth (number of layers) versus the width (model dimensions or number of attention heads) in an Encoder-Decoder Transformer. What are the implications for training stability and performance?",
            "response_guideline": "A thorough answer would compare increased depth which can model more complex hierarchical representations against increased width that allows more diverse feature representation. The answer should address training stability, computational cost, overfitting risks, and potential techniques to mitigate these issues (like learning rate adjustments, layer normalization tuning, or using residual connections effectively)."
        },
        {
            "question": "11. What are some potential pitfalls or edge cases that might arise during the training of an Encoder-Decoder Transformer on multilingual datasets, and how might you address them?",
            "response_guideline": "An ideal answer would mention issues like class imbalance, vocabulary mismatches, and the risk of overfitting to dominant languages. Strategies might include using shared sub-word tokenization (like Byte Pair Encoding), data augmentation, language-specific layers, and careful sampling techniques to ensure balanced training across languages."
        },
        {
            "question": "12. How would you modify the Transformer\u2019s Encoder-Decoder structure to accommodate multimodal inputs (e.g., combining image and text information) for tasks such as image captioning?",
            "response_guideline": "The candidate should suggest architectural extensions such as separate encoders for each modality and an integration mechanism (like cross-modal attention) to fuse the features. They should discuss challenges like aligning representations from different modalities and the need for specialized positional encodings or embedding strategies for non-text data."
        }
    ]
}