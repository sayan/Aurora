{
    "questions": [
        {
            "question": "1. Can you explain the basic architecture of GANs, specifically detailing the roles of the generator and the discriminator, and how they interact during training?",
            "response_guideline": "A good answer should cover the adversarial setup, the purpose of the generator and discriminator, and the iterative training process. The candidate should highlight how the generator produces samples to fool the discriminator while the discriminator learns to distinguish real from fake data, setting the stage for a minimax game."
        },
        {
            "question": "2. What are the common challenges encountered when training GANs? Please discuss issues such as mode collapse, training instability, and non-convergence, and suggest strategies to mitigate these problems.",
            "response_guideline": "The ideal response should identify and define issues like mode collapse where the generator produces limited variety, difficulties with balancing the learning of both networks, and unstable gradients. The candidate should suggest techniques like using feature matching, mini-batch discrimination, gradient penalties, and architectural tweaks to help stabilize training."
        },
        {
            "question": "3. Describe the different loss functions commonly used in GANs, such as the minimax loss, Wasserstein loss, and least squares loss. How do these loss formulations impact the training dynamics and convergence behavior of GANs?",
            "response_guideline": "A comprehensive answer should outline the mathematical formulations of these loss functions and their implications. The candidate should explain how the standard minimax loss can suffer from gradient vanishing, why Wasserstein loss provides more stable gradients by approximating the earth mover's distance, and how least squares loss can smooth out decision boundaries. Discussion on trade-offs and empirical observations is expected."
        },
        {
            "question": "4. In real-world applications, data can be noisy and high-dimensional. How would you modify a GAN to effectively learn from such messy data and ensure scalability? Please detail changes in data preprocessing, model architecture, and training strategies.",
            "response_guideline": "The candidate should talk about effective data preprocessing techniques and noise handling methods, such as normalization, augmentation, or dimensionality reduction. They should also discuss architectural modifications like using convolutional layers for high-dimensional image data, attention mechanisms, or progressive growing. Strategies like using distributed training frameworks, careful hyperparameter tuning, and robust loss functions to scale GANs should be mentioned."
        },
        {
            "question": "5. What are some recent advancements in GAN research aimed at improving convergence and mitigating mode collapse? Can you provide examples of novel techniques or architectures that address these issues?",
            "response_guideline": "A well-informed answer should reference state-of-the-art methods such as Progressive GANs, StyleGAN, self-attention GANs, and techniques involving spectral normalization or Two Time-Scale Update Rule (TTUR). The candidate should discuss how these innovations improve training stability, sample diversity, and convergence speed, and mention their applicability to various domains."
        },
        {
            "question": "6. Discuss the theoretical underpinnings and limitations of GANs. Are there any formal convergence guarantees, and under what conditions might these theoretical properties break down?",
            "response_guideline": "The answer should include discussions about the game-theoretical framework underlying GANs, such as Nash equilibria. The candidate should note that while theoretical guarantees exist under idealized assumptions, practical conditions (e.g., network capacity, non-convexity, finite data, and computation) often lead to issues. A nuanced discussion about the divergence between theory and practice, as well as potential research directions, is expected."
        }
    ]
}