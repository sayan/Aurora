{
    "questions": [
        {
            "question": "1. What is the role of ML pipelines in production machine learning systems, and why are tools like Airflow and Kubeflow critical for managing these pipelines?",
            "response_guideline": "A strong answer should cover how ML pipelines help in structuring, automating, tracking, and scaling ML workflows. The candidate should mention components such as data ingestion, preprocessing, training, evaluation, and deployment. They should also discuss how Airflow and Kubeflow aid in orchestration and reproducibility, and possibly point out challenges in production environments such as dependency management and version control."
        },
        {
            "question": "2. Compare and contrast Apache Airflow and Kubeflow Pipelines in terms of their design, scheduling, and orchestration capabilities. In what scenarios would you choose one over the other?",
            "response_guideline": "The answer should describe the fundamental differences between Airflow and Kubeflow, such as Airflow\u2019s DAG-based scheduling versus Kubeflow\u2019s focus on containerized and Kubernetes-native workflows. It should also address use case scenarios: Airflow for general-purpose scheduling and task orchestration across various domains, and Kubeflow for ML-specific pipelines where integration with Kubernetes and ML model management is essential. Look for insights on scalability, ease of integration with ML frameworks, and operational considerations."
        },
        {
            "question": "3. Describe how dependency management and execution scheduling work in Apache Airflow. How would you design your DAG to handle task failures, retries, and ensure idempotency in tasks?",
            "response_guideline": "A good response should detail Airflow\u2019s directed acyclic graph (DAG) structure, task dependencies, and scheduling features like retries, timeouts, and SLA monitoring. The candidate should talk about designing idempotent tasks (ensuring the same task can be safely retried), implementing error handling mechanisms, using sensors and hooks, and the importance of logging and alerting in a robust pipeline."
        },
        {
            "question": "4. In Kubeflow Pipelines, how do you integrate hyperparameter tuning and model versioning within your pipeline? What design patterns or tools would you leverage?",
            "response_guideline": "The candidate should mention integrating hyperparameter tuning tools such as Katib within Kubeflow to automate optimization. For model versioning, discussion should include the use of artifact repositories, metadata tracking, and pipeline steps that produce checkpoints. Solid answers would cover design patterns like modular pipeline steps, parameterization of workflow components, and ensuring reproducibility through containerization and metadata logging."
        },
        {
            "question": "5. Consider a real-world scenario where an ML pipeline running on Airflow frequently encounters timeouts and data inconsistencies. How would you diagnose, debug, and address these issues?",
            "response_guideline": "Look for a systematic troubleshooting process in the candidate\u2019s answer. They should discuss reviewing logs, setting up alert systems, validating data inputs, and handling timeouts through retries or adjusting DAG scheduling parameters. The answer should also explore the potential need for optimizing performance (e.g., reducing task durations or parallelizing workloads), handling data validation upfront, and ensuring robust error-handling mechanisms are in place."
        },
        {
            "question": "6. How would you design an ML pipeline that is both scalable and maintainable, taking into account messy data inputs, dependency conflicts, and version control challenges? Illustrate your approach using features from either Airflow or Kubeflow.",
            "response_guideline": "A thorough answer should explain the importance of modular, loosely coupled design for pipeline components to address scalability and maintenance. The candidate should detail steps such as using containerized tasks, version control for both code and data, schema validation, and using environment isolation (e.g., virtual environments or Docker). They should highlight how Airflow or Kubeflow\u2019s orchestration capabilities and integration with versioning systems or metadata tracking (like ML Metadata) help manage these challenges."
        },
        {
            "question": "7. What best practices would you implement to ensure that your ML pipeline is reproducible, secure, and resilient in a multi-tenant environment when using Kubeflow?",
            "response_guideline": "The candidate should mention practices such as containerizing pipeline components, implementing resource quotas, using RBAC for access control, and detailed logging for audit trails. Reproducibility strategies may include using consistent environments (via Docker), rigorous version control of code and data, and metadata tracking. The answer should also mention how to manage multi-tenancy through isolation, namespace management, and monitoring to ensure resilience."
        }
    ]
}