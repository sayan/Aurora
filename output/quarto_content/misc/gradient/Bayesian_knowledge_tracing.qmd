---
title: "Bayesian Knowledge Tracing: A Mathematical Overview"
Question: "What is Bayesian Knowledge Tracing (BKT) and how does it work?"
format: 
  html:
    toc: true
    number-sections: true
date: "2025-02-13"
---

# Introduction

Bayesian Knowledge Tracing (BKT) is a probabilistic framework for modeling student knowledge as they interact with educational content. The model tracks a student's mastery of individual skills using Hidden Markov Models, where the hidden state represents the student's knowledge, and observations are their responses (correct/incorrect) to questions.

# Mathematical Framework

## Core Parameters

Let's define the key parameters that govern the BKT model:

- $P(L_0)$: Prior probability of initially knowing the skill
- $P(T)$: Probability of transitioning from not knowing to knowing (Learning Rate)
- $P(G)$: Probability of guessing correctly without knowing (Guess Rate)
- $P(S)$: Probability of making a mistake despite knowing (Slip Rate)

## Update Equations

The BKT model uses Bayes' theorem to update beliefs about student knowledge. Let $P(L_t)$ be the probability of knowing the skill at time $t$.

### Update After Correct Answer

When a student answers correctly:

$$P(L_t|correct) = \frac{P(L_t)(1-P(S))}{P(L_t)(1-P(S)) + (1-P(L_t))P(G)}$$

### Update After Incorrect Answer

When a student answers incorrectly:

$$P(L_t|incorrect) = \frac{P(L_t)P(S)}{P(L_t)P(S) + (1-P(L_t))(1-P(G))}$$

### Learning Update

After each observation, we account for potential learning:

$$P(L_{t+1}) = P(L_t|observation) + (1-P(L_t|observation))P(T)$$

# Worked Example

Let's walk through a concrete example of updating a student's knowledge state.

## Initial Parameters

Consider a student learning fraction addition with the following parameters:

- $P(L_0) = 0.2$ (20% chance of initial knowledge)
- $P(T) = 0.3$ (30% learning rate)
- $P(G) = 0.2$ (20% guess probability)
- $P(S) = 0.1$ (10% slip probability)

## Step-by-Step Updates

### Step 1: Student Answers Correctly

Initial knowledge state: $P(L_1) = 0.2$

Update after correct answer:
$$\begin{align*}
P(L_1|correct) &= \frac{0.2(1-0.1)}{0.2(1-0.1) + (1-0.2)0.2} \\
&= \frac{0.18}{0.18 + 0.16} \\
&\approx 0.529
\end{align*}$$

Learning update:
$$\begin{align*}
P(L_2) &= 0.529 + (1-0.529)0.3 \\
&\approx 0.670
\end{align*}$$

### Step 2: Student Answers Incorrectly

Starting from $P(L_2) = 0.670$

Update after incorrect answer:
$$\begin{align*}
P(L_2|incorrect) &= \frac{0.670(0.1)}{0.670(0.1) + (1-0.670)(1-0.2)} \\
&= \frac{0.067}{0.067 + 0.264} \\
&\approx 0.203
\end{align*}$$

Learning update:
$$\begin{align*}
P(L_3) &= 0.203 + (1-0.203)0.3 \\
&\approx 0.442
\end{align*}$$

# Parameter Properties

## Fixed Parameters

In standard BKT, $P(G)$, $P(S)$, and $P(T)$ are treated as constants for a given skill. These parameters:

1. Represent general characteristics of the skill
2. Are estimated from population data
3. Remain fixed during individual student interactions

## Parameter Estimation

The parameters are typically estimated using the Expectation-Maximization (EM) algorithm:

1. E-Step: Calculate posterior probabilities of knowledge states
2. M-Step: Update parameter estimates to maximize likelihood
3. Iterate until convergence

The estimates aim to maximize the likelihood of observed student performance data across the entire population.

# Limitations and Considerations

1. Parameter Identifiability: Multiple parameter sets can produce identical observable behavior
2. Skill Independence: BKT assumes skills are independent
3. Fixed Parameters: May not capture individual learning differences
4. Binary Observations: Only handles correct/incorrect responses

# References

The BKT model was first introduced by Corbett and Anderson (1995) and has since become a cornerstone of intelligent tutoring systems and educational data mining.