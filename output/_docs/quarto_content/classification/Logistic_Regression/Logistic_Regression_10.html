<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>logistic_regression_10</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-11.-compare-gradient-descent-with-second-order-optimization-methods-e.g.-newton-raphson-in-the-context-of-logistic-regression.-under-what-circumstances-might-you-prefer-one-over-the-other" class="level2">
<h2 class="anchored" data-anchor-id="question-11.-compare-gradient-descent-with-second-order-optimization-methods-e.g.-newton-raphson-in-the-context-of-logistic-regression.-under-what-circumstances-might-you-prefer-one-over-the-other">Question: 11. Compare gradient descent with second-order optimization methods (e.g., Newton-Raphson) in the context of logistic regression. Under what circumstances might you prefer one over the other?</h2>
<p><strong>Best Answer</strong></p>
<p>When training a logistic regression model, we aim to minimize the cost function, which is typically the negative log-likelihood. Both Gradient Descent (GD) and second-order methods like Newton-Raphson are iterative optimization algorithms used for this purpose, but they differ significantly in their approach and computational requirements.</p>
<p><strong>1. Gradient Descent (GD):</strong></p>
<ul>
<li><strong>Core Idea:</strong> GD is a first-order optimization algorithm that iteratively updates the model parameters <span class="math inline">\(\theta\)</span> in the direction of the negative gradient of the cost function <span class="math inline">\(J(\theta)\)</span>.</li>
<li><strong>Update Rule:</strong> The update rule for GD is given by: <span class="math display">\[\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)\]</span> where:
<ul>
<li><span class="math inline">\(\theta_t\)</span> is the parameter vector at iteration <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\alpha\)</span> is the learning rate (step size).</li>
<li><span class="math inline">\(\nabla J(\theta_t)\)</span> is the gradient of the cost function with respect to <span class="math inline">\(\theta\)</span> at iteration <span class="math inline">\(t\)</span>.</li>
</ul></li>
<li><strong>Logistic Regression Gradient:</strong> For logistic regression with a sigmoid activation function, the gradient of the cost function is relatively simple to compute. Given <span class="math inline">\(m\)</span> training examples <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^m\)</span> where <span class="math inline">\(x_i\)</span> is the feature vector, and <span class="math inline">\(y_i \in \{0, 1\}\)</span> is the corresponding label, the cost function is given by: <span class="math display">\[J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} [y_i \log(h_\theta(x_i)) + (1 - y_i) \log(1 - h_\theta(x_i))]\]</span> where <span class="math inline">\(h_\theta(x_i) = \frac{1}{1 + e^{-\theta^T x_i}}\)</span>. The gradient is: <span class="math display">\[\nabla J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i)x_i\]</span></li>
<li><strong>Computational Cost:</strong> GD has a lower computational cost per iteration, especially for large datasets, because it only requires computing the first derivative (gradient). The computational complexity is <span class="math inline">\(O(nd)\)</span> per iteration, where <span class="math inline">\(n\)</span> is the number of samples and <span class="math inline">\(d\)</span> is the number of features.</li>
<li><strong>Convergence:</strong> GD can be slower to converge, especially when the cost function has elongated or ill-conditioned contours. The learning rate <span class="math inline">\(\alpha\)</span> needs to be carefully tuned; a too-large learning rate can cause oscillations or divergence, while a too-small learning rate can result in very slow convergence.</li>
</ul>
<p><strong>2. Newton-Raphson Method:</strong></p>
<ul>
<li><strong>Core Idea:</strong> Newton-Raphson is a second-order optimization algorithm that uses both the gradient and the Hessian (matrix of second derivatives) of the cost function to find the minimum. It approximates the cost function with a quadratic function.</li>
<li><strong>Update Rule:</strong> The update rule is given by: <span class="math display">\[\theta_{t+1} = \theta_t - H^{-1}(\theta_t) \nabla J(\theta_t)\]</span> where:
<ul>
<li><span class="math inline">\(H(\theta_t)\)</span> is the Hessian matrix of the cost function evaluated at <span class="math inline">\(\theta_t\)</span>.</li>
<li><span class="math inline">\(\nabla J(\theta_t)\)</span> is the gradient of the cost function evaluated at <span class="math inline">\(\theta_t\)</span>.</li>
</ul></li>
<li><strong>Logistic Regression Hessian:</strong> For logistic regression, the Hessian matrix is given by: <span class="math display">\[H(\theta) = \frac{1}{m} \sum_{i=1}^{m} h_\theta(x_i)(1 - h_\theta(x_i))x_i x_i^T\]</span> The Hessian is a symmetric, positive semi-definite matrix (PSD), which ensures that the Newton step is a descent direction.</li>
<li><strong>Computational Cost:</strong> Newton-Raphson has a higher computational cost per iteration, especially for high-dimensional feature spaces, because it requires computing and inverting the Hessian matrix. The computational complexity for computing the Hessian is <span class="math inline">\(O(nd^2)\)</span>, and for inverting the Hessian, it is <span class="math inline">\(O(d^3)\)</span>. Thus, the per-iteration cost is dominated by <span class="math inline">\(O(nd^2 + d^3)\)</span>. In practice, computing the inverse directly is often avoided by solving the linear system <span class="math inline">\(H(\theta_t) \Delta \theta = \nabla J(\theta_t)\)</span> for <span class="math inline">\(\Delta \theta\)</span> and then updating <span class="math inline">\(\theta_{t+1} = \theta_t - \Delta \theta\)</span>. This can be done using Cholesky decomposition or conjugate gradient methods, which can be more efficient.</li>
<li><strong>Convergence:</strong> Newton-Raphson typically converges faster than GD, especially near the optimum, because it uses curvature information. It often requires fewer iterations to reach the minimum. It is also less sensitive to the choice of learning rate (or, strictly speaking, it does not require a learning rate parameter).</li>
<li><strong>Limitations:</strong>
<ul>
<li>The Hessian matrix must be invertible. If the Hessian is singular or poorly conditioned, the Newton-Raphson method can fail. Regularization can help to ensure that the Hessian is invertible.</li>
<li>The method can be unstable if the starting point is far from the optimum or if the cost function is highly non-convex.</li>
<li>For very large datasets, the cost of computing and inverting the Hessian can be prohibitive.</li>
</ul></li>
</ul>
<p><strong>Circumstances to Prefer One Over the Other:</strong></p>
<ul>
<li><strong>Prefer Gradient Descent:</strong>
<ul>
<li><strong>Large Datasets:</strong> When dealing with very large datasets (millions or billions of examples), the lower per-iteration cost of GD makes it more practical. Stochastic Gradient Descent (SGD) or mini-batch GD are often used in these cases to further reduce the computational burden.</li>
<li><strong>High-Dimensional Feature Space:</strong> If the number of features is very large, computing and inverting the Hessian becomes computationally expensive.</li>
<li><strong>Online Learning:</strong> GD is well-suited for online learning scenarios where data arrives sequentially because it only needs to process one data point (or a mini-batch) at a time.</li>
</ul></li>
<li><strong>Prefer Newton-Raphson:</strong>
<ul>
<li><strong>Small to Medium Datasets:</strong> For small to medium datasets (thousands of examples), the faster convergence of Newton-Raphson can outweigh the higher per-iteration cost.</li>
<li><strong>Well-Conditioned Problems:</strong> When the cost function is relatively well-behaved (e.g., close to quadratic near the optimum) and the Hessian is well-conditioned, Newton-Raphson can converge very quickly.</li>
<li><strong>When Accuracy is Paramount:</strong> If high accuracy is required and the computational cost is not a major concern, Newton-Raphson can be a good choice.</li>
</ul></li>
<li><strong>Other Considerations:</strong>
<ul>
<li><strong>Memory Constraints:</strong> Newton-Raphson requires storing the Hessian matrix, which can be a problem for high-dimensional feature spaces with limited memory.</li>
<li><strong>Quasi-Newton Methods:</strong> Methods like BFGS and L-BFGS are quasi-Newton methods that approximate the Hessian matrix using gradient information. They offer a compromise between the computational cost of GD and the faster convergence of Newton-Raphson and are often a good choice for medium-sized datasets.</li>
</ul></li>
</ul>
<p>In summary, the choice between GD and Newton-Raphson for logistic regression depends on the specific characteristics of the dataset and the computational resources available. GD is generally preferred for large datasets, while Newton-Raphson can be more efficient for small to medium datasets when high accuracy is required and the Hessian can be efficiently computed and inverted (or approximated).</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested approach for explaining this in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Basics:</strong></p>
<ul>
<li>“Both gradient descent and Newton-Raphson are iterative optimization algorithms used to minimize the cost function in logistic regression. However, they differ significantly in how they approach the optimization problem.”</li>
</ul></li>
<li><p><strong>Explain Gradient Descent (GD):</strong></p>
<ul>
<li>“Gradient descent is a first-order optimization method. It updates the model parameters by taking steps in the direction opposite to the gradient of the cost function. The update rule looks like this: <span class="math inline">\(\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)\)</span>”</li>
<li>“Here, <span class="math inline">\(\alpha\)</span> is the learning rate, which controls the step size. A key advantage of GD is its lower computational cost per iteration, especially for large datasets, since it only requires calculating the gradient.”</li>
<li>“However, GD can be slow to converge, particularly if the cost function has elongated contours, and it requires careful tuning of the learning rate.”</li>
</ul></li>
<li><p><strong>Introduce Newton-Raphson:</strong></p>
<ul>
<li>“Newton-Raphson, on the other hand, is a second-order optimization method. It uses both the gradient and the Hessian (the matrix of second derivatives) to approximate the cost function as a quadratic and find the minimum.”</li>
<li>“The update rule is: <span class="math inline">\(\theta_{t+1} = \theta_t - H^{-1}(\theta_t) \nabla J(\theta_t)\)</span>. The <span class="math inline">\(H^{-1}\)</span> is the inverse of the Hessian.”</li>
<li>“Newton-Raphson often converges faster than GD, especially near the optimum, because it considers the curvature of the cost function. It generally requires fewer iterations.”</li>
</ul></li>
<li><p><strong>Discuss Computational Cost Trade-offs:</strong></p>
<ul>
<li>“The trade-off is that Newton-Raphson has a much higher computational cost per iteration. Computing the Hessian and its inverse can be very expensive, especially in high-dimensional feature spaces. Approximating the inverse is often done by solving the system <span class="math inline">\(H \Delta \theta = \nabla J\)</span>, which can be done more efficiently with methods like Cholesky decomposition or conjugate gradient.”</li>
</ul></li>
<li><p><strong>Explain When to Prefer Each Method:</strong></p>
<ul>
<li>“I’d prefer gradient descent for very large datasets or high-dimensional feature spaces because the lower per-iteration cost makes it more practical. Stochastic or mini-batch GD are also useful for large datasets. Also, prefer GD in Online learning”</li>
<li>“I’d choose Newton-Raphson for smaller to medium-sized datasets, where the faster convergence outweighs the higher per-iteration cost, especially if high accuracy is important and the Hessian can be computed and inverted efficiently.”</li>
</ul></li>
<li><p><strong>Mention Limitations and Alternatives:</strong></p>
<ul>
<li>“It’s worth noting that Newton-Raphson has limitations. The Hessian needs to be invertible. If not regularization may help. Quasi-Newton methods like BFGS and L-BFGS offer a compromise by approximating the Hessian, making them suitable for medium-sized datasets.”</li>
</ul></li>
<li><p><strong>Conclude and Invite Further Questions:</strong></p>
<ul>
<li>“In summary, the choice between GD and Newton-Raphson depends on the specific problem and the available resources. GD is generally better for large datasets, while Newton-Raphson can be more efficient for smaller datasets. Are there any aspects you’d like me to elaborate on?”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to process the information.</li>
<li><strong>Simplify the Math:</strong> While including the equations is important to demonstrate expertise, explain them in plain language. For example, instead of just saying “<span class="math inline">\(\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)\)</span>”, say “The new value of the parameters is equal to the old value, minus the learning rate times the gradient.”</li>
<li><strong>Highlight Key Concepts:</strong> Emphasize words like “first-order,” “second-order,” “gradient,” “Hessian,” “convergence,” and “computational cost.”</li>
<li><strong>Engage the Interviewer:</strong> Ask questions to ensure they’re following along. For instance, “Are you familiar with the concept of the Hessian matrix?” or “Does this distinction between first-order and second-order methods make sense?”</li>
<li><strong>Be Ready to Elaborate:</strong> The interviewer may ask follow-up questions about specific aspects, such as the challenges of inverting the Hessian or the different types of gradient descent. Be prepared to provide more detail on these topics.</li>
<li><strong>Use Real-World Context:</strong> Connect the discussion to real-world scenarios where each method would be more appropriate, demonstrating practical understanding.</li>
</ul>
<p>By following this structure and keeping these communication tips in mind, you can effectively convey your understanding of gradient descent and Newton-Raphson and demonstrate your senior-level expertise.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>