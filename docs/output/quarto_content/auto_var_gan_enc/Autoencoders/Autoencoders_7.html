<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>autoencoders_7</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-8.-in-deploying-an-autoencoder-model-in-a-real-world-production-system-what-considerations-must-be-taken-into-account-regarding-scalability-latency-and-model-updating-how-would-you-ensure-the-model-remains-effective-over-time" class="level2">
<h2 class="anchored" data-anchor-id="question-8.-in-deploying-an-autoencoder-model-in-a-real-world-production-system-what-considerations-must-be-taken-into-account-regarding-scalability-latency-and-model-updating-how-would-you-ensure-the-model-remains-effective-over-time">Question: 8. In deploying an autoencoder model in a real-world production system, what considerations must be taken into account regarding scalability, latency, and model updating? How would you ensure the model remains effective over time?</h2>
<p><strong>Best Answer</strong></p>
<p>Deploying an autoencoder to a real-world production system requires careful consideration of scalability, latency, and model updating to ensure it remains effective over time. Let’s break down these considerations:</p>
<section id="scalability" class="level3">
<h3 class="anchored" data-anchor-id="scalability">Scalability</h3>
<ul>
<li><p><strong>Data Volume</strong>: Autoencoders, like other neural networks, can handle high-dimensional data, but processing large volumes of data in real-time requires optimized infrastructure. Consider using distributed computing frameworks like Spark or Dask for pre-processing and feature extraction if the data volume is extremely high before feeding data to the autoencoder. For the serving layer, consider horizontally scalable solutions such as Kubernetes.</p></li>
<li><p><strong>Model Size</strong>: Large autoencoder models can be memory-intensive. This can become a bottleneck when deploying numerous instances for high throughput. Model compression techniques become critical.</p></li>
</ul>
</section>
<section id="latency" class="level3">
<h3 class="anchored" data-anchor-id="latency">Latency</h3>
<p>Latency is crucial, particularly for real-time applications. Several techniques can minimize the time it takes for the autoencoder to process input and generate output:</p>
<ul>
<li><p><strong>Model Compression</strong>:</p>
<ul>
<li><strong>Pruning</strong>: Removing less important weights from the network can significantly reduce model size and improve inference speed. Structured pruning (e.g., removing entire filters or channels) is generally more hardware-friendly than unstructured pruning. Let <span class="math inline">\(W\)</span> represent the weight matrix of a layer. Pruning aims to find a binary mask <span class="math inline">\(M\)</span> such that <span class="math inline">\(W' = W \odot M\)</span>, where <span class="math inline">\(\odot\)</span> is element-wise multiplication and the elements of <span class="math inline">\(M\)</span> indicate which weights to keep (1) and which to prune (0). The goal is to minimize the performance degradation while maximizing the sparsity (number of zeros) in <span class="math inline">\(W'\)</span>.</li>
<li><strong>Quantization</strong>: Reducing the precision of the weights and activations (e.g., from float32 to int8) can greatly reduce memory footprint and accelerate computation, especially on hardware that supports lower precision operations. Quantization can be represented as: <span class="math display">\[Q(x) = scale \cdot round(\frac{x}{scale} + zero\_point)\]</span> Where <span class="math inline">\(x\)</span> is the original value, <span class="math inline">\(scale\)</span> and <span class="math inline">\(zero\_point\)</span> are quantization parameters, and <span class="math inline">\(Q(x)\)</span> is the quantized value.</li>
<li><strong>Knowledge Distillation</strong>: Training a smaller “student” model to mimic the behavior of a larger, pre-trained “teacher” autoencoder. The student model learns to predict the soft targets (probabilities) produced by the teacher, leading to better generalization than training from hard labels.</li>
</ul></li>
<li><p><strong>Hardware Acceleration</strong>: Utilize GPUs, TPUs, or specialized AI accelerators for faster inference. For example, NVIDIA’s TensorRT optimizes models for deployment on their GPUs, while Google’s Edge TPUs are designed for low-latency inference on edge devices.</p></li>
<li><p><strong>Batching</strong>: Process multiple inputs in a single batch to amortize the overhead of inference. However, increasing batch size can increase latency if the system becomes overloaded, so balance is key.</p></li>
<li><p><strong>Caching</strong>: For frequently occurring inputs, cache the autoencoder’s output to avoid redundant computations. This is especially useful if the input data distribution is skewed.</p></li>
<li><p><strong>Optimized Libraries</strong>: Use optimized deep learning inference libraries like TensorFlow Lite, PyTorch Mobile, or ONNX Runtime, which are designed for efficient deployment on various platforms.</p></li>
</ul>
</section>
<section id="model-updating" class="level3">
<h3 class="anchored" data-anchor-id="model-updating">Model Updating</h3>
<p>To ensure the autoencoder remains effective over time, you need a strategy for updating it to adapt to changes in the data distribution (data drift) and to improve its performance:</p>
<ul>
<li><strong>Data Drift Monitoring</strong>:
<ul>
<li>Monitor reconstruction error over time. A significant increase in reconstruction error can indicate data drift. Techniques like the Kolmogorov-Smirnov test or the Chi-squared test can be used to statistically compare the distributions of input data over time.</li>
<li>Implement alerting mechanisms that trigger retraining when reconstruction error exceeds a predefined threshold.</li>
</ul></li>
<li><strong>Retraining Strategies</strong>:
<ul>
<li><strong>Periodic Retraining</strong>: Retrain the model at regular intervals (e.g., weekly, monthly) using the latest data. This is a simple and effective way to adapt to gradual data drift.</li>
<li><strong>Trigger-Based Retraining</strong>: Retrain the model when data drift is detected or when performance metrics (e.g., reconstruction error) fall below a certain level. This is more adaptive than periodic retraining but requires robust drift detection mechanisms.</li>
<li><strong>Online Learning</strong>: Continuously update the model with new data as it arrives. This is suitable for rapidly changing environments but can be more complex to implement and requires careful monitoring to prevent instability.</li>
</ul></li>
<li><strong>Version Control &amp; A/B Testing</strong>:
<ul>
<li>Maintain version control of all models and configurations.</li>
<li>Conduct A/B testing to compare the performance of new and old models before deploying them to production. Evaluate metrics like reconstruction error, anomaly detection accuracy (if used for that), and any downstream business metrics.</li>
</ul></li>
<li><strong>Infrastructure for Model Updates</strong>:
<ul>
<li>Implement a CI/CD (Continuous Integration/Continuous Deployment) pipeline for automated model retraining, testing, and deployment.</li>
<li>Use a model serving platform that supports seamless model updates with minimal downtime (e.g., TensorFlow Serving, SageMaker, TorchServe).</li>
</ul></li>
</ul>
</section>
<section id="ensuring-model-effectiveness-over-time" class="level3">
<h3 class="anchored" data-anchor-id="ensuring-model-effectiveness-over-time">Ensuring Model Effectiveness Over Time</h3>
<p>In addition to the points above, these actions are important to keep the model effective:</p>
<ul>
<li><p><strong>Regular Evaluation</strong>: Periodically evaluate the autoencoder’s performance on a held-out dataset that represents the current data distribution.</p></li>
<li><p><strong>Feedback Loops</strong>: Establish feedback loops with domain experts to gather insights into potential data changes and model limitations.</p></li>
<li><p><strong>Anomaly Detection Tuning</strong>: If the autoencoder is used for anomaly detection, regularly review and adjust the anomaly detection thresholds based on the observed reconstruction error distribution and feedback from domain experts. For example, you might adjust the threshold to maintain a desired precision/recall trade-off.</p></li>
<li><p><strong>Feature Engineering Updates</strong>: Periodically revisit and update the feature engineering pipeline to ensure it remains relevant to the current data distribution. New features may need to be added or existing features may need to be transformed differently.</p></li>
</ul>
<p>By addressing scalability, latency, and model updating, you can successfully deploy an autoencoder to a production system and ensure it remains effective over time.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s how you could articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a high-level overview:</strong> “Deploying autoencoders in production requires careful attention to scalability, latency, and maintaining model effectiveness over time. Let’s break down each of these aspects.”</p></li>
<li><p><strong>Address Scalability:</strong> “Regarding scalability, we need to consider both data volume and model size. For very large datasets, distributed computing frameworks like Spark or Dask can be used for preprocessing. To manage model size, especially for high-throughput scenarios, model compression techniques are crucial.”</p></li>
<li><p><strong>Transition to Latency:</strong> “Latency is particularly important for real-time applications. Several methods can reduce latency. Model compression through pruning, quantization, or knowledge distillation are very effective. For instance, pruning involves removing less important weights. Quantization reduces the precision of weights, and knowledge distillation involves training a smaller model to mimic a larger one.”</p></li>
<li><p><strong>Explain Quantization with Care:</strong> “For example, quantization…it involves approximating the original weight values with lower-precision representations, which can significantly reduce the memory footprint and computational cost. The equation is:</p>
<equation>
<p>Q(x) = scale round( + zero_point)</p>
</equation>
<p>where x is the original value, scale and zero_point are the quantization parameters. But don’t worry about memorizing the formula, the key idea is the reduction in precision.”</p></li>
<li><p><strong>Continue discussing Latency:</strong> “Besides compression, hardware acceleration with GPUs or specialized AI accelerators, batching requests, caching, and using optimized libraries like TensorFlow Lite are helpful.”</p></li>
<li><p><strong>Move to Model Updating:</strong> “To ensure the model remains effective over time, we need a strategy for updating it. This involves monitoring data drift and retraining. We can monitor reconstruction error. A spike in reconstruction error often signals data drift. We could use statistical tests like the Kolmogorov-Smirnov test to detect differences in data distributions.”</p></li>
<li><p><strong>Explain Retraining Strategies:</strong> “Retraining can be done periodically, triggered by drift detection, or through online learning. Periodic is simple but less adaptive. Triggered is more adaptive but requires robust detection mechanisms. Online learning adapts continuously but is complex to implement.”</p></li>
<li><p><strong>Mention Version Control and A/B Testing:</strong> “It’s crucial to maintain version control of our models and use A/B testing to compare new and old models before deploying them. Key metrics include reconstruction error, anomaly detection accuracy, and downstream business metrics.”</p></li>
<li><p><strong>Discuss Infrastructure:</strong> “Finally, we need the right infrastructure – a CI/CD pipeline for automated updates and a model serving platform that supports seamless model updates, like TensorFlow Serving or SageMaker.”</p></li>
<li><p><strong>Summarize and emphasize ensuring model effectiveness:</strong> “In summary, by paying close attention to scalability and latency we can deploy performant autoencoder models, and by actively monitoring for data drift and having a retraining strategy, we can ensure the autoencoder model will remain effective over time. Regular evaluation, incorporating feedback from domain experts, and tuning anomaly detection thresholds are also important.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself</strong>: Don’t rush through the explanation.</li>
<li><strong>Use simple language</strong>: Avoid jargon where possible.</li>
<li><strong>Check for understanding</strong>: Ask the interviewer if they have any questions at various points.</li>
<li><strong>Emphasize practical aspects</strong>: Focus on how these techniques are used in real-world deployments.</li>
<li><strong>Be ready to elaborate</strong>: Have deeper explanations ready for any area where the interviewer seems particularly interested.</li>
<li><strong>Demonstrate confidence</strong>: You’ve thought about these issues and have solutions.</li>
</ul>
<p>By following these steps, you can demonstrate your expertise and effectively communicate your understanding of the challenges and solutions involved in deploying autoencoders to production.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>