<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>cyclegan__stylegan__etc__6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-7.-given-the-substantial-computational-resources-required-for-training-models-like-stylegan-how-would-you-optimize-the-training-pipeline-for-scalability-and-potentially-enable-real-time-inference-include-suggestions-for-both-software-and-hardware-optimizations." class="level2">
<h2 class="anchored" data-anchor-id="question-7.-given-the-substantial-computational-resources-required-for-training-models-like-stylegan-how-would-you-optimize-the-training-pipeline-for-scalability-and-potentially-enable-real-time-inference-include-suggestions-for-both-software-and-hardware-optimizations.">Question: 7. Given the substantial computational resources required for training models like StyleGAN, how would you optimize the training pipeline for scalability and potentially enable real-time inference? Include suggestions for both software and hardware optimizations.</h2>
<p><strong>Best Answer</strong></p>
<p>Optimizing the training pipeline for StyleGAN, or similar computationally intensive generative models, to achieve scalability and real-time inference requires a multi-faceted approach. This encompasses both software and hardware optimizations, as well as algorithmic improvements.</p>
<p><strong>I. Distributed Training Strategies:</strong></p>
<p>The fundamental challenge is to distribute the computational load across multiple devices. Here are some key approaches:</p>
<ul>
<li><p><strong>Data Parallelism:</strong> This is a common approach where the training data is split across multiple workers (GPUs/TPUs), each holding a complete copy of the model. Each worker computes gradients on its portion of the data, and these gradients are then aggregated (e.g., using All-Reduce) to update the model parameters.</p>
<ul>
<li><p><strong>Synchronous SGD:</strong> All workers compute gradients and then synchronize before updating the model. This approach generally provides better convergence but can be slower due to straggler effects (where one slow worker holds up the entire process).</p></li>
<li><p><strong>Asynchronous SGD:</strong> Workers update the model parameters independently without strict synchronization. This can improve training speed but may lead to less stable convergence.</p></li>
</ul>
<p>The communication overhead is a key bottleneck in data parallelism. Techniques like gradient compression (e.g., quantization, sparsification) can help reduce this overhead.</p>
<ul>
<li><p><strong>Mathematical Formulation (Data Parallelism):</strong></p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(L(\theta, x_i, y_i)\)</span> be the loss function for model parameters <span class="math inline">\(\theta\)</span> and data point <span class="math inline">\((x_i, y_i)\)</span>.</li>
<li><span class="math inline">\(N\)</span> be the total number of data points.</li>
<li><span class="math inline">\(K\)</span> be the number of workers.</li>
<li><span class="math inline">\(B = N/K\)</span> be the batch size per worker.</li>
<li><span class="math inline">\(\theta_t\)</span> be the model parameters at iteration <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\nabla L_k(\theta_t)\)</span> be the gradient computed by worker <span class="math inline">\(k\)</span> on its mini-batch.</li>
</ul>
<p>In synchronous SGD, the update rule is:</p>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \eta \frac{1}{K} \sum_{k=1}^{K} \nabla L_k(\theta_t)\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate.</p></li>
</ul></li>
<li><p><strong>Model Parallelism:</strong> This approach is used when the model itself is too large to fit on a single device. The model is partitioned across multiple devices, and each device is responsible for computing a portion of the forward and backward pass.</p>
<ul>
<li><p><strong>Pipeline Parallelism:</strong> The model is divided into stages, and each stage is assigned to a different device. Data flows through the pipeline, with each device processing its assigned stage. This can significantly improve throughput but introduces pipeline bubbles (idle time) if not carefully balanced.</p></li>
<li><p><strong>Tensor Parallelism:</strong> Individual tensors are sharded across multiple devices. Operations on these tensors are distributed accordingly. This approach requires careful consideration of communication costs and dependencies.</p></li>
<li><p><strong>Mathematical Formulation (Model Parallelism):</strong></p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(M\)</span> be the model consisting of layers <span class="math inline">\(M_1, M_2, ..., M_K\)</span>.</li>
<li>Each layer <span class="math inline">\(M_k\)</span> is assigned to device <span class="math inline">\(k\)</span>.</li>
<li><span class="math inline">\(x\)</span> be the input data.</li>
</ul>
<p>The forward pass in pipeline parallelism can be represented as:</p>
<p><span class="math display">\[y_1 = M_1(x)\]</span> <span class="math display">\[y_2 = M_2(y_1)\]</span> <span class="math display">\[\vdots\]</span> <span class="math display">\[y_K = M_K(y_{K-1})\]</span></p>
<p>Each <span class="math inline">\(y_k\)</span> is computed on device <span class="math inline">\(k\)</span>. Backpropagation follows a similar pipeline structure.</p></li>
</ul></li>
<li><p><strong>Hybrid Parallelism:</strong> A combination of data and model parallelism. For example, each device group might use model parallelism within the group, and data parallelism across the groups. This is often the most effective approach for very large models and datasets.</p></li>
</ul>
<p><strong>II. Software Optimizations:</strong></p>
<ul>
<li><p><strong>Mixed-Precision Training (FP16):</strong> Using lower precision floating-point numbers (FP16) can significantly reduce memory usage and improve computational throughput on modern GPUs that have specialized FP16 cores (e.g., Tensor Cores on NVIDIA GPUs). This often requires careful management of numerical stability (e.g., using loss scaling).</p>
<ul>
<li><p><strong>Mathematical Justification:</strong> The core idea is to represent tensors and perform computations using 16-bit floating-point numbers instead of 32-bit. The main challenge is the reduced dynamic range of FP16, which can lead to underflow or overflow issues. Loss scaling helps mitigate this by multiplying the loss by a scale factor before computing gradients, which prevents gradients from vanishing. The gradients are then unscaled before applying the update.</p>
<p>For example:</p>
<ol type="1">
<li>Forward pass and loss computation in FP16.</li>
<li>Scale the loss: <span class="math inline">\(L' = sL\)</span>, where <span class="math inline">\(s\)</span> is the scaling factor.</li>
<li>Compute gradients <span class="math inline">\(\nabla L'\)</span> in FP16.</li>
<li>Unscale the gradients: <span class="math inline">\(\nabla L = \nabla L' / s\)</span>.</li>
<li>Update model parameters using <span class="math inline">\(\nabla L\)</span>.</li>
</ol></li>
</ul></li>
<li><p><strong>Gradient Accumulation:</strong> Accumulate gradients over multiple mini-batches before performing a weight update. This effectively increases the batch size without increasing memory usage. This is especially useful when memory is limited.</p></li>
<li><p><strong>Optimized Libraries:</strong> Leverage highly optimized libraries such as cuDNN, cuBLAS, and TensorRT for GPU acceleration. These libraries provide highly tuned implementations of common deep learning operations.</p></li>
<li><p><strong>Memory Management:</strong> Careful memory management is crucial to prevent out-of-memory errors. Techniques like gradient checkpointing (recomputing activations during the backward pass to reduce memory footprint) can be employed.</p></li>
<li><p><strong>Efficient Data Loading:</strong> Optimize the data loading pipeline to ensure that data is fed to the GPUs efficiently. This may involve using multiple worker threads, prefetching data, and using efficient data formats (e.g., TFRecords).</p></li>
<li><p><strong>Model Compression:</strong> Reducing the size and complexity of the model can dramatically improve inference speed.</p>
<ul>
<li><strong>Quantization:</strong> Converting the model’s weights and activations to lower precision (e.g., INT8) can reduce memory usage and improve inference speed on hardware that supports INT8 operations. Quantization-aware training can help minimize the accuracy loss associated with quantization.</li>
<li><strong>Pruning:</strong> Removing less important weights from the model can reduce its size and computational complexity. Structured pruning (removing entire filters or channels) is often preferred as it can lead to more efficient hardware utilization.</li>
<li><strong>Knowledge Distillation:</strong> Training a smaller “student” model to mimic the behavior of a larger, more complex “teacher” model. This allows the student model to achieve comparable performance with significantly fewer parameters.</li>
</ul></li>
</ul>
<p><strong>III. Hardware Optimizations:</strong></p>
<ul>
<li><strong>GPUs:</strong> NVIDIA GPUs, particularly those with Tensor Cores (Volta, Turing, Ampere, Hopper architectures), are well-suited for training and inference of deep learning models.</li>
<li><strong>TPUs:</strong> Google’s Tensor Processing Units (TPUs) are custom-designed ASICs (Application-Specific Integrated Circuits) optimized for deep learning workloads. They offer significant performance advantages over GPUs for certain types of models.</li>
<li><strong>Specialized Hardware Accelerators:</strong> There are a growing number of specialized hardware accelerators designed for deep learning, such as FPGAs (Field-Programmable Gate Arrays) and ASICs from companies like Cerebras and Graphcore.</li>
<li><strong>Multi-GPU/TPU Systems:</strong> Training and inference can be significantly accelerated by using multiple GPUs or TPUs in parallel. This requires careful consideration of inter-device communication and synchronization.</li>
</ul>
<p><strong>IV. Algorithmic Improvements:</strong></p>
<ul>
<li><strong>Progressive Growing of GANs:</strong> As used in the original StyleGAN paper, this involves gradually increasing the resolution of the generated images during training. This can improve training stability and reduce training time.</li>
<li><strong>Efficient Network Architectures:</strong> Exploring more efficient network architectures, such as MobileNets or EfficientNets, can reduce the computational cost of StyleGAN. However, this may require careful tuning to maintain image quality.</li>
<li><strong>Attention Mechanisms:</strong> Utilizing attention mechanisms can allow the model to focus on the most important parts of the image, potentially improving image quality while reducing the computational cost.</li>
<li><strong>Regularization Techniques:</strong> Techniques like spectral normalization and gradient penalties can help improve training stability and reduce the need for large batch sizes, which can save memory.</li>
</ul>
<p><strong>V. Real-time Inference Considerations:</strong></p>
<p>For real-time inference, the following are critical:</p>
<ul>
<li><strong>Model Size:</strong> Reduce the model size as much as possible through compression techniques like quantization, pruning, and knowledge distillation.</li>
<li><strong>Hardware Acceleration:</strong> Use specialized hardware like GPUs, TPUs, or edge AI accelerators for inference.</li>
<li><strong>Batching:</strong> Batching multiple inference requests together can significantly improve throughput.</li>
<li><strong>Optimized Inference Engines:</strong> Use optimized inference engines like TensorRT (NVIDIA), TensorFlow Lite (Google), or ONNX Runtime (Microsoft) to accelerate inference.</li>
<li><strong>Quantization-Aware Training:</strong> Train the model with quantization in mind to minimize accuracy loss during quantization.</li>
</ul>
<p><strong>VI. Practical Considerations:</strong></p>
<ul>
<li><strong>Frameworks:</strong> TensorFlow, PyTorch, and JAX are popular deep learning frameworks that provide tools and libraries for distributed training, mixed-precision training, and model compression.</li>
<li><strong>Profiling:</strong> Use profiling tools to identify performance bottlenecks in the training and inference pipelines.</li>
<li><strong>Monitoring:</strong> Monitor resource utilization (CPU, GPU, memory, network) during training and inference to identify potential issues.</li>
<li><strong>Experimentation:</strong> Experiment with different optimization techniques to find the best combination for a specific model and dataset.</li>
</ul>
<p>In summary, optimizing StyleGAN for scalability and real-time inference requires a holistic approach involving distributed training, software optimizations, hardware acceleration, and algorithmic improvements. The specific techniques used will depend on the available resources and the desired trade-off between performance and accuracy.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this to an interviewer:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong></p>
<ul>
<li>“Optimizing StyleGAN for scalability and real-time inference is a multi-faceted challenge involving software and hardware optimizations, as well as algorithmic improvements. I’d approach this by considering distributed training, model compression, and efficient hardware utilization.”</li>
</ul></li>
<li><p><strong>Address Distributed Training:</strong></p>
<ul>
<li>“First, distributed training is crucial. I’d likely start with data parallelism, where the data is split across multiple GPUs. We could explore synchronous SGD for better convergence or asynchronous SGD for faster training, depending on the specific dataset and model characteristics. It’s also worth considering model parallelism if the model is too large to fit on a single GPU, perhaps using pipeline parallelism. The communication overhead between nodes becomes a vital thing to consider and mitigate.”</li>
<li>“To go into detail on Data parallelism (optionally only if the interviewer asks), the update rule in synchronous SGD is: <span class="math display">\[\theta_{t+1} = \theta_t - \eta \frac{1}{K} \sum_{k=1}^{K} \nabla L_k(\theta_t)\]</span> where <span class="math inline">\(\eta\)</span> is the learning rate, and the rest is defined as mentioned earlier.</li>
</ul></li>
<li><p><strong>Dive into Software Optimizations:</strong></p>
<ul>
<li>“Next, on the software side, mixed-precision training (FP16) can significantly improve throughput, especially on GPUs with Tensor Cores. We would also need to be careful with numerical stability, using techniques like loss scaling. We would also look to Gradient accumulation which is a must. Further, we would want to use optimized libraries such as cuDNN and TensorRT for GPU acceleration.”</li>
<li>(If asked about mixed precision) “The scaling factor is a critical point. Here’s how it works mathematically: [explain the process from the Best Answer section].”</li>
</ul></li>
<li><p><strong>Discuss Model Compression:</strong></p>
<ul>
<li>“Model compression techniques are critical for inference. Quantization (e.g., INT8) can reduce memory footprint and improve inference speed, but might require quantization-aware training to minimize accuracy loss. Pruning, especially structured pruning, can further reduce the model’s complexity. Finally, knowledge distillation could be applied, where we train a smaller model to mimic a larger one.”</li>
</ul></li>
<li><p><strong>Talk about Hardware Considerations:</strong></p>
<ul>
<li>“Hardware-wise, modern GPUs are a good starting point, but TPUs can offer further performance gains. Specialized hardware accelerators are also emerging. For real-time inference, the goal is to minimize latency, so hardware acceleration is paramount.”</li>
</ul></li>
<li><p><strong>Address Algorithmic Improvements (Optional):</strong></p>
<ul>
<li>“Algorithmically, we should consider using progressive growing of GANs, efficient network architectures, and attention mechanisms to reduce computational cost without sacrificing image quality. These are model specific but worthy of consideration”</li>
</ul></li>
<li><p><strong>Consider Real-Time Inference:</strong></p>
<ul>
<li>“For real-time inference, it’s crucial to reduce the model size as much as possible through compression techniques, leverage optimized inference engines like TensorRT, and batch multiple inference requests together when possible.”</li>
</ul></li>
<li><p><strong>Mention Practical Considerations:</strong></p>
<ul>
<li>“Finally, it’s important to use profiling tools to identify bottlenecks and monitor resource utilization. The right combination of these techniques will depend on the specific requirements and constraints of the application.”</li>
</ul></li>
<li><p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush. Explain each point clearly and concisely.</li>
<li><strong>Check for Understanding:</strong> Pause occasionally and ask if the interviewer has any questions.</li>
<li><strong>Avoid Jargon:</strong> Use technical terms where appropriate, but explain them if necessary.</li>
<li><strong>Focus on Key Concepts:</strong> Highlight the most important points and avoid getting bogged down in unnecessary details.</li>
<li><strong>Be Flexible:</strong> Be prepared to adjust your answer based on the interviewer’s questions and interests.</li>
<li><strong>Relate to Experience:</strong> If you have experience with any of these techniques, mention it and describe the results you achieved.</li>
</ul></li>
</ol>
<p>By following these guidelines, you can effectively communicate your expertise and demonstrate your ability to optimize StyleGAN for scalability and real-time inference.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>