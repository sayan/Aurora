<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>cyclegan__stylegan__etc__3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-4.-in-a-deployment-scenario-suppose-you-are-tasked-with-implementing-a-cyclegan-for-image-domain-transfer-on-a-dataset-that-is-messy-with-significant-noise-and-mismatched-distributions.-describe-your-approach-to-handling-the-data-quality-issues-model-training-and-validation-of-the-transformation-quality." class="level2">
<h2 class="anchored" data-anchor-id="question-4.-in-a-deployment-scenario-suppose-you-are-tasked-with-implementing-a-cyclegan-for-image-domain-transfer-on-a-dataset-that-is-messy-with-significant-noise-and-mismatched-distributions.-describe-your-approach-to-handling-the-data-quality-issues-model-training-and-validation-of-the-transformation-quality.">Question: 4. In a deployment scenario, suppose you are tasked with implementing a CycleGAN for image domain transfer on a dataset that is messy, with significant noise and mismatched distributions. Describe your approach to handling the data quality issues, model training, and validation of the transformation quality.</h2>
<p><strong>Best Answer</strong></p>
<p>Implementing a CycleGAN in a real-world deployment scenario with messy data, noise, and mismatched distributions requires a multi-faceted approach spanning data pre-processing, robust model training, and rigorous validation. Here’s a breakdown of the key steps:</p>
<p><strong>1. Data Understanding and Exploration:</strong></p>
<ul>
<li><strong>Initial Data Analysis:</strong> Begin by thoroughly examining the characteristics of both image domains (source A and target B). Compute summary statistics, visualize samples, and identify common types of noise, artifacts, or distribution skews present in each dataset.</li>
<li><strong>Mismatched Distribution Analysis:</strong> Quantify the differences between the distributions of domains A and B. Techniques like calculating the Fréchet Inception Distance (FID) between subsets of each domain can provide initial insights into the dissimilarity. A large initial FID score highlights the challenge. <span class="math display">\[FID(X, Y) = ||\mu_X - \mu_Y||^2 + Tr(\Sigma_X + \Sigma_Y - 2(\Sigma_X \Sigma_Y)^{1/2})\]</span> where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the feature representations of the real and generated images, respectively, obtained by the Inception-v3 network. <span class="math inline">\(\mu\)</span> denotes the mean and <span class="math inline">\(\Sigma\)</span> the covariance.</li>
<li><strong>Noise Assessment:</strong> Characterize the types of noise present (e.g., salt-and-pepper noise, Gaussian noise, blur, compression artifacts). Estimate the noise levels (e.g., Signal-to-Noise Ratio, SNR). A low SNR indicates high noise. <span class="math display">\[SNR = 10 log_{10} (\frac{P_{signal}}{P_{noise}})\]</span> where <span class="math inline">\(P_{signal}\)</span> and <span class="math inline">\(P_{noise}\)</span> are the power of the signal and noise, respectively.</li>
</ul>
<p><strong>2. Data Pre-processing and Cleaning:</strong></p>
<ul>
<li><strong>Outlier Removal:</strong> Identify and remove or correct outlier images. This could involve visual inspection, automated methods based on image quality metrics (e.g., blurriness, contrast, brightness), or domain-specific heuristics.</li>
<li><strong>Noise Reduction:</strong> Apply appropriate noise reduction techniques based on the identified noise characteristics. Median filtering is effective for salt-and-pepper noise. Gaussian blurring can reduce Gaussian noise. Wavelet denoising can be used for more complex noise patterns.</li>
<li><strong>Normalization:</strong> Normalize the pixel values to a standard range (e.g., [0, 1] or [-1, 1]). This helps stabilize training and improve convergence. Consider domain-specific normalization if the intensity ranges differ significantly.</li>
<li><strong>Data Augmentation:</strong> Augment both datasets to increase diversity and robustness. Common techniques include:
<ul>
<li><strong>Geometric Transformations:</strong> Rotations, translations, scaling, flips, and crops.</li>
<li><strong>Color Jittering:</strong> Adjusting brightness, contrast, saturation, and hue.</li>
<li><strong>Noise Addition:</strong> Adding synthetic noise to simulate real-world conditions.</li>
<li><strong>Mixup/CutMix:</strong> Creating new samples by combining existing ones to encourage smoother decision boundaries.</li>
</ul></li>
<li><strong>Distribution Alignment (Domain Adaptation Techniques):</strong> Since there is a mismatch, we should consider:
<ul>
<li><strong>Histogram Matching:</strong> Adjusting the histogram of one domain to match the other, which helps to align the global color and intensity distributions.</li>
<li><strong>Contrastive Learning:</strong> Augmenting the CycleGAN loss with a contrastive loss to pull embeddings of similar images in different domains closer together.</li>
</ul></li>
<li><strong>Addressing Class Imbalance (if applicable):</strong> If the datasets represent different classes with varying frequencies, employ techniques like oversampling the minority class or undersampling the majority class to mitigate bias.</li>
</ul>
<p><strong>3. CycleGAN Model Training:</strong></p>
<ul>
<li><strong>Base Architecture:</strong> Start with a standard CycleGAN architecture, typically consisting of two generators (G: A -&gt; B, F: B -&gt; A) and two discriminators (DA, DB). The generators are often based on ResNet or U-Net architectures.</li>
<li><strong>Loss Function Engineering:</strong> The standard CycleGAN loss function consists of three components:
<ul>
<li><p><strong>Adversarial Loss:</strong> Ensures that the generated images are indistinguishable from real images in the target domain. <span class="math display">\[L_{GAN}(G, D_B, A, B) = E_{b \sim p_{data}(b)}[log D_B(b)] + E_{a \sim p_{data}(a)}[log(1 - D_B(G(a)))]\]</span> And similarly for <span class="math inline">\(F\)</span> and <span class="math inline">\(D_A\)</span>.</p></li>
<li><p><strong>Cycle Consistency Loss:</strong> Ensures that an image translated from A to B and back to A is similar to the original image. <span class="math display">\[L_{cycle}(G, F, A, B) = E_{a \sim p_{data}(a)}[||F(G(a)) - a||_1] + E_{b \sim p_{data}(b)}[||G(F(b)) - b||_1]\]</span></p></li>
<li><p><strong>Identity Loss (Optional):</strong> Encourages the generator to preserve the identity of the input image when the input and output domains are similar. <span class="math display">\[L_{identity}(G, B) = E_{b \sim p_{data}(b)}[||G(b) - b||_1]\]</span></p></li>
<li><p><strong>Total Loss:</strong> The overall loss function is a weighted sum of these components: <span class="math display">\[L_{total} = \lambda_{GAN}(L_{GAN}(G, D_B, A, B) + L_{GAN}(F, D_A, B, A)) + \lambda_{cycle} L_{cycle}(G, F, A, B) + \lambda_{identity} L_{identity}(G, B)\]</span></p></li>
<li><p><strong>Loss Weight Tuning:</strong> Carefully tune the weights (<span class="math inline">\(\lambda_{GAN}\)</span>, <span class="math inline">\(\lambda_{cycle}\)</span>, <span class="math inline">\(\lambda_{identity}\)</span>) to balance the different loss components.</p></li>
</ul></li>
<li><strong>Robust Optimization Techniques:</strong>
<ul>
<li><strong>Gradient Clipping:</strong> Prevents exploding gradients during training, which can be common with noisy data.</li>
<li><strong>Spectral Normalization:</strong> Stabilizes the discriminator by normalizing its spectral norm, preventing it from becoming too strong too quickly.</li>
<li><strong>Two Time-Scale Update Rule (TTUR):</strong> Use different learning rates for the generator and discriminator to prevent one from overpowering the other.</li>
<li><strong>Learning Rate Scheduling:</strong> Employ learning rate decay or adaptive optimizers (e.g., Adam) to improve convergence.</li>
</ul></li>
<li><strong>Dealing with Noise Directly:</strong>
<ul>
<li><strong>Noise Regularization:</strong> Add noise to the discriminator’s inputs during training to make it more robust to noisy images. This is a form of data augmentation during training specifically targeting the discriminator.</li>
<li><strong>Perceptual Loss:</strong> Using a pre-trained network (e.g., VGG) can guide the generator to create outputs that are perceptually similar, which often reduces the impact of noise.</li>
<li><strong>Adversarial training with noise:</strong> Include noisy samples in the training set of the discriminator.</li>
</ul></li>
<li><strong>Batch Size and Epochs:</strong> Experiment with different batch sizes and training epochs. Larger batch sizes may help stabilize training but require more memory. Train for a sufficient number of epochs until convergence.</li>
<li><strong>Monitoring and Checkpointing:</strong> Monitor the training progress (loss values, image quality) and save checkpoints of the model regularly.</li>
</ul>
<p><strong>4. Validation and Evaluation:</strong></p>
<ul>
<li><strong>Quantitative Metrics:</strong>
<ul>
<li><strong>Fréchet Inception Distance (FID):</strong> As mentioned earlier, use FID to measure the similarity between the generated and real images in the target domain. Lower FID scores indicate better quality.</li>
<li><strong>Learned Perceptual Image Patch Similarity (LPIPS):</strong> LPIPS assesses the perceptual similarity between images, capturing more subtle differences than pixel-wise metrics.</li>
<li><strong>Kernel Inception Distance (KID):</strong> An alternative to FID that uses a different kernel to measure the distance between distributions.</li>
</ul></li>
<li><strong>Qualitative Evaluation:</strong>
<ul>
<li><strong>Visual Inspection:</strong> Carefully examine the generated images for artifacts, noise, and overall realism. Compare the generated images to real images in the target domain.</li>
<li><strong>User Studies:</strong> Conduct user studies to assess the perceived quality of the generated images. Ask human evaluators to rate the realism, naturalness, or faithfulness of the transformations.</li>
</ul></li>
<li><strong>Cross-Validation:</strong>
<ul>
<li><strong>k-Fold Cross-Validation:</strong> Divide the dataset into k folds and train the model k times, each time using a different fold as the validation set. This provides a more robust estimate of the model’s performance.</li>
</ul></li>
<li><strong>Application-Specific Evaluation:</strong> If the CycleGAN is used for a specific task (e.g., image segmentation), evaluate its performance on that task using relevant metrics (e.g., IoU, Dice score).</li>
<li><strong>Ablation Studies:</strong> Conduct ablation studies to assess the impact of different components of the loss function or pre-processing techniques. This helps to identify the most important factors for achieving good performance.</li>
</ul>
<p><strong>5. Deployment Considerations:</strong></p>
<ul>
<li><strong>Hardware Requirements:</strong> Consider the hardware requirements for inference. CycleGANs can be computationally expensive, so optimize the model for deployment on the target platform (e.g., using model quantization or pruning).</li>
<li><strong>Inference Speed:</strong> Measure the inference speed of the model and optimize it if necessary. This can involve using smaller models, optimized kernels, or hardware acceleration.</li>
<li><strong>Model Monitoring:</strong> Monitor the performance of the deployed model over time. Retrain the model periodically with new data to maintain its accuracy and robustness.</li>
<li><strong>Handling Unseen Data:</strong> Be aware that the model may not perform well on data that is significantly different from the training data. Consider implementing techniques to detect and handle out-of-distribution samples.</li>
</ul>
<p><strong>Best Practices Recap for Robustness</strong> * <strong>Prioritize Data Quality:</strong> No amount of model tuning can compensate for poor data quality. Invest time and effort in cleaning and pre-processing the data. * <strong>Regularization is Key:</strong> Use regularization techniques (e.g., weight decay, dropout) to prevent overfitting. * <strong>Monitor Training Closely:</strong> Keep a close eye on the training process and adjust the hyperparameters as needed. * <strong>Validate Thoroughly:</strong> Don’t rely solely on quantitative metrics. Conduct thorough qualitative evaluations to ensure that the generated images are visually appealing and useful. * <strong>Iterate and Refine:</strong> CycleGAN training can be challenging. Be prepared to iterate and refine your approach based on the results of your experiments.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to articulate this answer in an interview:</p>
<ol type="1">
<li><strong>Start with the Big Picture:</strong>
<ul>
<li>“Handling a CycleGAN in a messy, real-world scenario requires a comprehensive approach. I’d focus on three main areas: data preparation, robust model training, and rigorous validation.”</li>
</ul></li>
<li><strong>Data Preprocessing:</strong>
<ul>
<li>“First, a deep dive into data understanding is critical. I would analyze both the source and target domains, looking for noise, outliers, and distribution mismatches. I’d use techniques such as calculating the Fréchet Inception Distance (FID) to quantify the differences.” (If the interviewer looks interested, briefly explain FID.)</li>
<li>“Then, comes data cleaning. I would remove outliers, reduce noise using techniques like median filtering or wavelet denoising, and normalize the pixel values. Crucially, I’d augment the data with geometric transformations, color jittering, and even synthetic noise to improve robustness.”</li>
<li>“Given the mismatched distributions, I would consider domain adaptation techniques. For example, histogram matching could align the color distributions, or contrastive learning could pull embeddings of similar images closer.”</li>
</ul></li>
<li><strong>Model Training:</strong>
<ul>
<li>“For model training, I’d start with a standard CycleGAN architecture using ResNet or U-Net based generators, but then focus on customizing the loss function and using robust optimization techniques.”</li>
<li>“The loss function would be carefully engineered, balancing the adversarial loss, cycle consistency loss, and potentially an identity loss. The weights of these losses are important, so I would tune them appropriately.”</li>
<li>“To combat noise and instability, I would use gradient clipping, spectral normalization for the discriminator, and potentially different learning rates for the generator and discriminator. Adding noise to the discriminator inputs during training, i.e., noise regularization, can also increase robustness.”</li>
</ul></li>
<li><strong>Validation:</strong>
<ul>
<li>“Validation is critical to assess transformation quality. I would employ both quantitative and qualitative methods.”</li>
<li>“Quantitatively, I would use metrics like FID and LPIPS to measure the similarity and perceptual quality of the generated images.” (Briefly explain LPIPS if asked).</li>
<li>“Qualitatively, I would perform visual inspections and user studies to get human feedback on the realism and naturalness of the transformations. Cross-validation would also be used for a robust performance estimate.”</li>
<li>“I would also perform ablation studies to see what aspects of the training process are critical to performance”</li>
</ul></li>
<li><strong>Deployment &amp; Monitoring:</strong>
<ul>
<li>“Finally, for deployment, I would consider hardware requirements, optimize inference speed, and implement model monitoring to track performance over time. Handling out-of-distribution samples is also crucial.”</li>
</ul></li>
<li><strong>Communication Tips:</strong>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush. Explain the concepts clearly and concisely.</li>
<li><strong>Gauge interest:</strong> Watch the interviewer’s body language and facial expressions. If they seem confused or uninterested, adjust your explanation accordingly.</li>
<li><strong>Mathematical sections:</strong> For mathematical notations like FID, briefly explain the intuition behind the formula without getting bogged down in the details, unless the interviewer explicitly asks for a deeper explanation. For example, “FID essentially measures the distance between the feature distributions of the real and generated images”.</li>
<li><strong>Be confident:</strong> Speak with confidence and demonstrate your knowledge of the topic.</li>
<li><strong>Engage the interviewer:</strong> Ask questions like, “Does that make sense?” or “Would you like me to elaborate on any of these points?”</li>
<li><strong>Relate to real-world experience:</strong> If possible, share examples from your own experience where you have used CycleGAN or similar techniques to solve real-world problems.</li>
</ul></li>
</ol>
<p>By following this structure and communication tips, you can effectively demonstrate your senior-level expertise in CycleGAN and related techniques.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>