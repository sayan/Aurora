<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>cyclegan__stylegan__etc__5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-6.-consider-extending-cyclegan-beyond-image-translation-to-tasks-like-video-sequence-translation-or-cross-modality-translation-e.g.-audio-to-image.-what-modifications-or-additional-considerations-would-you-propose-and-what-challenges-might-you-anticipate" class="level2">
<h2 class="anchored" data-anchor-id="question-6.-consider-extending-cyclegan-beyond-image-translation-to-tasks-like-video-sequence-translation-or-cross-modality-translation-e.g.-audio-to-image.-what-modifications-or-additional-considerations-would-you-propose-and-what-challenges-might-you-anticipate">Question: 6. Consider extending CycleGAN beyond image translation to tasks like video sequence translation or cross-modality translation (e.g., audio to image). What modifications or additional considerations would you propose and what challenges might you anticipate?</h2>
<p><strong>Best Answer</strong></p>
<p>Extending CycleGAN to video sequence translation or cross-modality translation is a complex task requiring significant modifications to the original framework. The core idea of CycleGAN—unsupervised image-to-image translation using cycle consistency—provides a solid foundation, but adaptations are crucial for handling the temporal dimension in videos and the inherent differences between modalities.</p>
<p><strong>1. Video Sequence Translation:</strong></p>
<ul>
<li><p><strong>Temporal Consistency:</strong> The most significant challenge in video translation is maintaining temporal consistency. Frame-by-frame application of CycleGAN can lead to flickering or jerky transitions, disrupting the overall visual flow.</p>
<ul>
<li><p><strong>Optical Flow Regularization:</strong> Introduce an optical flow-based regularization term to encourage smoothness. Let <span class="math inline">\(O_{t, t+1}\)</span> be the optical flow between frame <span class="math inline">\(x_t\)</span> and <span class="math inline">\(x_{t+1}\)</span>. We can add a loss term: <span class="math display">\[L_{flow} = \mathbb{E}_{x \sim p(X)}[\sum_t ||O_{t, t+1} - O'_{t, t+1}||_1]\]</span> where <span class="math inline">\(O'_{t, t+1}\)</span> is the optical flow between translated frames <span class="math inline">\(G(x_t)\)</span> and <span class="math inline">\(G(x_{t+1})\)</span>. <span class="math inline">\(G\)</span> is the generator network.</p></li>
<li><p><strong>3D Convolutions:</strong> Replace 2D convolutions with 3D convolutions in the generator and discriminator networks to capture spatio-temporal features directly. This allows the network to learn correlations across adjacent frames. The convolution operation becomes: <span class="math display">\[y[i, j, k] = \sum_{l=0}^{K-1} \sum_{m=0}^{K-1} \sum_{n=0}^{K-1} x[i+l, j+m, k+n] * h[l, m, n]\]</span> Where <span class="math inline">\(x\)</span> is input volume, <span class="math inline">\(h\)</span> is the 3D kernel, and <span class="math inline">\(y\)</span> is the output volume. <span class="math inline">\(K\)</span> is the kernel size.</p></li>
<li><p><strong>Recurrent Neural Networks (RNNs):</strong> Incorporate RNNs, such as LSTMs or GRUs, to model temporal dependencies explicitly. The generator can use an RNN to process a sequence of frames and generate a consistent translated sequence. The hidden state <span class="math inline">\(h_t\)</span> at time <span class="math inline">\(t\)</span> is updated based on the current input <span class="math inline">\(x_t\)</span> and the previous hidden state <span class="math inline">\(h_{t-1}\)</span>: <span class="math display">\[h_t = f(x_t, h_{t-1})\]</span></p></li>
<li><p><strong>Temporal Cycle Consistency:</strong> Enforce cycle consistency not only in the image domain but also in the temporal domain. For example, translating a video from domain A to B and back to A should result in a video similar to the original, considering temporal dynamics. <span class="math display">\[L_{temporal\_cycle} = \mathbb{E}_{x \sim p(X)}[||F(G(x)) - x||_1]\]</span> Where <span class="math inline">\(F\)</span> is the generator for the reverse transformation (B to A), and <span class="math inline">\(G\)</span> is the generator from A to B.</p></li>
</ul></li>
<li><p><strong>Architectural Modifications:</strong></p>
<ul>
<li><strong>Video-specific Layers:</strong> Add layers tailored for video processing, such as motion estimation layers or temporal attention mechanisms.</li>
</ul></li>
</ul>
<p><strong>2. Cross-Modality Translation (e.g., Audio to Image):</strong></p>
<ul>
<li><p><strong>Modality Alignment:</strong> Aligning vastly different modalities (e.g., audio and images) is challenging. The network needs to learn meaningful correlations between the two modalities.</p>
<ul>
<li><p><strong>Shared Latent Space:</strong> Project both modalities into a shared latent space where they can be compared and manipulated. Variational Autoencoders (VAEs) or adversarial autoencoders can be used for this purpose. The VAE encodes both modalities into a latent space <span class="math inline">\(z\)</span>, trying to match the distributions: <span class="math display">\[z_A = Encoder_A(x_A)\]</span> <span class="math display">\[z_B = Encoder_B(x_B)\]</span> The objective is to minimize the distance between <span class="math inline">\(z_A\)</span> and <span class="math inline">\(z_B\)</span> and reconstruct the original inputs <span class="math inline">\(x_A\)</span> and <span class="math inline">\(x_B\)</span> from <span class="math inline">\(z_A\)</span> and <span class="math inline">\(z_B\)</span>.</p></li>
<li><p><strong>Attention Mechanisms:</strong> Use attention mechanisms to focus on relevant parts of the input from one modality when generating the output in the other modality. For example, specific audio events might correspond to specific visual elements in the generated image. The attention weight <span class="math inline">\(\alpha_{ij}\)</span> between the <span class="math inline">\(i\)</span>-th audio feature and <span class="math inline">\(j\)</span>-th image region indicates the importance of the audio feature for generating that image region. <span class="math display">\[\alpha_{ij} = softmax(a(x_i, y_j))\]</span></p></li>
<li><p><strong>Cross-Modal Cycle Consistency:</strong> Modify the cycle consistency loss to account for the differences in modalities. This can involve using different distance metrics or loss functions for each modality.</p></li>
</ul></li>
<li><p><strong>Loss Functions:</strong></p>
<ul>
<li><p><strong>Perceptual Loss:</strong> Use perceptual loss based on pre-trained networks (e.g., VGG) to ensure that the generated images are visually realistic and match the content of the audio. <span class="math display">\[L_{perceptual} = \mathbb{E}_{x \sim p(X)}[\sum_i ||\phi_i(G(x)) - \phi_i(x)||_2]\]</span> Where <span class="math inline">\(\phi_i\)</span> represents the feature maps from the <span class="math inline">\(i\)</span>-th layer of a pre-trained network (e.g., VGG), and <span class="math inline">\(G(x)\)</span> is the generated image.</p></li>
<li><p><strong>Adversarial Loss:</strong> Maintain adversarial loss to ensure the generated outputs are indistinguishable from real samples in the target modality.</p></li>
</ul></li>
<li><p><strong>Data Heterogeneity:</strong></p>
<ul>
<li><p><strong>Normalization:</strong> Normalize the input data appropriately for each modality to ensure that the network can effectively learn from both. Audio data might require normalization based on decibel levels, while image data requires pixel value scaling.</p></li>
<li><p><strong>Data Augmentation:</strong> Employ data augmentation techniques specific to each modality to increase the robustness of the model.</p></li>
</ul></li>
</ul>
<p><strong>3. Challenges:</strong></p>
<ul>
<li><strong>Computational Cost:</strong> Training CycleGANs, especially with the proposed modifications, can be computationally expensive, requiring significant GPU resources and training time.</li>
<li><strong>Mode Collapse:</strong> CycleGANs are prone to mode collapse, where the generator produces a limited variety of outputs. This can be addressed using techniques like instance normalization or spectral normalization.</li>
<li><strong>Evaluation Metrics:</strong> Evaluating the quality of translated videos or cross-modal outputs is challenging. Subjective evaluation by human observers is often necessary.</li>
<li><strong>Synchronization Issues:</strong> Accurately synchronizing data across modalities can be difficult. For example, in audio-to-image translation, ensuring that the audio and corresponding visual content are correctly aligned is crucial.</li>
<li><strong>Scalability:</strong> Scaling these models to high-resolution videos or complex cross-modal tasks requires careful consideration of memory and computational constraints. Techniques like distributed training and model parallelism may be necessary.</li>
</ul>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Foundation:</strong> “CycleGAN provides a strong foundation for unsupervised translation, but extending it to video or cross-modality tasks requires careful modifications.”</p></li>
<li><p><strong>Address Video Translation First:</strong></p>
<ul>
<li>“For video, the primary challenge is temporal consistency. Simply applying CycleGAN frame-by-frame leads to flickering.”</li>
<li>“To address this, we can use techniques like optical flow regularization. Essentially, we add a loss term that penalizes differences in optical flow between consecutive translated frames.” Briefly show the equation <span class="math inline">\(L_{flow}\)</span> but <em>don’t dwell on it</em>.</li>
<li>“Another approach is to use 3D convolutions to capture spatio-temporal features directly, or to incorporate RNNs to model temporal dependencies explicitly.”</li>
<li>“We should also consider temporal cycle consistency, ensuring that translating a video back and forth preserves its temporal dynamics.” Show <span class="math inline">\(L_{temporal\_cycle}\)</span> briefly if asked for specifics.</li>
</ul></li>
<li><p><strong>Move to Cross-Modality Translation:</strong></p>
<ul>
<li>“For cross-modality translation, the challenge is modality alignment. We need to find meaningful correlations between modalities like audio and images.”</li>
<li>“One approach is to project both modalities into a shared latent space using VAEs or adversarial autoencoders. This helps the network learn a common representation.”</li>
<li>“Attention mechanisms can also be valuable. They allow the network to focus on relevant parts of the input from one modality when generating the output in the other.” Mention the basic idea of the <span class="math inline">\(\alpha_{ij}\)</span> equation without writing it down.</li>
<li>“We’ll need to adjust the cycle consistency loss to account for the different modalities.”</li>
</ul></li>
<li><p><strong>Discuss Loss Functions and Data:</strong></p>
<ul>
<li>“Using perceptual loss, based on pre-trained networks, can help ensure visual realism. We’re essentially comparing feature maps of generated and real images.” Show the <span class="math inline">\(L_{perceptual}\)</span> equation if asked about loss specifics.</li>
<li>“Proper data normalization and augmentation are crucial, especially given the heterogeneity of modalities.”</li>
</ul></li>
<li><p><strong>Highlight Challenges:</strong></p>
<ul>
<li>“There are several challenges. The computational cost is significant, and CycleGANs are prone to mode collapse. Evaluation is also difficult, often requiring subjective human assessment.”</li>
<li>“Synchronization between modalities is critical and can be tricky to achieve.”</li>
<li>“Finally, scalability to high-resolution data requires careful consideration of memory and computational resources.”</li>
</ul></li>
<li><p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use Visual Aids (If Possible):</strong> If you are in a virtual interview, consider sharing your screen and using diagrams or illustrations to explain complex concepts.</li>
<li><strong>Engage the Interviewer:</strong> Ask if they have any questions or if they would like you to elaborate on a specific point. This shows that you are not just reciting information but are also engaged in a conversation.</li>
<li><strong>Avoid Jargon:</strong> While it’s important to demonstrate your technical knowledge, avoid using unnecessary jargon. Explain concepts in a clear and concise manner that is easy for the interviewer to understand.</li>
<li><strong>Be Confident:</strong> Speak confidently and clearly. This will help convey your expertise and demonstrate that you are comfortable with the topic.</li>
<li><strong>Focus on Key Concepts:</strong> Emphasize the key concepts and main ideas rather than getting bogged down in excessive details.</li>
<li><strong>Relate to Practical Applications:</strong> Whenever possible, relate the concepts to practical applications or real-world scenarios. This will help the interviewer understand the relevance and importance of the topic.</li>
<li><strong>Summarize:</strong> At the end of your explanation, provide a brief summary of the key points. This will help reinforce your message and ensure that the interviewer has a clear understanding of your answer.</li>
</ul></li>
</ol>
<p>By following these guidelines, you can effectively demonstrate your expertise in CycleGANs and related techniques while engaging the interviewer and ensuring they understand your points.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>