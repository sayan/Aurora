<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>transfer_learning_and_fine_tuning_strategies_11</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-in-a-scenario-where-you-need-to-scale-your-transfer-learning-model-for-deployment-e.g.-on-mobile-devices-or-in-a-distributed-system-what-considerations-would-you-take-into-account" class="level2">
<h2 class="anchored" data-anchor-id="question-in-a-scenario-where-you-need-to-scale-your-transfer-learning-model-for-deployment-e.g.-on-mobile-devices-or-in-a-distributed-system-what-considerations-would-you-take-into-account">Question: In a scenario where you need to scale your transfer learning model for deployment (e.g., on mobile devices or in a distributed system), what considerations would you take into account?</h2>
<p><strong>Best Answer</strong></p>
<p>Scaling a transfer learning model for deployment, especially on resource-constrained devices like mobile phones or in distributed systems, involves several critical considerations. The goal is to balance model accuracy with deployment feasibility, which means addressing model size, computational cost (latency), energy consumption, and system integration. Here’s a breakdown of the key areas:</p>
<section id="model-compression-techniques" class="level3">
<h3 class="anchored" data-anchor-id="model-compression-techniques">1. Model Compression Techniques</h3>
<p>The first step is usually to reduce the model size without significantly sacrificing accuracy. Several techniques can be employed:</p>
<ul>
<li><strong>Pruning:</strong> This involves removing weights or connections in the neural network that have minimal impact on performance. Structured pruning removes entire filters or channels, leading to more hardware-friendly speedups. Unstructured pruning removes individual weights, offering higher compression rates but requiring specialized hardware or software to realize speedups.
<ul>
<li><strong>Weight Pruning:</strong> Setting weights with magnitudes below a threshold to zero. The threshold can be determined empirically or through more sophisticated methods.</li>
<li><strong>Activation Pruning:</strong> Removing neurons that have consistently low activation values.</li>
<li>Formally, the objective can be framed as: <span class="math display">\[ \min_{W'} \mathcal{L}(X, Y; W') \quad \text{subject to} \quad ||W'||_0 \leq B \]</span> where <span class="math inline">\(W'\)</span> is the pruned weight matrix, <span class="math inline">\(\mathcal{L}\)</span> is the loss function, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the input and output data, respectively, and <span class="math inline">\(B\)</span> is the budget on the number of non-zero weights.</li>
</ul></li>
<li><strong>Quantization:</strong> This technique reduces the precision of the model’s weights and activations. For example, instead of using 32-bit floating-point numbers (FP32), we can use 16-bit floating-point (FP16), 8-bit integers (INT8), or even lower precisions. This reduces memory footprint and can significantly speed up computation on hardware optimized for lower precision arithmetic (e.g., mobile GPUs, TPUs).
<ul>
<li><strong>Post-Training Quantization:</strong> Quantizing the model after training. This is easier to implement but might lead to a drop in accuracy.</li>
<li><strong>Quantization-Aware Training:</strong> Simulating quantization during training, which allows the model to adapt to the reduced precision and mitigate accuracy loss.</li>
<li>Mathematically, quantization can be represented as: <span class="math display">\[ Q(x) = scale \cdot round(x / scale + zero\_point) \]</span> where <span class="math inline">\(x\)</span> is the original value, <span class="math inline">\(Q(x)\)</span> is the quantized value, <span class="math inline">\(scale\)</span> is a scaling factor, and <span class="math inline">\(zero\_point\)</span> is an offset.</li>
</ul></li>
<li><strong>Knowledge Distillation:</strong> Training a smaller “student” model to mimic the behavior of a larger, more accurate “teacher” model. The student model learns to predict the soft probabilities produced by the teacher, rather than just the hard labels. This can transfer the generalization ability of the larger model to a smaller one.
<ul>
<li>The distillation loss is typically a combination of the cross-entropy loss and a term that encourages the student model to match the teacher’s output probabilities.</li>
<li>The combined loss function can be expressed as: <span class="math display">\[ \mathcal{L} = (1 - \alpha) \mathcal{L}_{CE}(x, y) + \alpha \mathcal{L}_{KL}(p_T(x), p_S(x)) \]</span> where <span class="math inline">\(\mathcal{L}_{CE}\)</span> is the cross-entropy loss between the student’s predictions and the true labels, <span class="math inline">\(\mathcal{L}_{KL}\)</span> is the Kullback-Leibler divergence between the teacher’s and student’s output probabilities, <span class="math inline">\(\alpha\)</span> is a weighting factor, <span class="math inline">\(p_T(x)\)</span> and <span class="math inline">\(p_S(x)\)</span> are the probability distributions output by the teacher and student models, respectively, and <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are the input and the true label.</li>
</ul></li>
<li><strong>Model Decomposition:</strong> Techniques like Singular Value Decomposition (SVD) can be used to decompose weight matrices into lower-rank approximations, reducing the number of parameters.</li>
</ul>
</section>
<section id="deployment-frameworks-and-hardware-acceleration" class="level3">
<h3 class="anchored" data-anchor-id="deployment-frameworks-and-hardware-acceleration">2. Deployment Frameworks and Hardware Acceleration</h3>
<p>The choice of deployment framework and hardware is crucial:</p>
<ul>
<li><strong>Mobile Deployment:</strong>
<ul>
<li><strong>TensorFlow Lite:</strong> Optimized for mobile and embedded devices. Supports quantization and other model optimization techniques.</li>
<li><strong>Core ML:</strong> Apple’s framework for deploying models on iOS devices. Leverages the Neural Engine on Apple’s chips for hardware acceleration.</li>
<li><strong>PyTorch Mobile:</strong> A framework that enables deploying PyTorch models on mobile devices.</li>
<li><strong>ONNX Runtime:</strong> A cross-platform inference engine that supports various hardware backends.</li>
</ul></li>
<li><strong>Distributed Systems Deployment:</strong>
<ul>
<li><strong>TensorFlow Serving:</strong> A flexible, high-performance serving system for deploying models.</li>
<li><strong>TorchServe:</strong> PyTorch’s model serving framework.</li>
<li><strong>Kubeflow:</strong> An open-source machine learning platform built on Kubernetes, which enables easy deployment and scaling of models in the cloud.</li>
</ul></li>
<li><strong>Hardware Acceleration:</strong> Leveraging specialized hardware like GPUs, TPUs, or dedicated AI accelerators (e.g., Intel Movidius, NVIDIA Jetson) can significantly improve inference speed and energy efficiency.</li>
</ul>
</section>
<section id="latency-considerations" class="level3">
<h3 class="anchored" data-anchor-id="latency-considerations">3. Latency Considerations</h3>
<ul>
<li><strong>Profiling:</strong> Before deployment, it’s crucial to profile the model’s performance on the target hardware to identify bottlenecks. Tools like TensorFlow Profiler and PyTorch Profiler can help with this.</li>
<li><strong>Batching:</strong> In a distributed system, batching requests can improve throughput, but it also increases latency. The batch size should be tuned carefully to balance these two factors.</li>
<li><strong>Asynchronous Inference:</strong> Using asynchronous inference can prevent blocking the main thread and improve responsiveness, especially in mobile applications.</li>
<li><strong>Edge Computing:</strong> Pushing computation to the edge (i.e., closer to the data source) can reduce network latency and improve privacy.</li>
</ul>
</section>
<section id="system-integration" class="level3">
<h3 class="anchored" data-anchor-id="system-integration">4. System Integration</h3>
<ul>
<li><strong>API Design:</strong> The model should be exposed through a well-defined API that is easy to use and integrates seamlessly with the existing system.</li>
<li><strong>Data Preprocessing and Postprocessing:</strong> Ensure that the data preprocessing and postprocessing steps are optimized for the target environment.</li>
<li><strong>Monitoring:</strong> Implement monitoring to track model performance, detect anomalies, and identify potential issues. Key metrics to monitor include:
<ul>
<li><strong>Latency:</strong> The time it takes to process a single request.</li>
<li><strong>Throughput:</strong> The number of requests processed per second.</li>
<li><strong>Accuracy:</strong> The model’s performance on a representative dataset.</li>
<li><strong>Resource Utilization:</strong> CPU, memory, and GPU usage.</li>
</ul></li>
<li><strong>Versioning:</strong> Implement a robust versioning system to manage model updates and rollbacks.</li>
</ul>
</section>
<section id="real-world-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="real-world-trade-offs">5. Real-World Trade-offs</h3>
<ul>
<li><strong>Accuracy vs.&nbsp;Performance:</strong> There’s often a trade-off between model accuracy and performance. The acceptable level of accuracy depends on the specific application.</li>
<li><strong>Model Complexity vs.&nbsp;Resource Constraints:</strong> More complex models typically require more resources. It’s important to choose a model that is appropriate for the available resources.</li>
<li><strong>Development Time vs.&nbsp;Optimization Effort:</strong> Spending more time on model optimization can improve performance, but it also increases development time.</li>
</ul>
</section>
<section id="fine-tuning-considerations" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-considerations">6. Fine-tuning Considerations</h3>
<ul>
<li><strong>Domain Adaptation:</strong> If the deployment environment differs significantly from the training environment, fine-tuning the model on data from the deployment environment can improve performance.</li>
<li><strong>Continual Learning:</strong> If the data distribution changes over time, continual learning techniques can be used to update the model without forgetting previous knowledge.</li>
</ul>
<p>By carefully considering these factors, it’s possible to deploy transfer learning models efficiently and effectively in a variety of environments.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this to an interviewer:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“Scaling transfer learning models for deployment, especially on mobile or in distributed systems, is a multifaceted challenge. It’s all about finding the right balance between accuracy, resource usage, and latency.”</li>
<li>“Essentially, we need to make the model smaller and faster without sacrificing too much performance.”</li>
</ul></li>
<li><strong>Discuss Model Compression Techniques (Focus on a few, not all):</strong>
<ul>
<li>“One of the first things I’d look at is model compression. Several techniques are available. Let me highlight a couple…”</li>
<li><strong>Pruning:</strong> “Pruning involves removing less important connections or even entire filters from the network. There’s structured pruning, which is hardware-friendly, and unstructured pruning, which can achieve higher compression rates, though it may require specialized libraries.” (Don’t dwell too much on the formulas unless asked explicitly).</li>
<li><strong>Quantization:</strong> “Quantization reduces the precision of the model’s weights. Instead of using 32-bit floating-point numbers, we can use 8-bit integers. This can significantly reduce memory footprint and speed up computation, especially on devices with dedicated hardware for lower precision arithmetic. We can also consider Quantization-Aware Training.”</li>
<li><strong>Knowledge Distillation:</strong> “Alternatively, we can train a smaller ‘student’ model to mimic the behavior of a larger ‘teacher’ model, transferring the knowledge without the computational overhead.”</li>
</ul></li>
<li><strong>Transition to Deployment Frameworks and Hardware:</strong>
<ul>
<li>“The choice of deployment framework is also critical. Depending on the target environment, I’d consider…”</li>
<li><strong>Mobile:</strong> “For mobile, frameworks like TensorFlow Lite, Core ML, and PyTorch Mobile are designed for efficient inference on mobile devices.”</li>
<li><strong>Distributed Systems:</strong> “For distributed systems, TensorFlow Serving, TorchServe, or Kubernetes-based solutions like Kubeflow provide scalable deployment options.”</li>
<li>“Leveraging specialized hardware like GPUs or TPUs can also dramatically improve performance.”</li>
</ul></li>
<li><strong>Address Latency and System Integration:</strong>
<ul>
<li>“Latency is a key concern, especially in real-time applications. Profiling the model on the target hardware is essential to identify bottlenecks. Consider techniques like batching, asynchronous inference, and edge computing.”</li>
<li>“The model needs to integrate seamlessly into the existing system, which means designing a clean API, optimizing data preprocessing and postprocessing pipelines, and implementing robust monitoring and versioning.”</li>
</ul></li>
<li><strong>Highlight Real-World Trade-offs:</strong>
<ul>
<li>“Ultimately, deployment involves trade-offs. We need to balance accuracy with performance, model complexity with resource constraints, and development time with optimization effort.”</li>
<li>“It’s about understanding the specific requirements of the application and making informed decisions based on those requirements.”</li>
</ul></li>
<li><strong>Conclude with Fine-tuning (Optional):</strong>
<ul>
<li>“Depending on the situation, fine-tuning on data from the deployment environment (domain adaptation) or using continual learning techniques might be necessary to maintain performance over time.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush. Speak clearly and deliberately.</li>
<li><strong>Use Signposting:</strong> Use phrases like “First,” “Second,” “Another important aspect is…” to guide the interviewer through your explanation.</li>
<li><strong>Be Prepared to Dive Deeper:</strong> The interviewer may ask follow-up questions about specific techniques. Be ready to provide more detail or examples.</li>
<li><strong>Don’t Be Afraid to Say “It Depends”:</strong> Deployment decisions are often context-dependent. Acknowledge this and explain the factors that would influence your decision.</li>
<li><strong>Ask Clarifying Questions:</strong> If the question is ambiguous, ask for clarification. For example, “Are we deploying to iOS or Android?,” “What are the latency requirements?”</li>
<li><strong>Be Confident but Humble:</strong> Show that you have a strong understanding of the topic, but also acknowledge that there’s always more to learn.</li>
<li><strong>When discussing equations:</strong> Explain in plain english what the different symbols mean and the overall purpose of the equation. Avoid diving into rigorous mathematical proofs unless specifically asked.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>