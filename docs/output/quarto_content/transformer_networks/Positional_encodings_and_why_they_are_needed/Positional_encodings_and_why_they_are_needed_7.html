<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>positional_encodings_and_why_they_are_needed_7</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-8.-discuss-challenges-and-considerations-when-integrating-positional-encodings-in-multimodal-architectures-for-instance-combining-text-with-image-features." class="level2">
<h2 class="anchored" data-anchor-id="question-8.-discuss-challenges-and-considerations-when-integrating-positional-encodings-in-multimodal-architectures-for-instance-combining-text-with-image-features.">Question: 8. Discuss challenges and considerations when integrating positional encodings in multimodal architectures, for instance, combining text with image features.</h2>
<p><strong>Best Answer</strong></p>
<p>Integrating positional encodings in multimodal architectures, especially when combining modalities like text and images, presents several significant challenges and requires careful consideration. These challenges arise from the inherent differences in the nature and structure of the modalities themselves.</p>
<section id="why-positional-encodings-are-needed" class="level3">
<h3 class="anchored" data-anchor-id="why-positional-encodings-are-needed">Why Positional Encodings are Needed</h3>
<p>Positional encodings are crucial in architectures like Transformers because the self-attention mechanism is permutation-invariant. In simpler terms, self-attention processes all input elements simultaneously and doesn’t inherently account for the order or position of these elements. Positional encodings inject information about the position of tokens or features, enabling the model to understand sequential or spatial relationships.</p>
</section>
<section id="challenges-in-multimodal-integration" class="level3">
<h3 class="anchored" data-anchor-id="challenges-in-multimodal-integration">Challenges in Multimodal Integration</h3>
<ol type="1">
<li><p><strong>Different Spatial or Temporal Structures</strong>:</p>
<ul>
<li><p><strong>Text</strong>: Text data has a sequential, one-dimensional structure. Words appear in a specific order, and this order is critical to meaning. Positional encodings capture this temporal relationship directly.</p></li>
<li><p><strong>Images</strong>: Images, on the other hand, possess a two-dimensional spatial structure. Pixels are arranged in a grid, and their relative positions determine the objects and scenes depicted. We might represent an image as a sequence of flattened patches, but simply concatenating positional encodings in a 1D manner will fail to capture 2D spatial relationships effectively.</p></li>
<li><p><strong>Challenge</strong>: Aligning and integrating these fundamentally different structures is not trivial. A positional encoding scheme designed for text may not be directly applicable or effective for images, and vice versa.</p></li>
</ul></li>
<li><p><strong>Varying Semantic Density</strong>:</p>
<ul>
<li>Text often carries a high semantic load in each token. The position of a word can significantly alter the meaning of a sentence.</li>
<li>Images, especially when processed as patches or features, may have a more distributed semantic representation. The meaning is often derived from the collective arrangement of features rather than individual feature positions.</li>
<li>Challenge: The <em>importance</em> of positional information can vary across modalities. A multimodal model must account for these differences when weighting or fusing positional encodings.</li>
</ul></li>
<li><p><strong>Encoding Scheme Compatibility</strong>:</p>
<ul>
<li><p>Different modalities may require distinct encoding schemes to effectively capture their inherent structure. For example, text commonly uses sinusoidal positional encodings or learned embeddings. Images may benefit from 2D positional encodings or convolutional approaches that implicitly encode spatial information.</p></li>
<li><p>Challenge: Ensuring compatibility between these different encoding schemes and designing a fusion mechanism that can effectively combine them poses a design challenge.</p></li>
</ul></li>
<li><p><strong>Cross-Modal Alignment</strong>:</p>
<ul>
<li>The goal of a multimodal architecture is often to understand the relationships <em>between</em> modalities. Positional encodings play a role in this by helping the model attend to the correct parts of each modality when performing cross-modal attention.</li>
<li>Challenge: If positional encodings are not aligned or are not informative enough, cross-modal attention mechanisms may fail to learn meaningful relationships.</li>
</ul></li>
</ol>
</section>
<section id="possible-approaches-and-considerations" class="level3">
<h3 class="anchored" data-anchor-id="possible-approaches-and-considerations">Possible Approaches and Considerations</h3>
<ol type="1">
<li><p><strong>Separate Encoding Schemes</strong>:</p>
<ul>
<li><p>Employ distinct positional encoding schemes for each modality tailored to its specific characteristics. For text, sinusoidal encodings or learned embeddings can be used. For images, consider:</p>
<ul>
<li><p><strong>2D Positional Encodings</strong>: Extend 1D positional encodings to two dimensions to directly encode the row and column indices of image patches. This can be achieved by encoding <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> coordinates independently. <span class="math display">\[
PE(pos, 2i) = sin(\frac{pos}{10000^{2i/d}})
\]</span> <span class="math display">\[
PE(pos, 2i+1) = cos(\frac{pos}{10000^{2i/d}})
\]</span> Where <span class="math inline">\(pos\)</span> is the position, <span class="math inline">\(i\)</span> is the dimension, and <span class="math inline">\(d\)</span> is the dimensionality of the positional encoding. This can be adapted for 2D by applying this formula to <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> coordinates separately and concatenating or adding the resulting encodings.</p></li>
<li><p><strong>Relative Positional Encodings</strong>: Focus on the relative distances between image patches rather than absolute positions. This can be more robust to variations in image size and resolution.</p></li>
<li><p><strong>Convolutional Approaches</strong>: Use convolutional layers early in the image processing pipeline. Convolutions inherently encode spatial relationships through their receptive fields and weight sharing.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Fusion Strategies</strong>:</p>
<ul>
<li><p><strong>Early Fusion</strong>: Concatenate or add positional encodings <em>before</em> feeding the data into the Transformer layers. This is simple but may not be optimal if the modalities have very different scales or distributions. <span class="math display">\[
x_{fused} = Concat(PE_{text}(x_{text}), PE_{image}(x_{image}))
\]</span></p></li>
<li><p><strong>Late Fusion</strong>: Apply positional encodings to each modality separately and fuse the representations <em>after</em> they have been processed by individual Transformer encoders. This allows each modality to learn its own representation before interaction.</p></li>
<li><p><strong>Attention-Based Fusion</strong>: Use cross-modal attention mechanisms to dynamically weight and combine the positional encodings from different modalities. This allows the model to learn which positional information is most relevant for a given task. For example, a cross-attention mechanism could be defined as: <span class="math display">\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span> Where <span class="math inline">\(Q\)</span> is the query (e.g., text representation), <span class="math inline">\(K\)</span> is the key (e.g., image representation with positional encoding), and <span class="math inline">\(V\)</span> is the value (e.g., image representation with positional encoding). The attention weights will then reflect the relevance of image positions to the text query.</p></li>
</ul></li>
<li><p><strong>Normalization and Scaling</strong>:</p>
<ul>
<li>Ensure that the positional encodings from different modalities are normalized or scaled appropriately before fusion. This prevents one modality from dominating the others due to differences in magnitude.</li>
</ul></li>
<li><p><strong>Task-Specific Considerations</strong>:</p>
<ul>
<li>The optimal approach to integrating positional encodings will depend on the specific task. For example, image captioning might benefit from aligning text and image positions at a fine-grained level, while visual question answering might require a more abstract representation of spatial relationships.</li>
</ul></li>
<li><p><strong>Learnable vs.&nbsp;Fixed Encodings</strong>:</p>
<ul>
<li>Consider whether to use fixed positional encodings (e.g., sinusoidal) or learnable embeddings. Learnable embeddings can adapt to the specific dataset and task, but they may also require more data to train effectively.</li>
</ul></li>
<li><p><strong>Handling Variable Input Sizes</strong>:</p>
<ul>
<li>Multimodal architectures often need to handle inputs of variable sizes (e.g., different length sentences, different resolution images). Ensure that the positional encoding scheme can accommodate these variations. For fixed positional encodings, this might involve interpolation or padding. For learnable embeddings, consider using a maximum sequence length or dynamic sequence length bucketing.</li>
</ul></li>
</ol>
<p>In summary, effectively integrating positional encodings in multimodal architectures requires careful consideration of the inherent differences between modalities, the design of appropriate encoding schemes, and the selection of a suitable fusion strategy. Experimentation and task-specific tuning are often necessary to achieve optimal performance.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to present this answer in an interview setting:</p>
<ol type="1">
<li><p><strong>Start with the Basics (Context)</strong>:</p>
<ul>
<li>“Positional encodings are essential in Transformer architectures because the self-attention mechanism is permutation-invariant. They provide information about the order or position of elements in the input sequence.”</li>
<li>“When we move to multimodal architectures, especially combining text and images, integrating positional encodings becomes more complex because of the fundamental differences in these modalities.”</li>
</ul></li>
<li><p><strong>Explain the Challenges (Highlight Key Issues)</strong>:</p>
<ul>
<li>“One of the main challenges is the differing spatial or temporal structures. Text is sequential, while images are spatial. Simply applying the same positional encoding to both doesn’t work well.” (Pause, allow for a nod or indication of understanding).</li>
<li>“Another challenge lies in varying semantic density. The position of a word can drastically change meaning, but the meaning in images is more distributed across pixel arrangements.”</li>
<li>“Finally, different encoding schemes like sinusoidal for text and potentially 2D encodings for images need to be made compatible to ensure effective cross-modal alignment.”</li>
</ul></li>
<li><p><strong>Discuss Possible Approaches (Offer Solutions)</strong>:</p>
<ul>
<li>“To address these challenges, several approaches can be taken. One is to use separate encoding schemes tailored to each modality. For images, we might consider 2D positional encodings, relative encodings, or even rely on the spatial encoding inherent in convolutional layers.” (Briefly explain one of the 2D positional encoding methods, without diving too deep into the equations unless asked).</li>
<li>“Regarding fusion strategies, early fusion, late fusion, and attention-based fusion are options. Attention-based fusion is particularly promising as it allows the model to dynamically weigh positional information from different modalities.”</li>
<li>“Normalization is important to ensure that no one modality overpowers the other due to differences in encoding magnitudes.</li>
</ul></li>
<li><p><strong>Address Task Specificity and Practical Considerations (Demonstrate Depth)</strong>:</p>
<ul>
<li>“The optimal approach is very task-dependent. Image captioning, for instance, needs fine-grained alignment, while visual question answering might do better with a more abstract spatial representation.”</li>
<li>“Whether to use fixed or learned encodings is another consideration. Learnable encodings are more flexible, but require more data.”</li>
<li>“Handling variable-sized inputs, a common scenario, is also vital. This calls for mechanisms to deal with varying sentence and image sizes.”</li>
</ul></li>
<li><p><strong>Conclude with Summary (Reinforce Understanding)</strong>:</p>
<ul>
<li>“In summary, effectively integrating positional encodings in multimodal architectures requires careful consideration of the modality-specific characteristics, design of encoding schemes, and selection of a suitable fusion strategy. Experimentation and tuning are key to success.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself</strong>: Don’t rush. Give the interviewer time to process the information.</li>
<li><strong>Use Visual Aids (If Possible)</strong>: If in person, consider sketching a simple diagram to illustrate the different fusion strategies. If remote, consider having a slide prepared.</li>
<li><strong>Gauge Understanding</strong>: Watch for cues that the interviewer is following along. If they seem confused, pause and offer clarification. Ask, “Does that make sense?” or “Would you like me to elaborate on that point?”</li>
<li><strong>Simplify Math</strong>: If you mention an equation, explain its purpose in plain English. For instance, instead of just writing the attention equation, say, “This formula calculates attention weights, which essentially tell us how much each image patch should contribute to understanding the text.”</li>
<li><strong>Be Ready to Elaborate</strong>: The interviewer may ask for more detail on a specific point. Be prepared to provide deeper explanations and examples.</li>
<li><strong>Don’t Be Afraid to Say “It Depends”</strong>: The optimal solution often depends on the specific problem. Acknowledge this and explain the factors that would influence your decision. This shows practical wisdom.</li>
<li><strong>End Strong</strong>: Summarize your main points and reiterate the importance of experimentation and tuning.</li>
</ul>
<p>By following these steps, you can deliver a comprehensive and compelling answer that demonstrates your senior-level expertise in multimodal machine learning.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>