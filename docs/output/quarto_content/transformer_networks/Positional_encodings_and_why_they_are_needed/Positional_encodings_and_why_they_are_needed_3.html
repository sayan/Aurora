<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>positional_encodings_and_why_they_are_needed_3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-4.-how-do-positional-encodings-integrate-with-the-self-attention-mechanism-in-transformers-please-provide-a-mathematical-explanation-or-formulation-if-possible." class="level2">
<h2 class="anchored" data-anchor-id="question-4.-how-do-positional-encodings-integrate-with-the-self-attention-mechanism-in-transformers-please-provide-a-mathematical-explanation-or-formulation-if-possible.">Question: 4. How do positional encodings integrate with the self-attention mechanism in transformers? Please provide a mathematical explanation or formulation if possible.</h2>
<p><strong>Best Answer</strong></p>
<p>Positional encodings are a crucial component of the Transformer architecture, particularly because the self-attention mechanism itself is permutation-invariant. This means that if you shuffle the order of the input tokens, the self-attention mechanism will produce the same output. While this is desirable in some contexts, most natural language tasks are sensitive to the order of words. Positional encodings are designed to inject information about the position of tokens in the sequence into the model.</p>
<p>Here’s a breakdown of how positional encodings work and their interaction with self-attention, including a mathematical perspective:</p>
<p><strong>1. The Need for Positional Encodings:</strong></p>
<p>Traditional recurrent neural networks (RNNs) inherently process sequential data in order, implicitly capturing positional information. However, Transformers, to enable parallelization and capture long-range dependencies more effectively, process the entire input sequence at once. As a result, they need an explicit way to encode the position of each token.</p>
<p><strong>2. Positional Encoding Methods:</strong></p>
<p>There are two primary ways to incorporate positional information:</p>
<ul>
<li><p><strong>Learned Positional Encodings:</strong> These are embedding vectors that are learned during training, just like word embeddings. The index of the word becomes the input. The positional encodings, <span class="math inline">\(P \in \mathbb{R}^{max\_sequence\_length \times embedding\_dimension}\)</span>, are trainable parameters.</p></li>
<li><p><strong>Fixed Positional Encodings:</strong> These are pre-defined encoding vectors that are not learned during training. The original Transformer paper uses sinusoidal functions to create these encodings.</p></li>
</ul>
<p>We will focus on the <em>fixed sinusoidal positional encodings</em>, as they are conceptually interesting and were used in the original paper. They are defined as:</p>
<p><span class="math display">\[
PE_{(pos, 2i)} = sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span></p>
<p><span class="math display">\[
PE_{(pos, 2i+1)} = cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(pos\)</span> is the position of the token in the sequence (ranging from 0 to <span class="math inline">\(max\_sequence\_length -1\)</span>).</li>
<li><span class="math inline">\(i\)</span> is the dimension index (ranging from 0 to <span class="math inline">\(d_{model}/2 - 1\)</span>).</li>
<li><span class="math inline">\(d_{model}\)</span> is the dimensionality of the embedding space.</li>
<li><span class="math inline">\(PE_{(pos, j)}\)</span> is the positional encoding for position <span class="math inline">\(pos\)</span> and dimension <span class="math inline">\(j\)</span>.</li>
</ul>
<p><strong>3. Integration with Input Embeddings:</strong></p>
<p>Before the input sequence enters the first layer of the Transformer, the positional encodings are <em>added</em> to the input embeddings. Let <span class="math inline">\(X \in \mathbb{R}^{sequence\_length \times d_{model}}\)</span> be the input embeddings. The combined input <span class="math inline">\(Z\)</span> to the first layer is:</p>
<p><span class="math display">\[
Z = X + PE
\]</span></p>
<p>where <span class="math inline">\(PE \in \mathbb{R}^{sequence\_length \times d_{model}}\)</span> is the positional encoding matrix, with each row corresponding to the positional encoding for the corresponding position. The addition operation ensures that the positional information is embedded within the input representation.</p>
<p><strong>4. Impact on Self-Attention:</strong></p>
<p>The self-attention mechanism calculates attention weights based on the similarity between the “query” (<span class="math inline">\(Q\)</span>), “key” (<span class="math inline">\(K\)</span>), and “value” (<span class="math inline">\(V\)</span>) matrices. These matrices are obtained by linearly transforming the combined input <span class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[
Q = ZW_Q
\]</span> <span class="math display">\[
K = ZW_K
\]</span> <span class="math display">\[
V = ZW_V
\]</span></p>
<p>where <span class="math inline">\(W_Q, W_K, W_V \in \mathbb{R}^{d_{model} \times d_k}\)</span> are the weight matrices for the query, key, and value transformations (<span class="math inline">\(d_k\)</span> is the dimensionality of the key/query space, often equal to <span class="math inline">\(d_{model}/n\_heads\)</span>).</p>
<p>The attention weights are then computed using the scaled dot-product attention:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>Crucially, because the positional encodings <span class="math inline">\(PE\)</span> are added to <span class="math inline">\(X\)</span> to form <span class="math inline">\(Z\)</span>, they influence the values of <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span>. Consider the dot product <span class="math inline">\(QK^T\)</span> which forms the core of the attention mechanism. <span class="math display">\[
QK^T = (X + PE)W_Q ((X + PE)W_K)^T = (X + PE)W_Q W_K^T(X + PE)^T
\]</span></p>
<p>The dot product between the query and key now incorporates information about the positions of the tokens. Because the dot product reflects similarity, the self-attention mechanism can now “attend” to other tokens based not only on their semantic similarity but also on their positional relationships. The network can learn to use these positional relationships to understand word order, syntactic structure, and long-range dependencies.</p>
<p><strong>5. Properties of Sinusoidal Encodings (Why Sinusoids?):</strong></p>
<ul>
<li><p><strong>Uniqueness:</strong> Sinusoidal functions with different frequencies create unique patterns for each position, allowing the model to distinguish between them.</p></li>
<li><p><strong>Generalization to Longer Sequences:</strong> The sinusoidal functions allow the model to extrapolate to sequence lengths longer than those seen during training because the relative positional relationships are preserved.</p></li>
<li><p><strong>Relative Position Encoding:</strong> The original paper notes that for any fixed offset <em>k</em>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear function of <span class="math inline">\(PE_{pos}\)</span>. That is, <span class="math inline">\(PE_{pos+k} = M \cdot PE_{pos}\)</span>, where <span class="math inline">\(M\)</span> is a matrix. This allows the model to easily attend to tokens at a consistent relative offset. This property arises because the sines and cosines can be expressed as linear transformations of each other using trigonometric identities. For example, <span class="math inline">\(sin(a+b) = sin(a)cos(b) + cos(a)sin(b)\)</span> and <span class="math inline">\(cos(a+b) = cos(a)cos(b) - sin(a)sin(b)\)</span>.</p></li>
</ul>
<p><strong>6. Implementation Considerations:</strong></p>
<ul>
<li><strong>Pre-computation:</strong> Positional encodings are typically pre-computed and stored in a lookup table for efficiency.</li>
<li><strong>Normalization:</strong> Normalizing the input embeddings and positional encodings can sometimes improve training stability.</li>
<li><strong>Alternative Encoding Schemes:</strong> While sinusoidal encodings are common, other fixed or learned encodings can be used, depending on the specific application.</li>
<li><strong>Relative Positional Encodings:</strong> In relative positional encodings, instead of encoding the absolute position, the model encodes the relative distance between tokens. This can be particularly effective for tasks where the precise absolute position is less important than the relationships between tokens.</li>
</ul>
<p>In summary, positional encodings are an essential component of the Transformer architecture. By injecting positional information into the input embeddings, they enable the self-attention mechanism to consider the order of tokens in the sequence, leading to improved performance on a wide range of natural language processing tasks. The mathematical formulation highlights how the addition of positional information influences the attention weights, allowing the model to learn relationships based on both semantic content and position.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to deliver this answer effectively in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Why:</strong> Begin by emphasizing <em>why</em> positional encodings are necessary in the first place. Mention the permutation-invariant nature of self-attention and the importance of word order in language.</p>
<ul>
<li>“The self-attention mechanism is inherently permutation-invariant, meaning it doesn’t inherently understand the order of words. However, word order is crucial in language, so we need a way to inject positional information.”</li>
</ul></li>
<li><p><strong>Explain the High-Level Idea:</strong> Briefly describe the general idea of positional encodings – vectors added to word embeddings.</p>
<ul>
<li>“Positional encodings are vectors that are added to the input word embeddings to provide information about the position of each word in the sequence.”</li>
</ul></li>
<li><p><strong>Introduce Different Types:</strong> Mention that there are learned and fixed positional encodings. State you will focus on fixed positional encodings.</p>
<ul>
<li>“There are two main types of positional encodings: learned and fixed. I’ll focus on the fixed sinusoidal encodings used in the original Transformer paper, as they have some interesting properties.”</li>
</ul></li>
<li><p><strong>Present the Math (Carefully):</strong> Introduce the sinusoidal formulas, explaining the variables involved. Don’t dive into <em>every</em> detail at once.</p>
<ul>
<li>“The sinusoidal encodings are defined by these equations [Write or display equations]. <em>pos</em> represents the position, <em>i</em> is the dimension index, and <span class="math inline">\(d_{model}\)</span> is the embedding dimension. Essentially, each position is encoded by a vector of sines and cosines with different frequencies.”</li>
</ul></li>
<li><p><strong>Explain the Addition:</strong> Clearly state that positional encodings are <em>added</em> to the input embeddings.</p>
<ul>
<li>“These positional encodings are then <em>added</em> to the word embeddings before being fed into the Transformer layers.” You can write <span class="math inline">\(Z = X + PE\)</span>.</li>
</ul></li>
<li><p><strong>Connect to Self-Attention:</strong> Explain how the added positional information affects the query, key, and value matrices and, consequently, the attention weights.</p>
<ul>
<li>“Because the positional encodings are added to the input embeddings, they influence the query and key matrices in the self-attention mechanism. This means that the attention weights are now based not only on semantic similarity but also on positional relationships.”</li>
</ul></li>
<li><p><strong>Highlight Sinusoidal Properties (If Time Allows):</strong> Briefly mention the benefits of sinusoidal encodings, such as their ability to generalize to longer sequences and encode relative positions.</p>
<ul>
<li>“One advantage of using sinusoidal functions is that they allow the model to extrapolate to longer sequences than those seen during training. Also, they encode relative positional information, which allows the model to easily attend to tokens at a consistent relative offset.” You can write <span class="math inline">\(PE_{pos+k} = M \cdot PE_{pos}\)</span>.</li>
</ul></li>
<li><p><strong>Mention Implementation Details (Briefly):</strong> Mention pre-computation and normalization as practical considerations.</p>
<ul>
<li>“In practice, positional encodings are often pre-computed for efficiency. Normalizing the input embeddings and positional encodings can also improve training stability.”</li>
</ul></li>
<li><p><strong>End with a Summary:</strong> Reiterate the importance of positional encodings and their impact on Transformer performance.</p>
<ul>
<li>“In summary, positional encodings are critical for Transformers because they allow the model to understand the order of words in the sequence, leading to improved performance on NLP tasks.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the mathematical explanations. Give the interviewer time to process the information.</li>
<li><strong>Visual Aids:</strong> If possible, use a whiteboard or virtual drawing tool to illustrate the equations and concepts.</li>
<li><strong>Check for Understanding:</strong> Periodically ask the interviewer if they have any questions or if you should clarify anything.</li>
<li><strong>Tailor Your Answer:</strong> Adjust the level of detail and complexity based on the interviewer’s background and the flow of the conversation. If they are very technical, you can dig deeper into the linear algebra aspects. If they are more product-focused, highlight the benefits and practical implications.</li>
<li><strong>Be Confident:</strong> Speak clearly and confidently, demonstrating your expertise in the topic.</li>
</ul>
<p>By following these guidelines, you can effectively explain the integration of positional encodings with the self-attention mechanism in Transformers, showcasing your senior-level knowledge and communication skills.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>