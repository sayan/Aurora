<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>positional_encodings_and_why_they_are_needed_6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-7.-in-handling-variable-length-inputs-or-sequences-extending-beyond-the-training-distribution-what-modifications-or-techniques-might-be-needed-for-positional-encodings" class="level2">
<h2 class="anchored" data-anchor-id="question-7.-in-handling-variable-length-inputs-or-sequences-extending-beyond-the-training-distribution-what-modifications-or-techniques-might-be-needed-for-positional-encodings">Question: 7. In handling variable-length inputs or sequences extending beyond the training distribution, what modifications or techniques might be needed for positional encodings?</h2>
<p><strong>Best Answer</strong></p>
<p>Positional encodings are crucial in sequence models like Transformers because, unlike recurrent neural networks (RNNs), Transformers process all sequence elements in parallel. Therefore, positional encodings provide information about the position of each element in the sequence. Without them, the model would be permutation-invariant, meaning it wouldn’t distinguish between different orderings of the same elements.</p>
<p>The original Transformer paper uses sinusoidal positional encodings, but learned embeddings are also common. Handling variable-length inputs or sequences longer than those seen during training requires careful consideration, as positional encodings are inherently tied to sequence length. Let’s examine several techniques and their implications:</p>
<p><strong>1. Sinusoidal Positional Encodings (Original Transformer):</strong></p>
<ul>
<li><p><strong>Formula:</strong> The original paper uses sine and cosine functions of different frequencies:</p>
<p><span class="math display">\[
PE(pos, 2i) = sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span></p>
<p><span class="math display">\[
PE(pos, 2i+1) = cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(pos\)</span> is the position in the sequence.</li>
<li><span class="math inline">\(i\)</span> is the dimension index.</li>
<li><span class="math inline">\(d_{model}\)</span> is the dimensionality of the positional encoding (and the model’s embedding).</li>
</ul></li>
<li><p><strong>Extrapolation:</strong> Sinusoidal encodings inherently provide a degree of extrapolation. Because the functions are periodic, they continue to generate values for positions beyond the training sequence length. However, extrapolation performance degrades as the sequence length increases <em>significantly</em> beyond the training range, because the model hasn’t explicitly learned relationships for those distant positions. While the encoding values exist, their semantic meaning might drift.</p></li>
<li><p><strong>Variable Lengths During Inference:</strong> For sequences shorter than the maximum training length, we simply use the first <span class="math inline">\(n\)</span> positional encodings, where <span class="math inline">\(n\)</span> is the length of the input sequence.</p></li>
<li><p><strong>Longer Sequences than Training:</strong> For sequences longer than the maximum length seen during training, one can directly apply the positional encodings as defined above. The model <em>might</em> generalize to longer sequences, especially if it has learned position-invariant features. However, the performance may degrade, and fine-tuning on longer sequences is generally recommended.</p></li>
</ul>
<p><strong>2. Learned Positional Embeddings:</strong></p>
<ul>
<li>Instead of using a fixed formula, learned positional embeddings are parameters that the model learns during training. A lookup table maps each position index to a corresponding embedding vector.</li>
<li><strong>Limitation with Extrapolation:</strong> A major limitation of learned embeddings is their inability to extrapolate to sequence lengths longer than those seen during training. If the maximum training length is <span class="math inline">\(L\)</span>, the model will only have embeddings for positions <span class="math inline">\(0\)</span> to <span class="math inline">\(L-1\)</span>.</li>
<li><strong>Possible Solutions for Longer Sequences:</strong>
<ul>
<li><strong>Retraining:</strong> The most reliable solution is to retrain the model with a larger maximum sequence length. This can be computationally expensive.</li>
<li><strong>Interpolation:</strong> You can interpolate the learned embeddings to cover longer sequence lengths. For example, if you need an embedding for position <span class="math inline">\(L+1\)</span> and you have embeddings for <span class="math inline">\(L-1\)</span> and <span class="math inline">\(L\)</span>, you could linearly interpolate between them. However, the effectiveness of interpolation decreases as the gap between known positions increases. This might also require some fine-tuning to adapt.</li>
<li><strong>Fine-tuning with extrapolated embeddings:</strong> Another strategy involves initializing positional embeddings for lengths exceeding the trained length using interpolation or random initialization, followed by fine-tuning the model on sequences of the extended length.</li>
</ul></li>
</ul>
<p><strong>3. Relative Positional Encodings:</strong></p>
<ul>
<li><p><strong>Concept:</strong> Instead of encoding absolute positions, relative positional encodings encode the <em>relative distance</em> between tokens. This is particularly useful when the absolute position is less important than the relationship between tokens. The relative distance between tokens <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is simply <span class="math inline">\(i - j\)</span>.</p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li><strong>Better Generalization to Variable Lengths:</strong> Relative positional encodings can generalize better to variable-length sequences because they focus on relationships between tokens rather than absolute positions. The model learns how tokens relate to each other regardless of their absolute positions. For example, the T5 model uses relative positional embeddings.</li>
<li><strong>Extrapolation:</strong> Extrapolation with relative position embeddings is generally smoother, as the model can learn position-invariant features based on relative distance.</li>
</ul></li>
<li><p><strong>Implementation:</strong> In self-attention, the attention weights are modified based on the relative position between the query and key.</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax\left(\frac{QK^T + R}{ \sqrt{d_k}}\right)V
\]</span></p>
<p>Where <span class="math inline">\(R\)</span> represents the relative positional encoding matrix. <span class="math inline">\(R_{ij}\)</span> is the positional encoding representing the distance between token <span class="math inline">\(i\)</span> and token <span class="math inline">\(j\)</span>.</p></li>
</ul>
<p><strong>4. Extending Positional Encodings via Periodic Extrapolation (for sinusoidal):</strong></p>
<ul>
<li><p>If we consider sinusoidal positional encodings, we can view the basic idea as the use of Fourier features. Extending this, one can explicitly model the period of the underlying sine and cosine functions. If we observe the model struggles with sequences significantly longer, we can adaptively learn these periods by introducing learnable scaling factors to the <code>pos</code> variable in the formula. That is, optimize scaling parameters <span class="math inline">\(s_i\)</span> for each dimension <span class="math inline">\(i\)</span> such that:</p>
<p><span class="math display">\[
  PE(pos, 2i) = sin\left(\frac{pos * s_i}{10000^{2i/d_{model}}}\right)
  \]</span></p>
<p><span class="math display">\[
  PE(pos, 2i+1) = cos\left(\frac{pos * s_i}{10000^{2i/d_{model}}}\right)
  \]</span></p>
<p>This adaptive scaling could help the model “compress” the positional space more effectively.</p></li>
</ul>
<p><strong>5. Considerations for Very Long Sequences:</strong></p>
<ul>
<li><strong>Memory Constraints:</strong> Very long sequences can lead to memory issues due to the quadratic complexity of the attention mechanism (<span class="math inline">\(O(n^2)\)</span>). Techniques like sparse attention, longformer attention, or other attention mechanisms with sub-quadratic complexity are necessary.</li>
<li><strong>Computational Cost:</strong> Processing very long sequences can be computationally expensive. Consider using techniques like gradient accumulation or mixed-precision training to reduce the computational burden.</li>
<li><strong>Positional Encoding Resolution:</strong> For extremely long sequences, standard positional encodings might not provide sufficient resolution to differentiate between closely spaced tokens. You might need to increase the dimensionality of the positional encodings or use a hierarchical positional encoding scheme.</li>
</ul>
<p><strong>In summary:</strong> The choice of positional encoding and the strategy for handling variable-length inputs depends on the specific application and the expected range of sequence lengths. Sinusoidal encodings offer some degree of out-of-the-box extrapolation but might degrade for very long sequences. Learned embeddings are more powerful within the training range but require retraining or interpolation for longer sequences. Relative positional encodings often provide better generalization and extrapolation capabilities. For extremely long sequences, memory and computational constraints become significant, requiring specialized attention mechanisms and optimization techniques.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to present this information in an interview:</p>
<ol type="1">
<li><strong>Start with the Basics (Context):</strong>
<ul>
<li>“Positional encodings are essential in Transformers because, unlike RNNs, they process the entire sequence in parallel. This means we need a mechanism to inject information about the order of tokens.”</li>
<li>“Without positional encodings, the model would be permutation-invariant, and the order of words would not matter.”</li>
</ul></li>
<li><strong>Introduce Sinusoidal Encodings (If the Interviewer Seems Less Technical, Keep This High-Level):</strong>
<ul>
<li><p>“The original Transformer paper used sinusoidal positional encodings, which are based on sine and cosine functions of different frequencies. The key benefit is some level of ‘free’ extrapolation because of the periodic nature of the functions.”</p></li>
<li><p>(If they want more detail) “The formula looks like this: (Write the formula on a whiteboard or virtually share your screen)</p>
<p><span class="math display">\[
PE(pos, 2i) = sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span></p>
<p><span class="math display">\[
PE(pos, 2i+1) = cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span></p>
<p>where <code>pos</code> is the position, <code>i</code> is the dimension, and <code>d_model</code> is the dimensionality of the model. “Emphasize <em>why</em> these are useful.”</p></li>
</ul></li>
<li><strong>Discuss Learned Embeddings:</strong>
<ul>
<li>“An alternative is to use learned positional embeddings. Here, the model learns a vector for each position during training. The advantage is that the model can optimize these embeddings for the specific task.”</li>
<li>“However, the downside is that learned embeddings don’t naturally extrapolate to sequence lengths longer than those seen during training. This is where things get interesting.”</li>
</ul></li>
<li><strong>Explain the Challenges and Solutions for Extrapolation (Focus on Practical Considerations):</strong>
<ul>
<li>“When dealing with sequences longer than the training length, there are a few options for Learned Embeddings: Retraining with longer sequences is the most robust but computationally expensive. Interpolation is an option but might not be very accurate for significantly longer sequences.”</li>
<li>“For Sinusoidal, the extrapolation might degrade in practice because the model has not <em>learned</em> those long-range dependencies. Fine-tuning on longer sequences is usually needed.”</li>
</ul></li>
<li><strong>Introduce Relative Positional Encodings (Emphasize Benefits):</strong>
<ul>
<li>“A more elegant solution is to use relative positional encodings. Instead of encoding absolute positions, we encode the distance between tokens. This often leads to better generalization to variable-length sequences.”</li>
<li>“The model learns how tokens relate to each other irrespective of absolute position.”</li>
<li>(If they want more detail) “In the attention mechanism, the attention weights are modified based on the relative position like so… (Show the equation). Where R represents relative position between query and key tokens”</li>
</ul></li>
<li><strong>Address Very Long Sequences (Show Awareness of Limitations):</strong>
<ul>
<li>“For <em>extremely</em> long sequences, other challenges arise, like memory constraints due to the quadratic complexity of attention. That’s where techniques like sparse attention become necessary.”</li>
<li>“Also, for very long sequences, the resolution of the positional encodings themselves can become an issue. You might need higher-dimensional encodings or hierarchical schemes.”</li>
</ul></li>
<li><strong>Conclude with a Summary:</strong>
<ul>
<li>“In summary, the right approach depends on the application and the range of sequence lengths. Sinusoidal encodings offer some extrapolation, learned embeddings can be more powerful within the training range but need careful handling for longer sequences, and relative positional encodings often generalize best. And for extremely long sequences, we need to worry about memory and computation.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Gauge the Interviewer’s Level:</strong> Start with a high-level explanation and then add technical details based on their reactions and questions.</li>
<li><strong>Use Visual Aids:</strong> If you’re in a virtual interview, share your screen and show equations or diagrams. If you’re in person, use the whiteboard.</li>
<li><strong>Pause and Check for Understanding:</strong> After explaining a complex concept or equation, pause and ask, “Does that make sense?” or “Do you have any questions about that?”</li>
<li><strong>Emphasize Trade-offs:</strong> Highlight the pros and cons of each technique. This demonstrates a deep understanding of the material.</li>
<li><strong>Speak Clearly and Confidently:</strong> Maintain a professional tone and project confidence in your knowledge.</li>
<li><strong>Be Ready to Elaborate:</strong> The interviewer might ask follow-up questions about any aspect of your explanation. Be prepared to provide more details or examples.</li>
<li><strong>Relate to Real-World Applications:</strong> If possible, connect the concepts to real-world examples where these techniques are used. This demonstrates practical knowledge.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>