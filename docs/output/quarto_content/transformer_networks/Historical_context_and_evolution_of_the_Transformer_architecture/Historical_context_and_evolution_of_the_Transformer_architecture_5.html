<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-6.-in-your-view-how-has-the-historical-evolution-of-transformer-models-influenced-areas-beyond-nlp-such-as-computer-vision-or-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="question-6.-in-your-view-how-has-the-historical-evolution-of-transformer-models-influenced-areas-beyond-nlp-such-as-computer-vision-or-reinforcement-learning">Question: 6. In your view, how has the historical evolution of Transformer models influenced areas beyond NLP, such as computer vision or reinforcement learning?</h2>
<p><strong>Best Answer</strong></p>
<p>The historical evolution of Transformer models, initially designed for Natural Language Processing (NLP), has profoundly impacted other fields like Computer Vision (CV) and Reinforcement Learning (RL). The core innovation of the Transformer, the self-attention mechanism, has proven to be remarkably versatile and adaptable, leading to significant advancements and new architectural designs in these domains.</p>
<section id="influence-on-computer-vision-cv" class="level3">
<h3 class="anchored" data-anchor-id="influence-on-computer-vision-cv">Influence on Computer Vision (CV)</h3>
<ol type="1">
<li><strong>Vision Transformer (ViT):</strong>
<ul>
<li><p>The most direct influence is the Vision Transformer (ViT). ViT departs from traditional Convolutional Neural Networks (CNNs) by treating images as sequences of patches. Specifically, an input image <span class="math inline">\(x \in \mathbb{R}^{H \times W \times C}\)</span> is divided into <span class="math inline">\(N\)</span> patches of size <span class="math inline">\(P \times P\)</span>, where <span class="math inline">\(N = \frac{HW}{P^2}\)</span>. Each patch is linearly embedded into a <span class="math inline">\(D\)</span>-dimensional vector, and these embeddings are then fed into a standard Transformer encoder.</p></li>
<li><p>Mathematically, let <span class="math inline">\(x_p \in \mathbb{R}^{P \times P \times C}\)</span> be a patch. The linear embedding is given by:</p>
<p><span class="math display">\[z_0 = x_pE + E_{pos}, \quad E \in \mathbb{R}^{(P^2C) \times D}, \quad E_{pos} \in \mathbb{R}^{N \times D}\]</span></p>
<p>where <span class="math inline">\(E\)</span> is the embedding matrix and <span class="math inline">\(E_{pos}\)</span> is the positional encoding. The sequence <span class="math inline">\(z_0\)</span> is then processed by a series of Transformer encoder layers.</p></li>
<li><p>ViT demonstrated that Transformers could achieve state-of-the-art performance in image classification tasks, challenging the dominance of CNNs. Its success stems from the ability of self-attention to capture long-range dependencies between image regions, something CNNs struggle with due to their local receptive fields.</p></li>
</ul></li>
<li><strong>DETR (DEtection TRansformer):</strong>
<ul>
<li><p>DETR leverages Transformers for object detection by formulating object detection as a set prediction problem. It eliminates the need for hand-designed components like anchor boxes and Non-Maximum Suppression (NMS).</p></li>
<li><p>DETR uses a CNN backbone to extract feature maps from the input image. These feature maps are then fed into a Transformer encoder-decoder architecture. The decoder outputs a fixed-size set of object detections, which are then matched to ground-truth objects using a bipartite matching loss.</p></li>
<li><p>The bipartite matching loss is crucial for DETR’s success. Given a set of predicted bounding boxes <span class="math inline">\(\hat{y} = \{\hat{b}_i\}_{i=1}^N\)</span> and a set of ground-truth bounding boxes <span class="math inline">\(y = \{b_i\}_{i=1}^N\)</span>, the optimal assignment <span class="math inline">\(\sigma \in \mathfrak{S}_N\)</span> (where <span class="math inline">\(\mathfrak{S}_N\)</span> is the permutation group) is found by minimizing the cost:</p>
<p><span class="math display">\[ \hat{\sigma} = \underset{\sigma \in \mathfrak{S}_N}{\text{argmin}} \sum_{i=1}^N \mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)}) \]</span></p>
<p>where <span class="math inline">\(\mathcal{L}_{match}\)</span> is a matching cost function.</p></li>
<li><p>DETR’s end-to-end training and its ability to directly predict a set of objects have made it a significant advancement in object detection.</p></li>
</ul></li>
<li><strong>Swin Transformer:</strong>
<ul>
<li>Swin Transformer introduces a hierarchical Transformer architecture with shifted windows. This allows for efficient computation of self-attention on larger images and enables multi-scale feature representations.</li>
<li>The shifted window approach allows connections between different windows in deeper layers, addressing a limitation of the original ViT.</li>
</ul></li>
<li><strong>Underlying Mechanisms &amp; Adaptations:</strong>
<ul>
<li><p>The self-attention mechanism, central to Transformers, allows each element in a sequence (e.g., image patch) to attend to all other elements, capturing global context. This is particularly useful in vision tasks where understanding relationships between distant parts of an image is crucial.</p></li>
<li><p><strong>Challenges:</strong> Adapting Transformers to CV requires addressing differences in data characteristics. Images have inherent 2D structure, while text is inherently 1D. ViT addresses this by dividing the image into patches, but other approaches include using convolutional layers to pre-process images before feeding them into a Transformer.</p></li>
<li><p><strong>Modifications:</strong> Positional embeddings are crucial in Transformers to encode the order of the input sequence. In vision, positional embeddings can be learned or fixed (e.g., sinusoidal). 2D positional embeddings are also used to capture the spatial relationships between image patches.</p></li>
</ul></li>
</ol>
</section>
<section id="influence-on-reinforcement-learning-rl" class="level3">
<h3 class="anchored" data-anchor-id="influence-on-reinforcement-learning-rl">Influence on Reinforcement Learning (RL)</h3>
<ol type="1">
<li><p><strong>Decision Transformer:</strong></p>
<ul>
<li><p>Decision Transformer formulates RL as a sequence modeling problem. It represents trajectories of states, actions, and rewards as sequences and uses a Transformer to predict future actions based on past experiences.</p></li>
<li><p>The input sequence consists of state embeddings <span class="math inline">\(s_t\)</span>, action embeddings <span class="math inline">\(a_t\)</span>, and reward-to-go embeddings <span class="math inline">\(\hat{R}_t\)</span>, where reward-to-go is the sum of future rewards: <span class="math inline">\(\hat{R}_t = \sum_{t'=t}^T r_{t'}\)</span>. The Transformer is trained to predict the action <span class="math inline">\(a_{t+1}\)</span> given the sequence <span class="math inline">\((s_1, a_1, \hat{R}_1, s_2, a_2, \hat{R}_2, ..., s_t, a_t, \hat{R}_t)\)</span>.</p></li>
<li><p>Decision Transformer allows for offline RL, where the agent learns from a fixed dataset of experiences without interacting with the environment.</p></li>
</ul></li>
<li><p><strong>Trajectory Transformer:</strong></p>
<ul>
<li><p>Trajectory Transformer also treats RL as a sequence modeling problem but focuses on generating entire trajectories of states and actions.</p></li>
<li><p>It uses a Transformer to model the joint distribution of states and actions, allowing it to generate diverse and plausible trajectories.</p></li>
</ul></li>
<li><p><strong>Benefits in RL:</strong></p>
<ul>
<li><p>Transformers in RL enable learning long-term dependencies and planning over extended horizons. The self-attention mechanism allows the agent to consider the entire history of the episode when making decisions.</p></li>
<li><p>Transformers can also handle variable-length sequences, which is useful in RL environments where the episode length can vary.</p></li>
</ul></li>
<li><p><strong>Adaptations &amp; Considerations:</strong></p>
<ul>
<li><p><strong>Reward Conditioning:</strong> A key adaptation in RL is reward conditioning, where the Transformer is conditioned on the desired reward or return. This allows the agent to learn policies that achieve specific goals.</p></li>
<li><p><strong>Offline RL:</strong> Transformers are particularly well-suited for offline RL because they can learn from large datasets of pre-collected experiences without requiring online interaction with the environment.</p></li>
</ul></li>
</ol>
</section>
<section id="general-considerations-and-challenges" class="level3">
<h3 class="anchored" data-anchor-id="general-considerations-and-challenges">General Considerations and Challenges</h3>
<ol type="1">
<li><p><strong>Computational Cost:</strong> Transformers have a quadratic computational complexity with respect to the sequence length, which can be a limiting factor when dealing with long sequences or high-resolution images. Techniques like sparse attention, linear attention, and hierarchical Transformers have been developed to address this issue.</p></li>
<li><p><strong>Data Requirements:</strong> Transformers typically require large amounts of data to train effectively. This can be a challenge in domains where data is scarce. Techniques like transfer learning and data augmentation can help mitigate this issue.</p></li>
<li><p><strong>Interpretability:</strong> Interpreting the decisions made by Transformers can be challenging. Attention maps can provide some insight into which parts of the input sequence the model is attending to, but further research is needed to develop more interpretable Transformer models.</p></li>
<li><p><strong>Multi-Modal Learning:</strong> The success of Transformers has spurred research into multi-modal learning, where Transformers are used to process and integrate information from multiple modalities, such as vision, language, and audio.</p></li>
</ol>
<p>In summary, the Transformer architecture, driven by its self-attention mechanism, has had a revolutionary impact beyond NLP, especially in Computer Vision and Reinforcement Learning. While challenges remain, ongoing research continues to refine and adapt Transformers for these new domains, paving the way for even more significant advances.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested way to present this information in an interview:</p>
<ol type="1">
<li><strong>Start with the Core Idea:</strong>
<ul>
<li>“The Transformer architecture, originally designed for NLP, has profoundly impacted other fields like computer vision and reinforcement learning because of its core innovation: the self-attention mechanism.”</li>
</ul></li>
<li><strong>Discuss the Impact on Computer Vision:</strong>
<ul>
<li>“In computer vision, the most direct influence is the Vision Transformer, or ViT. ViT treats images as sequences of patches, similar to how text is treated in NLP. This allows it to capture long-range dependencies that CNNs often struggle with.”</li>
<li><strong>(Optional: Briefly describe the patch embedding process and mention the key formula):</strong> “Specifically, an image is divided into patches, linearly embedded, and positional encodings are added. This can be represented as <span class="math inline">\(z_0 = x_pE + E_{pos}\)</span>.” (Don’t dive too deep unless the interviewer asks).</li>
<li>“Beyond ViT, DETR uses Transformers for object detection by formulating it as a set prediction problem. It gets rid of things like anchor boxes and NMS.”</li>
<li>“More recent architectures, like Swin Transformer, improve efficiency by using shifted windows, allowing for better performance on larger images.”</li>
</ul></li>
<li><strong>Transition to Reinforcement Learning:</strong>
<ul>
<li>“Transformers have also made inroads into Reinforcement Learning. The Decision Transformer, for example, frames RL as a sequence modeling problem.”</li>
<li>“Instead of learning a policy directly, it learns to predict actions based on past states, actions, and rewards. Think of it as learning from a history of episodes.”</li>
<li><strong>(Optional: Mention the reward-to-go concept):</strong> “A key concept here is ‘reward-to-go’, where the Transformer is conditioned on the sum of future rewards. This helps it learn policies that achieve specific goals.”</li>
<li>“Another example is the Trajectory Transformer, which focuses on generating entire trajectories of states and actions.”</li>
</ul></li>
<li><strong>Address Challenges and Considerations:</strong>
<ul>
<li>“While Transformers have shown great promise, there are challenges. Their computational cost is quadratic with respect to sequence length, which can be a problem for long sequences or high-resolution images. Techniques like sparse attention are being developed to address this.”</li>
<li>“Also, Transformers typically require large amounts of data, which can be a limitation in some domains.”</li>
<li>“Interpretability is another area of ongoing research. Attention maps can give some insight, but we need better ways to understand why Transformers make the decisions they do.”</li>
</ul></li>
<li><strong>Conclude with a Forward-Looking Statement:</strong>
<ul>
<li>“In summary, the Transformer’s self-attention mechanism has had a revolutionary impact beyond NLP. While challenges remain, ongoing research is adapting Transformers to these new domains, promising even more significant advancements in the future.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush. Speak clearly and deliberately.</li>
<li><strong>Gauge the Interviewer:</strong> Pay attention to the interviewer’s reactions. If they seem particularly interested in a specific area, elaborate further. If they seem less engaged, keep it brief and move on.</li>
<li><strong>Simplify Mathematical Content:</strong> When discussing equations, focus on the high-level concept rather than getting bogged down in the details. For example, instead of reading out the equation verbatim, say something like, “This equation shows how the image patches are linearly embedded and positional encodings are added.”</li>
<li><strong>Use Visual Aids (If Possible):</strong> If you’re interviewing remotely, consider sharing your screen and showing diagrams or visualizations to illustrate key concepts. If in person, draw a simple diagram on the whiteboard.</li>
<li><strong>Be Ready for Follow-Up Questions:</strong> The interviewer will likely ask follow-up questions to probe your understanding. Be prepared to discuss the advantages and disadvantages of Transformers compared to other approaches, the trade-offs involved in different design choices, and the latest research in the field.</li>
<li><strong>Enthusiasm is Key</strong>: Show that you are excited about this topic.</li>
</ul>
<p>By following these guidelines, you can effectively communicate your senior-level knowledge of Transformers and their impact beyond NLP in a way that is both informative and engaging.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>