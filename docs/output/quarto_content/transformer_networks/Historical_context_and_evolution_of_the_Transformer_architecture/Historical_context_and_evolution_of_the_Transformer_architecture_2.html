<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-3.-what-role-do-positional-encodings-play-in-the-transformer-architecture-and-how-have-they-evolved-in-modern-implementations" class="level2">
<h2 class="anchored" data-anchor-id="question-3.-what-role-do-positional-encodings-play-in-the-transformer-architecture-and-how-have-they-evolved-in-modern-implementations">Question: 3. What role do positional encodings play in the Transformer architecture, and how have they evolved in modern implementations?</h2>
<p><strong>Best Answer</strong></p>
<p>Positional encodings are a crucial component of the Transformer architecture, addressing a fundamental limitation of self-attention mechanisms. Unlike recurrent neural networks (RNNs) or convolutional neural networks (CNNs), self-attention, by design, is permutation-equivariant (or invariant depending on the specific implementation). This means that if you change the order of the input sequence, the output will change in the same way or not change at all. It does not inherently understand the <em>position</em> or <em>order</em> of tokens within a sequence, which is essential for many sequence processing tasks like natural language understanding and generation. Positional encodings inject information about the position of each token in the input sequence, allowing the Transformer to leverage the order of the data.</p>
<p><strong>Why Positional Encodings are Necessary</strong></p>
<p>The self-attention mechanism computes a weighted sum of all input tokens to represent each token. The weights are determined by the “attention” scores, which measure the relatedness of each pair of tokens. While attention scores capture relationships between tokens, they are independent of their absolute or relative positions in the sequence.</p>
<p>Consider a sentence “The cat sat on the mat”. Without positional encodings, the transformer would process “cat the on mat sat the” the same way.</p>
<p>Mathematically, the self-attention mechanism can be described as follows:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where: * <span class="math inline">\(Q\)</span> is the query matrix. * <span class="math inline">\(K\)</span> is the key matrix. * <span class="math inline">\(V\)</span> is the value matrix. * <span class="math inline">\(d_k\)</span> is the dimension of the keys.</p>
<p>As you can see, this operation is agnostic to the order of the inputs. The positional encodings rectify this.</p>
<p><strong>Original Transformer Positional Encodings (Fixed)</strong></p>
<p>The original Transformer paper (Vaswani et al., 2017) introduced fixed positional encodings based on sine and cosine functions of different frequencies:</p>
<p><span class="math display">\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p><span class="math display">\[
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(pos\)</span> is the position of the token in the sequence.</li>
<li><span class="math inline">\(i\)</span> is the dimension index within the positional encoding vector.</li>
<li><span class="math inline">\(d_{\text{model}}\)</span> is the dimensionality of the model’s embeddings.</li>
</ul>
<p>The intuition behind this approach is to create a unique “fingerprint” for each position. The use of sine and cosine functions allows the model to easily learn relative positions. Specifically, for any fixed offset <em>k</em>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear transformation of <span class="math inline">\(PE_{pos}\)</span>. This facilitates the model’s ability to attend to tokens at a consistent relative distance.</p>
<p><strong>Learnable Positional Encodings</strong></p>
<p>An alternative to fixed positional encodings is to learn them. In this approach, positional embeddings are randomly initialized and then updated during training, just like word embeddings.</p>
<p>Learnable positional encodings offer a potential advantage: the model can directly learn the optimal positional representations for the specific task. However, they also have some drawbacks:</p>
<ul>
<li><strong>Limited Extrapolation:</strong> Learnable positional encodings are typically defined for a maximum sequence length. If the model encounters sequences longer than this during inference, it may struggle to generalize.</li>
<li><strong>Increased Parameters:</strong> Learnable embeddings add to the model’s parameter count, which may be a concern when dealing with limited data.</li>
</ul>
<p><strong>Evolution and Modern Implementations</strong></p>
<p>Several variations and improvements to positional encodings have emerged since the original Transformer:</p>
<ul>
<li><p><strong>Relative Positional Encodings:</strong> Instead of encoding the absolute position of each token, relative positional encodings encode the distance between pairs of tokens. This approach has been shown to be more effective in some tasks, particularly those involving long sequences. For example, in the Transformer-XL architecture (Dai et al., 2019), relative positional encodings are used to enable the model to process sequences much longer than those seen during training. The attention score is modified to include the relative position:</p>
<p><span class="math display">\[
\text{Attention}_{i,j} = q_i^T k_j + q_i^T a_{i-j}
\]</span></p>
<p>where <span class="math inline">\(a_{i-j}\)</span> is the embedding for the relative position between tokens <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p></li>
<li><p><strong>Rotary Positional Embeddings (RoPE):</strong> RoPE, used in models like RoFormer (Su et al., 2021), incorporates positional information through rotation matrices. It encodes absolute position information via a rotation matrix and naturally incorporates explicit relative position dependency into self-attention.</p></li>
<li><p><strong>Complex-Valued Positional Encodings:</strong> This approach extends the original sinusoidal encodings to the complex domain. It has been found to improve performance in certain tasks.</p></li>
<li><p><strong>Alibi Positional Encoding:</strong> Instead of adding positional embeddings to the token embeddings, ALiBi (Attention with Linear Biases) directly biases the attention scores with a linear function of the distance between tokens. This method has been shown to be effective for extrapolation to longer sequences.</p></li>
</ul>
<p><strong>Impact on Models like BERT and GPT</strong></p>
<ul>
<li><p><strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> BERT uses learnable positional embeddings. This choice was likely driven by the masked language modeling objective, where learning positional information directly might be advantageous.</p></li>
<li><p><strong>GPT (Generative Pre-trained Transformer):</strong> The original GPT also used learnable positional embeddings. Later versions, such as GPT-3, have explored variations on this theme, but learnable embeddings remain a common choice.</p></li>
</ul>
<p><strong>Real-World Considerations</strong></p>
<ul>
<li><strong>Sequence Length:</strong> The choice of positional encoding scheme should consider the expected sequence length. Fixed positional encodings can be pre-computed and efficiently used for any sequence length, while learnable positional encodings are limited by the maximum sequence length seen during training (unless techniques like relative positional encoding or RoPE are used).</li>
<li><strong>Computational Cost:</strong> Different positional encoding schemes have varying computational costs. Relative positional encodings, for instance, can increase the memory footprint due to the need to store relative position embeddings.</li>
<li><strong>Task-Specific Performance:</strong> The optimal positional encoding scheme is often task-dependent. Experimentation is crucial to determine which scheme works best for a given application.</li>
</ul>
<p>In summary, positional encodings are essential for imbuing the Transformer architecture with an understanding of sequential order. While the original Transformer employed fixed sinusoidal encodings, modern implementations have explored learnable embeddings, relative positional encodings, and other innovative approaches to improve performance and generalization. The choice of positional encoding scheme depends on factors like sequence length, computational cost, and task-specific requirements.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this to an interviewer:</p>
<ol type="1">
<li><strong>Start with the “Why”:</strong> Begin by explaining <em>why</em> positional encodings are necessary. Emphasize that the self-attention mechanism itself is order-agnostic, and therefore, the Transformer needs a way to understand the position of tokens in a sequence.
<ul>
<li><em>“The core of the Transformer, the self-attention mechanism, doesn’t inherently understand the order of words in a sequence. This is a problem because word order is crucial for meaning. Positional encodings are the solution—they add information about the position of each word.”</em></li>
</ul></li>
<li><strong>Briefly Explain Self-Attention:</strong> Give a one-sentence overview of how self-attention works.
<ul>
<li><em>“Self-attention computes relationships between words in a sequence to understand context, but it does this without considering their position.”</em></li>
</ul></li>
<li><strong>Introduce the Original Solution:</strong> Describe the original, fixed positional encodings using sine and cosine functions.
<ul>
<li><em>“The original Transformer paper introduced a clever solution: fixed positional encodings. They used sine and cosine functions of different frequencies to create a unique pattern for each position in the sequence.”</em></li>
</ul></li>
<li><strong>Show, Don’t Just Tell (Optional):</strong> If the interviewer seems receptive to technical details, you can briefly show the equations. However, <em>don’t get bogged down in the math</em>. Explain the intuition behind the equations.
<ul>
<li><em>“The encoding is based on these equations: [Show equations]. The key idea is that each position gets a unique ‘fingerprint’ based on sine and cosine waves. This allows the model to easily learn the relationship between words at different positions.”</em></li>
</ul></li>
<li><strong>Introduce Learnable Positional Encodings:</strong> Explain the alternative of learnable positional encodings.
<ul>
<li><em>“An alternative approach is to use learnable positional encodings, where the model learns the best representation for each position during training. This can be more flexible, but might not generalize to longer sequences.”</em></li>
</ul></li>
<li><strong>Discuss Modern Implementations and Evolution:</strong> Move on to discuss more recent developments, like relative positional encodings and RoPE.
<ul>
<li><em>“Since the original Transformer, there have been many advancements in positional encodings. For example, relative positional encodings encode the distance between words rather than their absolute position, which can be more effective for long sequences.”</em></li>
<li><em>“Another interesting approach is RoPE, or Rotary Position Embeddings. These use rotation matrices to encode positional information in a way that naturally incorporates relative position dependency into self-attention.”</em></li>
</ul></li>
<li><strong>Relate to BERT and GPT:</strong> Briefly mention how positional encodings are used in popular models like BERT and GPT.
<ul>
<li><em>“Models like BERT use learnable positional embeddings, while others have experimented with variations of these techniques. The choice often depends on the specific task and dataset.”</em></li>
</ul></li>
<li><strong>Touch on Real-World Considerations:</strong> Mention practical factors to consider when choosing a positional encoding scheme.
<ul>
<li><em>“When choosing a positional encoding scheme, it’s important to consider factors like the expected sequence length, the computational cost, and the specific task you’re trying to solve. Experimentation is key.”</em></li>
</ul></li>
<li><strong>Communication Tips:</strong>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Give the interviewer time to process the information.</li>
<li><strong>Use simple language:</strong> Avoid jargon when possible. Explain technical terms clearly.</li>
<li><strong>Check for understanding:</strong> Pause periodically and ask if the interviewer has any questions.</li>
<li><strong>Focus on the “big picture”:</strong> While it’s good to show technical depth, don’t get lost in the details. Always relate your answer back to the overall goals and principles.</li>
<li><strong>Be enthusiastic:</strong> Show your passion for the subject matter.</li>
</ul></li>
<li><strong>Walking through Equations:</strong> If you do include equations, do not just read them out loud. Explain what each term represents and the intuition behind the equation. Always relate the math back to the concepts. Focus on the “story” the equation tells.</li>
</ol>
<p>By following these steps, you can deliver a comprehensive and engaging answer that showcases your expertise in positional encodings and their role in the Transformer architecture. Remember to tailor your response to the interviewer’s level of understanding and interests.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>