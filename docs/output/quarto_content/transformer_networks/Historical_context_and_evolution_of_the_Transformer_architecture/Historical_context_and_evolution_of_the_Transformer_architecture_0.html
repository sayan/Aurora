<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_0</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-1.-explain-the-key-innovations-introduced-in-the-original-attention-is-all-you-need-paper.-how-did-these-innovations-depart-from-previous-sequence-models-that-relied-on-rnns-or-cnns" class="level2">
<h2 class="anchored" data-anchor-id="question-1.-explain-the-key-innovations-introduced-in-the-original-attention-is-all-you-need-paper.-how-did-these-innovations-depart-from-previous-sequence-models-that-relied-on-rnns-or-cnns">Question: 1. Explain the key innovations introduced in the original ‘Attention Is All You Need’ paper. How did these innovations depart from previous sequence models that relied on RNNs or CNNs?</h2>
<p><strong>Best Answer</strong></p>
<p>The “Attention Is All You Need” paper (Vaswani et al., 2017) revolutionized sequence modeling by introducing the Transformer architecture. It departed significantly from previous sequence models that were predominantly based on recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The core innovations and their impact are detailed below:</p>
<p><strong>1. Self-Attention Mechanism:</strong></p>
<ul>
<li><p><strong>Innovation:</strong> The Transformer replaced recurrence with self-attention. Self-attention allows the model to relate different positions of a single sequence to compute a representation of the same sequence. This is in stark contrast to RNNs, which process sequences sequentially, and CNNs, which have a limited receptive field.</p></li>
<li><p><strong>Mathematical Formulation:</strong> The self-attention mechanism computes attention weights based on three learned matrices: Query (Q), Key (K), and Value (V). <span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span> where <span class="math inline">\(d_k\)</span> is the dimension of the key vectors. The division by <span class="math inline">\(\sqrt{d_k}\)</span> is a scaling factor to prevent the softmax from becoming too peaked, which can hinder learning.</p></li>
<li><p><strong>Impact:</strong> Self-attention enables parallel computation, unlike RNNs, and captures long-range dependencies more effectively than both RNNs and CNNs. Each position in the sequence can directly attend to any other position, regardless of distance.</p></li>
</ul>
<p><strong>2. Multi-Head Attention:</strong></p>
<ul>
<li><p><strong>Innovation:</strong> Instead of using a single attention mechanism, the Transformer employs multiple “heads” that perform self-attention independently and in parallel.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ol type="1">
<li><p>Project the queries, keys, and values <span class="math inline">\(h\)</span> times with different, learned linear projections to <span class="math inline">\(d_k\)</span>, <span class="math inline">\(d_k\)</span> and <span class="math inline">\(d_v\)</span> dimensions, respectively. <span class="math display">\[
Q_i = QW_i^Q, K_i = KW_i^K, V_i = VW_i^V
\]</span></p></li>
<li><p>Apply attention to each of projected version of queries, keys, and values in parallel <span class="math display">\[
\text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_iK_i^T}{\sqrt{d_k}}\right)V_i
\]</span></p></li>
<li><p>Concatenate and project <span class="math display">\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O \\
\text{where head}_i = \text{Attention}(Q_i, K_i, V_i)
\]</span> where <span class="math inline">\(W_i^Q \in \mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W_i^K \in \mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W_i^V \in \mathbb{R}^{d_{model} \times d_v}\)</span> and <span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_{model}}\)</span></p></li>
</ol></li>
<li><p><strong>Impact:</strong> This allows the model to capture different aspects of relationships within the data. Each attention head can learn a different representation of the input sequence, providing a richer understanding of the relationships between words or tokens. Multi-head attention significantly boosts the model’s ability to capture diverse dependencies.</p></li>
</ul>
<p><strong>3. Positional Encoding:</strong></p>
<ul>
<li><p><strong>Innovation:</strong> Since self-attention is permutation-invariant (i.e., it doesn’t inherently capture the order of the sequence), the Transformer uses positional encodings to incorporate information about the position of tokens in the sequence.</p></li>
<li><p><strong>Mathematical Formulation:</strong> The paper uses sine and cosine functions of different frequencies: <span class="math display">\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) \\
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span> where <span class="math inline">\(pos\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension. This encoding is added to the input embeddings. Other positional encoding schemes (learned or fixed) can also be used.</p></li>
<li><p><strong>Impact:</strong> This allows the model to understand the order of the words or tokens, which is crucial for sequence modeling tasks. Positional encodings provide a way to inject information about the absolute or relative position of the tokens in the sequence.</p></li>
</ul>
<p><strong>4. Encoder-Decoder Structure:</strong></p>
<ul>
<li><p><strong>Innovation:</strong> The Transformer follows the encoder-decoder structure, similar to many sequence-to-sequence models. The encoder processes the input sequence, and the decoder generates the output sequence.</p></li>
<li><p><strong>Impact:</strong> This structure is effective for tasks like machine translation, where the input and output sequences may have different lengths and structures. The encoder creates a representation of the input sequence, and the decoder uses this representation to generate the output sequence, attending to relevant parts of the encoded input using attention mechanisms.</p></li>
</ul>
<p><strong>5. Feed-Forward Networks:</strong></p>
<ul>
<li><p><strong>Innovation:</strong> Each encoder and decoder layer contains a feed-forward network, which is applied to each position separately and identically. This network typically consists of two linear transformations with a ReLU activation in between.</p></li>
<li><p><strong>Mathematical Formulation:</strong> <span class="math display">\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span> where <span class="math inline">\(W_1\)</span>, <span class="math inline">\(W_2\)</span>, <span class="math inline">\(b_1\)</span>, and <span class="math inline">\(b_2\)</span> are learned parameters.</p></li>
<li><p><strong>Impact:</strong> This provides additional non-linearity and feature transformation capabilities at each layer. The feed-forward networks introduce complexity and allow the model to learn more intricate patterns in the data.</p></li>
</ul>
<p><strong>6. Residual Connections and Layer Normalization:</strong></p>
<ul>
<li><p><strong>Innovation:</strong> The Transformer uses residual connections around each sub-layer (self-attention, feed-forward networks), followed by layer normalization.</p></li>
<li><p><strong>Mathematical Formulation:</strong> <span class="math display">\[
\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sigma} + \beta
\]</span> where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the mean and standard deviation of the layer’s inputs, and <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learnable scale and shift parameters.</p></li>
<li><p><strong>Impact:</strong> Residual connections help to mitigate the vanishing gradient problem, allowing for the training of deeper networks. Layer normalization stabilizes the training process and improves convergence.</p></li>
</ul>
<p><strong>Departures from RNNs and CNNs:</strong></p>
<ul>
<li><p><strong>RNNs:</strong> RNNs process sequences sequentially, making parallelization difficult. They also struggle with long-range dependencies due to the vanishing gradient problem. The Transformer addresses these issues with self-attention and parallel computation.</p></li>
<li><p><strong>CNNs:</strong> CNNs, while parallelizable, have a limited receptive field. To capture long-range dependencies, multiple convolutional layers or dilated convolutions are required, which can be computationally expensive. Self-attention allows the Transformer to capture long-range dependencies directly.</p></li>
</ul>
<p>In summary, the Transformer’s key innovations—self-attention, multi-head attention, positional encodings, and a fully attention-based architecture—have enabled it to outperform RNNs and CNNs on a variety of sequence modeling tasks, while also offering significant advantages in terms of parallelization and capturing long-range dependencies.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to present this answer in an interview:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“The ‘Attention Is All You Need’ paper introduced the Transformer architecture, which marked a significant shift from traditional RNNs and CNNs for sequence modeling.”</li>
<li>“The key innovations revolve around the self-attention mechanism and the elimination of recurrence.”</li>
</ul></li>
<li><strong>Explain Self-Attention:</strong>
<ul>
<li>“The core idea is self-attention, which allows the model to relate different parts of the input sequence to each other in order to understand the context.”</li>
<li>“Unlike RNNs, which process data sequentially, self-attention allows for parallel computation, significantly speeding up training.”</li>
<li>Present the attention formula: “Mathematically, self-attention can be represented as follows: Attention(Q, K, V) = softmax(QK<sup>T</sup>/√(d<sub>k</sub>))V where Q, K, and V are the query, key, and value matrices.” Briefly explain the components and the scaling factor.</li>
</ul></li>
<li><strong>Discuss Multi-Head Attention:</strong>
<ul>
<li>“To capture diverse relationships in the data, the Transformer uses multi-head attention. This involves running multiple self-attention mechanisms in parallel, each learning a different representation.”</li>
<li>“Each head operates on different projections of the query, key, and value matrices, allowing the model to capture different aspects of the input sequence.”</li>
</ul></li>
<li><strong>Explain Positional Encoding:</strong>
<ul>
<li>“Since self-attention is permutation-invariant, positional encodings are added to the input embeddings to provide information about the position of tokens.”</li>
<li>“The paper uses sine and cosine functions of different frequencies to create these encodings.” Show the positional encoding equations.</li>
</ul></li>
<li><strong>Mention Encoder-Decoder Structure &amp; Other Components:</strong>
<ul>
<li>“The Transformer uses an encoder-decoder structure. The encoder processes the input sequence, and the decoder generates the output sequence, attending to the encoded input.”</li>
<li>“Each layer includes Feed-Forward Networks after the attention layer to provide non-linearity and allow the model to learn more intricate patterns”</li>
<li>“It also employs residual connections and layer normalization, which helps in training deeper networks and stabilizes the learning process.”</li>
</ul></li>
<li><strong>Contrast with RNNs and CNNs:</strong>
<ul>
<li>“RNNs struggle with parallelization and long-range dependencies due to their sequential nature and the vanishing gradient problem.”</li>
<li>“CNNs, while parallelizable, have a limited receptive field, requiring multiple layers to capture long-range dependencies. The Transformer addresses these issues with self-attention.”</li>
<li>“Self-attention allows the Transformer to capture long-range dependencies directly and enables parallel computation, making it more efficient than RNNs and CNNs.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Take your time to ensure clarity.</li>
<li><strong>Use Visual Aids (if available):</strong> If you have access to a whiteboard or a screen, consider drawing a simplified diagram of the Transformer architecture to illustrate the key components.</li>
<li><strong>Check for Understanding:</strong> Pause occasionally and ask if the interviewer has any questions.</li>
<li><strong>Focus on the “Why”:</strong> Emphasize the motivations behind each innovation. Explain why each component was introduced and how it contributes to the overall performance of the model.</li>
<li><strong>Relate to Practical Applications:</strong> If possible, mention how these innovations have impacted real-world applications, such as machine translation, natural language understanding, and computer vision.</li>
<li><strong>Math Accessibility:</strong> When presenting the equations, explain each term and the purpose of the equation in simple terms. Avoid getting bogged down in excessive mathematical detail unless prompted.</li>
</ul>
<p>By following these guidelines, you can effectively communicate your understanding of the Transformer architecture and its innovations in a clear, concise, and engaging manner.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>