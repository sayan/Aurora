<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-9.-from-a-historical-perspective-what-were-some-of-the-initial-criticisms-or-limitations-of-the-transformer-model-and-how-have-subsequent-developments-addressed-these-concerns" class="level2">
<h2 class="anchored" data-anchor-id="question-9.-from-a-historical-perspective-what-were-some-of-the-initial-criticisms-or-limitations-of-the-transformer-model-and-how-have-subsequent-developments-addressed-these-concerns">Question: 9. From a historical perspective, what were some of the initial criticisms or limitations of the Transformer model, and how have subsequent developments addressed these concerns?</h2>
<p><strong>Best Answer</strong></p>
<p>The Transformer model, introduced in the seminal paper “Attention is All You Need” (Vaswani et al., 2017), revolutionized sequence modeling and has become the cornerstone of modern NLP. However, its initial form was not without limitations and criticisms. Over the years, subsequent research has actively addressed these concerns, leading to significant advancements.</p>
<p>Here’s a breakdown of the initial challenges and how they have been mitigated:</p>
<ol type="1">
<li><p><strong>Quadratic Complexity:</strong></p>
<ul>
<li><p><strong>Criticism:</strong> The original Transformer’s self-attention mechanism has a time and memory complexity of <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length. This quadratic scaling quickly becomes a bottleneck when dealing with long sequences, making it computationally expensive and memory-intensive. The attention mechanism involves calculating attention weights for each pair of tokens in the sequence. <span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span> Where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> are the query, key, and value matrices, respectively, and <span class="math inline">\(d_k\)</span> is the dimension of the keys. Computing <span class="math inline">\(QK^T\)</span> is the <span class="math inline">\(O(n^2)\)</span> operation.</p></li>
<li><p><strong>Mitigation:</strong> Several efficient attention mechanisms have been developed to reduce the complexity. These methods approximate the full attention matrix or use sparse attention patterns:</p>
<ul>
<li><strong>Sparse Attention:</strong> Techniques like Longformer (Beltagy et al., 2020) and Big Bird (Zaheer et al., 2020) introduce sparse attention patterns, reducing the complexity to <span class="math inline">\(O(n)\)</span>. Longformer uses a combination of sliding window, dilated window, and global attention. Big Bird uses a combination of random, windowed, and global attention.</li>
<li><strong>Linear Attention:</strong> Methods like Linear Transformers (Katharopoulos et al., 2020) and Performer (Choromanski et al., 2021) reduce complexity to <span class="math inline">\(O(n)\)</span> by using kernel methods to approximate the attention mechanism.</li>
<li><strong>Low-Rank Attention:</strong> This approach reduces the dimensionality of the attention matrix by projecting the query and key matrices into a lower-dimensional space.</li>
</ul></li>
</ul></li>
<li><p><strong>Training Instability:</strong></p>
<ul>
<li><p><strong>Criticism:</strong> Training Transformers, particularly very deep ones, can be unstable. This manifests as vanishing or exploding gradients, making it difficult to achieve convergence. The multiplicative nature of the attention mechanism and the depth of the network can exacerbate these issues.</p></li>
<li><p><strong>Mitigation:</strong></p>
<ul>
<li><strong>Layer Normalization (LayerNorm):</strong> Applying LayerNorm helps stabilize training by normalizing the activations within each layer. LayerNorm computes the mean and variance across the features for each sample, effectively reducing internal covariate shift.</li>
<li><strong>Residual Connections:</strong> Residual connections (He et al., 2016) allow gradients to flow more easily through the network, mitigating the vanishing gradient problem. The output of a layer is added to its input, creating a shortcut connection.</li>
<li><strong>Careful Initialization:</strong> Proper weight initialization (e.g., Xavier/Glorot initialization or Kaiming/He initialization) is crucial for stable training. These methods initialize the weights based on the number of input and output units, preventing gradients from becoming too large or too small.</li>
<li><strong>Learning Rate Warmup:</strong> Gradually increasing the learning rate during the initial training steps (warmup) helps stabilize training. This prevents the network from making large, disruptive updates early on.</li>
<li><strong>Gradient Clipping:</strong> Clipping the gradients to a certain threshold prevents them from becoming too large, avoiding exploding gradients.</li>
</ul></li>
</ul></li>
<li><p><strong>Lack of Interpretability:</strong></p>
<ul>
<li><p><strong>Criticism:</strong> Transformers, like many deep learning models, were initially seen as “black boxes.” Understanding <em>why</em> a Transformer makes a particular prediction was challenging. The complex interactions within the attention mechanism made it difficult to discern which parts of the input sequence were most influential.</p></li>
<li><p><strong>Mitigation:</strong></p>
<ul>
<li><strong>Attention Visualization:</strong> Visualizing the attention weights can provide insights into which words or tokens the model is attending to. However, attention weights are not always a reliable indicator of importance.</li>
<li><strong>Attention Rollout:</strong> Attention rollout methods propagate the attention weights through the network to determine the overall influence of each token.</li>
<li><strong>Layer-wise Relevance Propagation (LRP):</strong> LRP and similar techniques propagate the prediction backwards through the network to assign relevance scores to each input feature.</li>
<li><strong>Probing:</strong> Training auxiliary classifiers to predict specific properties of the input or output from the hidden states of the Transformer. This can reveal what information the model has learned and where it is stored.</li>
</ul></li>
</ul></li>
<li><p><strong>Positional Encoding Limitations:</strong></p>
<ul>
<li><p><strong>Criticism:</strong> The original Transformer uses fixed positional encodings (sine and cosine functions) to provide information about the position of tokens in the sequence. While effective, these fixed encodings lack the flexibility to generalize to sequences longer than those seen during training. <span class="math display">\[PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})\]</span> <span class="math display">\[PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\]</span> Where <span class="math inline">\(pos\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension.</p></li>
<li><p><strong>Mitigation:</strong></p>
<ul>
<li><strong>Learned Positional Embeddings:</strong> Replacing fixed positional encodings with learned embeddings allows the model to learn the positional relationships directly from the data.</li>
<li><strong>Relative Positional Embeddings:</strong> Representing the position of each token relative to other tokens in the sequence. This can improve generalization and allow the model to handle longer sequences.</li>
</ul></li>
</ul></li>
<li><p><strong>Difficulty with Discrete Data (prior to Tokenizers):</strong></p>
<ul>
<li><p><strong>Criticism:</strong> Initially, Transformers and attention mechanisms more generally were designed primarily for continuous data. Adapting them to discrete data (like words, categories) required careful embedding strategies.</p></li>
<li><p><strong>Mitigation:</strong></p>
<ul>
<li><strong>Subword Tokenization:</strong> Byte-Pair Encoding (BPE) and WordPiece tokenization techniques broke down words into subword units. This approach allowed the model to handle out-of-vocabulary words and improved generalization across different languages.</li>
<li><strong>Learned Embeddings:</strong> The use of learnable word embeddings (e.g., Word2Vec, GloVe) provided a dense, continuous representation of words, making them suitable for use with Transformers.</li>
</ul></li>
</ul></li>
<li><p><strong>Optimization Challenges:</strong></p>
<ul>
<li><p><strong>Criticism:</strong> Training very large Transformer models required significant computational resources and careful hyperparameter tuning. Finding the optimal learning rate, batch size, and other hyperparameters could be a time-consuming and expensive process.</p></li>
<li><p><strong>Mitigation:</strong></p>
<ul>
<li><strong>Adaptive Optimization Algorithms:</strong> Algorithms like Adam and its variants (e.g., AdamW) have become the standard for training Transformers. Adam adapts the learning rate for each parameter based on its historical gradients.</li>
<li><strong>Distributed Training:</strong> Using multiple GPUs or TPUs to train the model in parallel can significantly reduce training time. Data parallelism and model parallelism are common strategies for distributed training.</li>
<li><strong>Mixed Precision Training:</strong> Using a combination of single-precision (FP32) and half-precision (FP16) floating-point numbers can reduce memory usage and improve training speed.</li>
</ul></li>
</ul></li>
</ol>
<p>These are some of the major initial criticisms and how the research community has addressed them. The continuous evolution of the Transformer architecture demonstrates its adaptability and robustness, making it a powerful tool for a wide range of tasks.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to present this information in an interview setting:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong></p>
<ul>
<li>“The Transformer model was a breakthrough, but it had initial limitations that researchers have actively addressed over time.”</li>
<li>“I can discuss some key criticisms and the innovations that have mitigated them.”</li>
</ul></li>
<li><p><strong>Address Quadratic Complexity:</strong></p>
<ul>
<li>“One major initial concern was the quadratic complexity of the self-attention mechanism, scaling as <span class="math inline">\(O(n^2)\)</span> with sequence length, making it impractical for long sequences.”</li>
<li>“To explain, the core attention calculation involves <span class="math inline">\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span>, where the <span class="math inline">\(QK^T\)</span> multiplication leads to the quadratic term.” (If the interviewer seems interested in more detail, you can briefly explain Q, K, and V).</li>
<li>“Numerous efficient attention variants have been developed, such as Sparse Attention (Longformer, Big Bird) and Linear Attention (Linear Transformers, Performer), which reduce complexity to <span class="math inline">\(O(n)\)</span>.”</li>
</ul></li>
<li><p><strong>Explain Training Instability:</strong></p>
<ul>
<li>“Training instability was another hurdle, with vanishing/exploding gradients being common in deep Transformers.”</li>
<li>“Techniques like Layer Normalization, Residual Connections, careful weight initialization, learning rate warm-up, and gradient clipping have proven crucial in stabilizing training.”</li>
<li>“For instance, Layer Normalization normalizes activations within each layer, reducing internal covariate shift. Residual connections allow gradients to flow more easily, mitigating the vanishing gradient problem.”</li>
</ul></li>
<li><p><strong>Discuss Lack of Interpretability:</strong></p>
<ul>
<li>“Initially, Transformers were considered ‘black boxes’ with limited interpretability.”</li>
<li>“Methods like Attention Visualization, Attention Rollout, and Layer-wise Relevance Propagation (LRP) have improved our understanding of what the model attends to and why.”</li>
<li>“Attention visualization helps see which parts of the input the model focuses on, while LRP traces the prediction backward to assign relevance scores.”</li>
</ul></li>
<li><p><strong>Touch on Positional Encoding:</strong></p>
<ul>
<li>“The original fixed positional encodings had limitations in generalizing to longer sequences.”</li>
<li>“Learned positional embeddings and relative positional embeddings provide more flexibility and improved generalization.”</li>
</ul></li>
<li><p><strong>Mention Discrete Data Handling &amp; Optimization:</strong></p>
<ul>
<li>“Early challenges included adapting the model to discrete data, which was addressed by subword tokenization and learned embeddings.”</li>
<li>“Optimization was also a challenge, mitigated by adaptive algorithms like AdamW, distributed training, and mixed precision training.”</li>
</ul></li>
<li><p><strong>Summarize and Offer Additional Detail:</strong></p>
<ul>
<li>“In summary, the Transformer has evolved significantly to address initial limitations. Each advancement has contributed to its robustness and wide applicability.”</li>
<li>“Depending on the interviewer’s interests, I can elaborate on specific techniques or the math behind them.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pause:</strong> Allow time for the interviewer to absorb information, especially after introducing equations or complex concepts.</li>
<li><strong>Gauge Interest:</strong> Pay attention to their body language and questions. If they seem particularly interested in a specific area, delve deeper. If they look confused or disengaged, simplify your explanation or move on to another topic.</li>
<li><strong>Avoid Jargon Overload:</strong> Use technical terms appropriately but avoid overwhelming the interviewer with jargon. Define terms as needed.</li>
<li><strong>Relate to Real-World Applications:</strong> If possible, connect the techniques you discuss to real-world applications to demonstrate their practical value.</li>
<li><strong>Show Enthusiasm:</strong> Let your passion for the topic shine through. This will make your answer more engaging and memorable.</li>
<li><strong>Be Ready to Simplify:</strong> If the interviewer seems less technical, be prepared to simplify your explanations without sacrificing accuracy.</li>
</ul>
<p>By following these steps, you can deliver a comprehensive and engaging answer that demonstrates your senior-level expertise in Transformer models.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>