<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-7.-considering-the-deployment-of-transformer-based-models-what-are-the-scalability-and-hardware-challenges-and-how-can-they-be-addressed-in-practical-production-level-scenarios" class="level2">
<h2 class="anchored" data-anchor-id="question-7.-considering-the-deployment-of-transformer-based-models-what-are-the-scalability-and-hardware-challenges-and-how-can-they-be-addressed-in-practical-production-level-scenarios">Question: 7. Considering the deployment of Transformer-based models, what are the scalability and hardware challenges, and how can they be addressed in practical, production-level scenarios?</h2>
<p><strong>Best Answer</strong></p>
<p>Deploying Transformer-based models in production presents significant scalability and hardware challenges primarily stemming from their inherent architectural characteristics. These challenges manifest in several key areas: model size, computational complexity (particularly during inference), memory requirements, and the need for specialized hardware acceleration.</p>
<section id="model-size-and-parameter-count" class="level3">
<h3 class="anchored" data-anchor-id="model-size-and-parameter-count">1. Model Size and Parameter Count:</h3>
<p>Transformers, especially large language models (LLMs), can have billions or even trillions of parameters. This leads to enormous storage requirements and difficulty in fitting models into memory, particularly on edge devices or in resource-constrained environments.</p>
<ul>
<li><p><strong>Problem:</strong> Memory limitations and slow loading times.</p></li>
<li><p><strong>Solutions:</strong></p>
<ul>
<li><strong>Model Compression Techniques:</strong> These techniques reduce model size while preserving accuracy.
<ul>
<li><p><strong>Quantization:</strong> Reduces the precision of weights and activations (e.g., from FP32 to FP16 or INT8). This reduces memory footprint and improves inference speed on hardware that supports lower precision arithmetic. There are several methods:</p>
<ul>
<li><p><strong>Post-Training Quantization (PTQ):</strong> Quantizes the model after training, which is relatively easy to implement but may lead to accuracy degradation. Mathematically, if <span class="math inline">\(w\)</span> represents a weight, and <span class="math inline">\(Q(w)\)</span> its quantized version, we have:</p>
<p><span class="math display">\[Q(w) = scale * round(w / scale)\]</span></p>
<p>Where <code>scale</code> is a quantization factor. The crucial aspect lies in choosing the optimal <code>scale</code> to minimize information loss during quantization.</p></li>
<li><p><strong>Quantization-Aware Training (QAT):</strong> Simulates quantization during training to make the model more robust to quantization effects. This generally yields better accuracy than PTQ but requires retraining the model. During the forward pass, the quantization operation <span class="math inline">\(Q(w)\)</span> is applied. The backward pass may use a Straight-Through Estimator (STE) which approximates the derivative of the rounding function as 1.</p>
<p><span class="math display">\[\frac{\partial Q(w)}{\partial w} \approx 1\]</span></p></li>
</ul></li>
<li><p><strong>Pruning:</strong> Removes unimportant weights or connections from the network, reducing the model size and computational cost.</p>
<ul>
<li><strong>Unstructured Pruning:</strong> Removes individual weights. This can be irregular and challenging to accelerate on standard hardware.</li>
<li><strong>Structured Pruning:</strong> Removes entire neurons or channels, which is more hardware-friendly.</li>
</ul>
<p>Pruning involves defining a <em>sparsity</em> level, <span class="math inline">\(s\)</span>, representing the fraction of weights to be removed. A common approach involves thresholding weights based on their magnitude. The remaining weights are then fine-tuned.</p></li>
<li><p><strong>Knowledge Distillation:</strong> Trains a smaller “student” model to mimic the behavior of a larger, pre-trained “teacher” model. The student model learns to replicate the teacher’s outputs, including the soft probabilities produced by the teacher’s softmax layer. The loss function for knowledge distillation often includes a combination of the student’s classification loss and a distillation loss that measures the difference between the student’s and teacher’s outputs.</p>
<p><span class="math display">\[Loss = \alpha L_{CE}(y, p_s) + (1 - \alpha) L_{KL}(p_t, p_s)\]</span></p>
<p>Where <span class="math inline">\(L_{CE}\)</span> is the cross-entropy loss, <span class="math inline">\(L_{KL}\)</span> is the Kullback-Leibler divergence, <span class="math inline">\(y\)</span> is the ground truth, <span class="math inline">\(p_s\)</span> is the student’s prediction, <span class="math inline">\(p_t\)</span> is the teacher’s prediction, and <span class="math inline">\(\alpha\)</span> is a weighting factor.</p></li>
</ul></li>
<li><strong>Low-Rank Factorization:</strong> Decomposes weight matrices into lower-rank matrices, reducing the number of parameters. For example, a weight matrix <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span> can be approximated by two smaller matrices <span class="math inline">\(U \in \mathbb{R}^{m \times k}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{k \times n}\)</span>, where <span class="math inline">\(k &lt; min(m, n)\)</span>. Thus <span class="math inline">\(W \approx UV\)</span>. The choice of <span class="math inline">\(k\)</span> determines the trade-off between compression and accuracy.</li>
</ul></li>
</ul>
</section>
<section id="computational-complexity-inference" class="level3">
<h3 class="anchored" data-anchor-id="computational-complexity-inference">2. Computational Complexity (Inference):</h3>
<p>The attention mechanism in Transformers has a quadratic complexity with respect to the input sequence length, <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length. This makes inference very computationally expensive, especially for long sequences.</p>
<ul>
<li><p><strong>Problem:</strong> Slow inference speed and high latency.</p></li>
<li><p><strong>Solutions:</strong></p>
<ul>
<li><p><strong>Efficient Attention Mechanisms:</strong></p>
<ul>
<li><strong>Sparse Attention:</strong> Reduces the number of attention operations by attending to only a subset of the input sequence. Examples include:
<ul>
<li><strong>Longformer:</strong> Uses a combination of global attention, sliding window attention, and dilated sliding window attention.</li>
<li><strong>BigBird:</strong> Uses random attention, global attention, and window attention.</li>
</ul></li>
<li><strong>Linear Attention:</strong> Approximates the attention mechanism with linear complexity, <span class="math inline">\(O(n)\)</span>. Examples include:
<ul>
<li><strong>Linformer:</strong> Projects the key and value matrices to a lower-dimensional space.</li>
<li><strong>Performer:</strong> Uses Fast Attention via Positive Orthogonal Random Features (FAVOR+) to approximate the attention mechanism.</li>
</ul></li>
</ul></li>
<li><p><strong>Kernel Fusion:</strong> Combines multiple operations into a single kernel to reduce memory access and improve computational efficiency.</p></li>
<li><p><strong>Speculative Decoding:</strong> Uses a smaller, faster model (the “draft model”) to generate candidate tokens, which are then verified by the larger, more accurate model. This can significantly speed up inference, especially when the draft model is accurate.</p></li>
</ul></li>
</ul>
</section>
<section id="memory-requirements" class="level3">
<h3 class="anchored" data-anchor-id="memory-requirements">3. Memory Requirements:</h3>
<p>Transformers require significant memory to store weights, activations, and intermediate results during both training and inference. This can be a bottleneck, especially when dealing with very large models or long sequences.</p>
<ul>
<li><p><strong>Problem:</strong> Out-of-memory errors and slow training/inference.</p></li>
<li><p><strong>Solutions:</strong></p>
<ul>
<li><strong>Gradient Checkpointing:</strong> Reduces memory usage during training by recomputing activations during the backward pass instead of storing them. This trades off computation for memory. Mathematically, in the standard backpropagation, we store the activations <span class="math inline">\(a_i = f_i(x_{i-1})\)</span> for each layer <span class="math inline">\(i\)</span>. Gradient checkpointing involves only storing a subset of these activations and recomputing the rest during backpropagation.</li>
<li><strong>Mixed Precision Training:</strong> Uses a combination of FP32 and FP16 precision to reduce memory usage and improve training speed.</li>
<li><strong>Offloading to CPU/Disk:</strong> Temporarily moves less critical data (e.g., activations) to CPU memory or disk to free up GPU memory.</li>
</ul></li>
</ul>
</section>
<section id="hardware-acceleration-and-distributed-computing" class="level3">
<h3 class="anchored" data-anchor-id="hardware-acceleration-and-distributed-computing">4. Hardware Acceleration and Distributed Computing:</h3>
<p>Transformers benefit greatly from specialized hardware accelerators and distributed computing.</p>
<ul>
<li><p><strong>Problem:</strong> Inefficient utilization of hardware resources and limitations in scaling training and inference.</p></li>
<li><p><strong>Solutions:</strong></p>
<ul>
<li><strong>GPUs (Graphics Processing Units):</strong> GPUs are well-suited for the parallel computations required by Transformers.</li>
<li><strong>TPUs (Tensor Processing Units):</strong> TPUs are custom-designed ASICs (Application-Specific Integrated Circuits) optimized for deep learning workloads.</li>
<li><strong>Distributed Training:</strong> Splits the training workload across multiple GPUs or TPUs.
<ul>
<li><strong>Data Parallelism:</strong> Replicates the model on each device and splits the data across devices. Each device computes gradients on its portion of the data, and the gradients are then aggregated.</li>
<li><strong>Model Parallelism:</strong> Splits the model across devices. This is necessary when the model is too large to fit on a single device. Requires careful consideration to minimize communication overhead between devices.</li>
<li><strong>Pipeline Parallelism:</strong> Divides the model into stages and processes different mini-batches in parallel, similar to an assembly line. Requires careful load balancing to maximize throughput.</li>
</ul></li>
<li><strong>Optimized Libraries and Frameworks:</strong> Use optimized libraries and frameworks (e.g., PyTorch, TensorFlow, JAX) that provide efficient implementations of Transformer operations and support for hardware acceleration. Specifically, libraries like <code>torch.compile</code> in PyTorch 2.0 can significantly optimize transformer inference.</li>
</ul></li>
</ul>
</section>
<section id="serving-strategies" class="level3">
<h3 class="anchored" data-anchor-id="serving-strategies">5. Serving Strategies</h3>
<p>Efficient serving strategies are crucial for deploying Transformer models in production.</p>
<ul>
<li><p><strong>Problem:</strong> High latency and low throughput.</p></li>
<li><p><strong>Solutions:</strong></p>
<ul>
<li><strong>Batching:</strong> Processes multiple requests in a single batch to improve throughput.</li>
<li><strong>Caching:</strong> Caches the results of previous requests to reduce latency. Effective for scenarios where similar requests are common.</li>
<li><strong>Asynchronous Inference:</strong> Handles requests asynchronously to prevent blocking the main thread.</li>
<li><strong>Model Servers:</strong> Use dedicated model serving frameworks (e.g., TensorFlow Serving, TorchServe, Triton Inference Server) that provide features such as model management, versioning, and scaling.</li>
</ul></li>
</ul>
</section>
<section id="real-world-considerations" class="level3">
<h3 class="anchored" data-anchor-id="real-world-considerations">Real-World Considerations:</h3>
<ul>
<li><strong>Trade-offs:</strong> Many of the solutions described above involve trade-offs between accuracy, speed, and memory usage. The optimal choice depends on the specific application and hardware constraints.</li>
<li><strong>Hardware-Aware Optimization:</strong> It’s crucial to optimize models for the specific hardware on which they will be deployed. This may involve choosing appropriate data types, using optimized libraries, and tuning hyperparameters.</li>
<li><strong>Monitoring and Profiling:</strong> Continuously monitor the performance of deployed models and profile their resource usage to identify bottlenecks and areas for optimization.</li>
<li><strong>Dynamic Batching</strong>: Adapt batch sizes to changing traffic patterns to optimize for both throughput and latency.</li>
</ul>
<p>By carefully considering these challenges and implementing appropriate solutions, it is possible to deploy Transformer-based models effectively in practical, production-level scenarios.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested approach to narrating this in an interview:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“Deploying Transformers, especially large ones, in production introduces significant hurdles. These mainly revolve around model size, computational cost, memory constraints, and the need for specialized hardware.”</li>
</ul></li>
<li><strong>Discuss Model Size and Parameter Count:</strong>
<ul>
<li>“A primary challenge is the sheer size of these models. Billions or trillions of parameters lead to memory bottlenecks and slow load times. We can address this through model compression.”</li>
<li>“The key techniques here are quantization, which reduces the precision of the weights. For example, Post-Training Quantization involves quantizing the model <em>after</em> training and can be expressed mathematically as… [briefly explain the <span class="math inline">\(Q(w)\)</span> equation without getting bogged down]. Quantization-Aware Training is more involved, but often yields better results.”</li>
<li>“Another approach is pruning, where we remove less important connections. We can do this in an unstructured way, removing individual weights, or a structured way by removing entire neurons or channels, which is more hardware-friendly.”</li>
<li>“Finally, Knowledge Distillation allows us to train a smaller, faster model that mimics the behavior of a larger model, which is useful where lower computational footprint is needed.”</li>
</ul></li>
<li><strong>Address Computational Complexity (Inference):</strong>
<ul>
<li>“The attention mechanism’s quadratic complexity is a major bottleneck during inference, especially for long sequences. We need to find ways to make attention more efficient.”</li>
<li>“Sparse attention mechanisms like Longformer and BigBird reduce the number of attention calculations. Linear attention mechanisms, such as Linformer and Performer, offer even more dramatic speedups by approximating attention with linear complexity.”</li>
<li>“Kernel fusion is another optimization – we combine multiple operations to reduce memory access and improve performance. Speculative decoding also offers speedups at the cost of a more complex implementation.”</li>
</ul></li>
<li><strong>Explain Memory Requirements:</strong>
<ul>
<li>“Transformers need a lot of memory for weights and activations, leading to out-of-memory errors, especially with long inputs. Gradient Checkpointing, mixed-precision training, and temporarily offloading data to the CPU can help alleviate these issues.”</li>
<li>“Gradient Checkpointing trades computation for memory. Instead of storing every activation, we recompute them during backpropagation. This means we use less memory, but the backward pass takes longer.”</li>
</ul></li>
<li><strong>Discuss Hardware Acceleration and Distributed Computing:</strong>
<ul>
<li>“To fully leverage these models, we need specialized hardware and distributed computing strategies. GPUs and TPUs provide the necessary parallel processing power. We can distribute the training workload using data parallelism, model parallelism, or pipeline parallelism, each with its own trade-offs.”</li>
</ul></li>
<li><strong>Highlight Serving Strategies:</strong>
<ul>
<li>“Efficient serving is also paramount. Batching multiple requests together, caching results, and handling inference asynchronously can significantly improve performance. Using model servers like TensorFlow Serving and TorchServe is the recommended approach.”</li>
</ul></li>
<li><strong>Conclude with Real-World Considerations:</strong>
<ul>
<li>“Ultimately, deploying Transformers involves trade-offs. We need to balance accuracy, speed, and memory usage based on the application and available resources. Continuous monitoring and profiling are crucial to identify and address bottlenecks. Hardware-aware optimization, which is optimizing models to the particular target hardware, is also a critical component.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use clear and concise language:</strong> Avoid jargon unless necessary.</li>
<li><strong>Provide examples:</strong> Use real-world examples to illustrate your points.</li>
<li><strong>Check for understanding:</strong> Pause periodically and ask if the interviewer has any questions.</li>
<li><strong>Be prepared to go deeper:</strong> The interviewer may ask you to elaborate on specific topics.</li>
<li><strong>If you mention an equation, briefly explain each term and its significance.</strong> Avoid reciting the equation without context.</li>
<li><strong>Adapt to the interviewer’s level of understanding:</strong> If the interviewer is less familiar with the topic, provide a more high-level explanation. If they are more familiar, you can go into more detail.</li>
<li><strong>End with a summary and your key takeaways.</strong> This reinforces your understanding and leaves a lasting impression.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>