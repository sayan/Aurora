<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_7</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-8.-how-would-you-approach-adapting-a-transformer-model-to-handle-real-world-messy-text-data-that-may-include-noise-imbalances-or-non-standard-inputs-identify-potential-pitfalls-and-propose-mitigation-strategies." class="level2">
<h2 class="anchored" data-anchor-id="question-8.-how-would-you-approach-adapting-a-transformer-model-to-handle-real-world-messy-text-data-that-may-include-noise-imbalances-or-non-standard-inputs-identify-potential-pitfalls-and-propose-mitigation-strategies.">Question: 8. How would you approach adapting a Transformer model to handle real-world, messy text data that may include noise, imbalances, or non-standard inputs? Identify potential pitfalls and propose mitigation strategies.</h2>
<p><strong>Best Answer</strong></p>
<p>Adapting Transformer models to real-world, messy text data requires a multi-faceted approach, considering data preprocessing, model robustness, and potential biases. Here’s a detailed breakdown:</p>
<p><strong>1. Data Understanding and Profiling:</strong></p>
<p>Before any model adaptation, the initial step is thorough data exploration:</p>
<ul>
<li><strong>Noise Assessment:</strong> Quantify the types and frequencies of noise (typos, grammatical errors, irrelevant characters, special symbols, etc.).</li>
<li><strong>Imbalance Detection:</strong> Identify skewed class distributions in text classification or generation tasks. For example, in sentiment analysis, one sentiment might be over-represented. Measure the imbalance using metrics like class-wise counts or entropy.</li>
<li><strong>Non-Standard Input Analysis:</strong> Characterize variations in language (e.g., slang, abbreviations, code-switching). Determine the prevalence of out-of-vocabulary (OOV) words and unusual sentence structures.</li>
</ul>
<p><strong>2. Data Preprocessing:</strong></p>
<p>Preprocessing is crucial for cleaning and standardizing the data:</p>
<ul>
<li><p><strong>Noise Reduction:</strong></p>
<ul>
<li><p><strong>Typo Correction:</strong> Implement algorithms like edit distance (Levenshtein distance) or probabilistic language models to correct typographical errors. A simple example using edit distance: The Levenshtein distance <span class="math inline">\(L(a, b)\)</span> between strings <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is defined recursively as:</p>
<p><span class="math display">\[
L(a, b) =
\begin{cases}
\max(|a|, |b|) &amp; \text{if } \min(|a|, |b|) = 0, \\
\min \begin{cases}
L(\text{tail}(a), b) + 1 \\
L(a, \text{tail}(b)) + 1 \\
L(\text{tail}(a), \text{tail}(b)) + c
\end{cases} &amp; \text{otherwise},
\end{cases}
\]</span> where <span class="math inline">\(c = 0\)</span> if the first characters of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are equal, and <span class="math inline">\(c = 1\)</span> otherwise. <span class="math inline">\(\text{tail}(s)\)</span> denotes the string <span class="math inline">\(s\)</span> without its first character.</p></li>
<li><p><strong>Special Character Removal:</strong> Filter out irrelevant characters or symbols.</p></li>
<li><p><strong>Grammatical Error Correction:</strong> Employ pre-trained models designed for grammatical error correction.</p></li>
</ul></li>
<li><p><strong>Text Normalization:</strong></p>
<ul>
<li><strong>Lowercasing:</strong> Convert text to lowercase (carefully, as it might remove information in some cases).</li>
<li><strong>Stemming/Lemmatization:</strong> Reduce words to their root form. Stemming is heuristic-based and faster, while lemmatization uses vocabulary and morphological analysis.</li>
<li><strong>Stop Word Removal:</strong> Eliminate common words (e.g., “the,” “a,” “is”) that often don’t contribute much to meaning.</li>
</ul></li>
<li><p><strong>Handling Imbalances:</strong></p>
<ul>
<li><p><strong>Oversampling:</strong> Duplicate samples from minority classes. Techniques like SMOTE (Synthetic Minority Oversampling Technique) generate synthetic samples based on existing ones.</p></li>
<li><p><strong>Undersampling:</strong> Randomly remove samples from majority classes.</p></li>
<li><p><strong>Cost-Sensitive Learning:</strong> Assign higher weights to misclassification errors for minority classes during training. The weighted loss function can be defined as:</p>
<p><span class="math display">\[
Loss = \frac{1}{N} \sum_{i=1}^{N} w_i L(y_i, \hat{y}_i)
\]</span></p>
<p>where <span class="math inline">\(w_i\)</span> is the weight for the <span class="math inline">\(i\)</span>-th sample, <span class="math inline">\(L\)</span> is the standard loss function, <span class="math inline">\(y_i\)</span> is the true label, and <span class="math inline">\(\hat{y}_i\)</span> is the predicted label.</p></li>
</ul></li>
<li><p><strong>Standardization:</strong></p>
<ul>
<li><strong>Consistent Formatting:</strong> Ensure uniformity in date formats, currency representations, and other structured data.</li>
<li><strong>Encoding Conversion:</strong> Handle different character encodings (e.g., UTF-8, ASCII).</li>
</ul></li>
</ul>
<p><strong>3. Robust Tokenization:</strong></p>
<p>Standard tokenizers may struggle with messy data. Consider these approaches:</p>
<ul>
<li><strong>Subword Tokenization:</strong> Use techniques like Byte Pair Encoding (BPE) or WordPiece to break words into smaller units. This helps handle OOV words by representing them as combinations of known subwords.</li>
<li><strong>Character-Level Tokenization:</strong> Tokenize at the character level, completely bypassing OOV issues, although at the cost of longer sequences and potentially less semantic information per token.</li>
<li><strong>Custom Tokenization:</strong> Train a tokenizer on the specific messy dataset to learn its unique characteristics.</li>
</ul>
<p><strong>4. Handling Out-of-Vocabulary (OOV) Words:</strong></p>
<ul>
<li><strong>Replacement with &lt;UNK&gt; Token:</strong> Replace OOV words with a special &lt;UNK&gt; token, which the model learns to handle.</li>
<li><strong>Character-Level Embeddings:</strong> Use character-level embeddings in addition to word embeddings to represent OOV words based on their character composition.</li>
<li><strong>Hybrid Approaches:</strong> Combine subword tokenization with character-level embeddings.</li>
</ul>
<p><strong>5. Data Augmentation:</strong></p>
<p>Augment the training data to improve the model’s robustness:</p>
<ul>
<li><strong>Back Translation:</strong> Translate text to another language and then back to the original language, introducing variations while preserving meaning.</li>
<li><strong>Random Insertion/Deletion/Swapping:</strong> Introduce small, random modifications to the text.</li>
<li><strong>Synonym Replacement:</strong> Replace words with their synonyms using a thesaurus or pre-trained word embeddings.</li>
</ul>
<p><strong>6. Model Fine-Tuning and Domain Adaptation:</strong></p>
<ul>
<li><strong>Pre-training on Related Data:</strong> If possible, pre-train the Transformer model on a large dataset of related text data before fine-tuning on the messy data.</li>
<li><strong>Fine-Tuning with a Low Learning Rate:</strong> Fine-tune the pre-trained model on the messy data with a low learning rate to avoid overfitting and preserve the knowledge learned during pre-training.</li>
<li><strong>Adversarial Training:</strong> Introduce adversarial examples during training to make the model more robust to noise.</li>
<li><strong>Layer Freezing:</strong> Freeze the initial layers of the Transformer and only fine-tune the later layers. This allows the model to retain general language knowledge while adapting to the specific characteristics of the messy data.</li>
</ul>
<p><strong>7. Bias and Fairness Considerations:</strong></p>
<ul>
<li><strong>Bias Detection:</strong> Analyze the data and model outputs for potential biases related to gender, race, religion, or other sensitive attributes.</li>
<li><strong>Bias Mitigation:</strong>
<ul>
<li><strong>Data Re-weighting:</strong> Adjust the weights of samples during training to reduce the impact of biased data.</li>
<li><strong>Adversarial Debias:</strong> Train the model to be invariant to sensitive attributes.</li>
<li><strong>Regularization Techniques:</strong> Use regularization techniques to prevent the model from relying on biased features.</li>
</ul></li>
</ul>
<p><strong>8. Evaluation Metrics:</strong></p>
<p>Choose evaluation metrics that are robust to noise and imbalances:</p>
<ul>
<li><strong>F1-score:</strong> Harmonic mean of precision and recall, useful for imbalanced datasets.</li>
<li><strong>AUC-ROC:</strong> Area Under the Receiver Operating Characteristic curve, less sensitive to class imbalances.</li>
<li><strong>BLEU score:</strong> (for translation/generation) can be noisy, consider variations like chrF++.</li>
<li><strong>Human Evaluation:</strong> Essential for assessing the quality of generated text and identifying potential biases.</li>
</ul>
<p><strong>9. Potential Pitfalls and Mitigation Strategies:</strong></p>
<ul>
<li><strong>Overfitting to Noise:</strong>
<ul>
<li><strong>Pitfall:</strong> The model learns the noise patterns in the training data, leading to poor generalization.</li>
<li><strong>Mitigation:</strong> Use regularization techniques, data augmentation, and early stopping.</li>
</ul></li>
<li><strong>Loss of Semantic Information:</strong>
<ul>
<li><strong>Pitfall:</strong> Aggressive noise reduction or text normalization removes important semantic information.</li>
<li><strong>Mitigation:</strong> Carefully balance noise reduction with information preservation. Evaluate the impact of preprocessing on downstream task performance.</li>
</ul></li>
<li><strong>Bias Amplification:</strong>
<ul>
<li><strong>Pitfall:</strong> Pre-existing biases in the data are amplified by the model.</li>
<li><strong>Mitigation:</strong> Implement bias detection and mitigation techniques. Carefully analyze model outputs for fairness.</li>
</ul></li>
<li><strong>Computational Cost:</strong>
<ul>
<li><strong>Pitfall:</strong> Complex preprocessing and augmentation techniques increase computational cost.</li>
<li><strong>Mitigation:</strong> Optimize preprocessing pipelines and use efficient data loading techniques.</li>
</ul></li>
</ul>
<p><strong>10. Monitoring and Iteration:</strong></p>
<p>Continuously monitor the model’s performance and adapt the approach as needed. Regularly re-evaluate the data, preprocessing techniques, and model parameters.</p>
<p><strong>Best Answer (Additional Notes)</strong> This answer is long, so keep in mind you likely would not cover all of it in an interview. The key is to demonstrate your depth of understanding, and your awareness of the breadth of considerations. Feel free to cut sections if there is not time.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview (30 seconds):</strong></p>
<ul>
<li>“Adapting Transformers to real-world messy data is a challenging but crucial task. It requires a comprehensive approach that addresses data quality, model robustness, and potential biases. I would approach this by focusing on data understanding, preprocessing, robust tokenization, handling OOV words, data augmentation, model fine-tuning, and bias mitigation.”</li>
</ul></li>
<li><p><strong>Data Understanding and Preprocessing (2-3 minutes):</strong></p>
<ul>
<li>“The first step is to deeply understand the data. I’d profile the data to assess the different kinds of noise and imbalances, and quantify them.”</li>
<li>“Then, I’d focus on preprocessing techniques. This involves noise reduction through typo correction using algorithms like Levenshtein distance &lt;pause to gauge interviewer’s interest - you could show the Levenshtein distance equation if they ask for more detail, or just say ‘algorithms like Levenshtein distance’ &gt;, special character removal, and grammatical error correction. Then I would apply text normalization like lowercasing, stemming or lemmatization, and stop word removal. Addressing class imbalances is key, which can be done through oversampling (like SMOTE), undersampling, or cost-sensitive learning (possibly show the weighted loss equation here if they ask for specifics).”</li>
<li>“The goal here is to clean and standardize the data, making it more suitable for the Transformer model, but it is very important to not introduce new biases into the data in this phase”</li>
</ul></li>
<li><p><strong>Tokenization and OOV Handling (1-2 minutes):</strong></p>
<ul>
<li>“Standard tokenizers often fail on messy data, so I’d use subword tokenization techniques like BPE or WordPiece to handle out-of-vocabulary words. Character-level tokenization is another more radical option. I’d consider using a custom tokenizer trained on the specific noisy dataset.”</li>
<li>“For OOV words, in addition to subword tokenization, replacing them with an <code>&lt;UNK&gt;</code> token or using character-level embeddings can be beneficial. It really depends on the data.”</li>
</ul></li>
<li><p><strong>Data Augmentation (1 minute):</strong></p>
<ul>
<li>“Data augmentation is key. I’d use back translation, random insertion/deletion/swapping of words, and synonym replacement to create more diverse and robust training data.”</li>
</ul></li>
<li><p><strong>Model Fine-Tuning and Domain Adaptation (2-3 minutes):</strong></p>
<ul>
<li>“I’d fine-tune a pre-trained Transformer model on the messy data, using a low learning rate to avoid overfitting. Techniques like adversarial training or layer freezing can further improve robustness.”</li>
<li>“If I had access to a larger related dataset, I would consider pre-training on it before fine-tuning.”</li>
</ul></li>
<li><p><strong>Bias and Fairness (1 minute):</strong></p>
<ul>
<li>“It’s crucial to address potential biases. I’d analyze the data and model outputs for biases and use mitigation techniques like data re-weighting, adversarial debiasing, or regularization.”</li>
</ul></li>
<li><p><strong>Evaluation and Pitfalls (1-2 minutes):</strong></p>
<ul>
<li>“I’d use robust evaluation metrics like F1-score or AUC-ROC, and also include human evaluation. I would explicitly look for overfitting, loss of semantic information, and bias amplification.”</li>
<li>“I’d mitigate overfitting through regularization and data augmentation. I would take care to preserve information during preprocessing. I would use bias mitigation techniques and carefully analyze model outputs.”</li>
</ul></li>
<li><p><strong>Concluding Remarks (30 seconds):</strong></p>
<ul>
<li>“In summary, adapting Transformers to messy data is an iterative process that requires a deep understanding of the data, careful preprocessing, robust tokenization, data augmentation, and bias mitigation. Continuous monitoring and evaluation are crucial for ensuring optimal performance and fairness.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pause and Gauge Interest:</strong> After introducing a complex concept or equation, pause and ask the interviewer if they want more detail. This shows that you are aware of their time and expertise level.</li>
<li><strong>Focus on the “Why”:</strong> Explain <em>why</em> each technique is important, not just <em>what</em> it is.</li>
<li><strong>Use Concrete Examples:</strong> Whenever possible, use concrete examples to illustrate your points.</li>
<li><strong>Be Prepared to Simplify:</strong> Have a simplified explanation ready in case the interviewer is not familiar with a specific technique.</li>
<li><strong>Show Enthusiasm:</strong> Express your enthusiasm for the topic, showing that you are genuinely interested in the challenges of working with messy data.</li>
<li><strong>Don’t Be Afraid to Say “It Depends”:</strong> Acknowledge that the best approach depends on the specific characteristics of the data and the task.</li>
</ul>
<p>By following these steps, you can effectively demonstrate your expertise in adapting Transformer models to real-world, messy text data, while also showcasing your communication skills and your ability to think critically about potential challenges and solutions.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>