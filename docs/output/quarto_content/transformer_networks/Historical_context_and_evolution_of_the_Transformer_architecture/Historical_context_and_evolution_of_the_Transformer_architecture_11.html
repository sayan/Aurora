<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_11</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-12.-considering-the-historical-context-where-do-you-see-the-future-of-transformer-architectures-going-in-research-and-applications-what-are-the-open-challenges-that-researchers-still-need-to-address" class="level2">
<h2 class="anchored" data-anchor-id="question-12.-considering-the-historical-context-where-do-you-see-the-future-of-transformer-architectures-going-in-research-and-applications-what-are-the-open-challenges-that-researchers-still-need-to-address">Question: 12. Considering the historical context, where do you see the future of Transformer architectures going in research and applications? What are the open challenges that researchers still need to address?</h2>
<p><strong>Best Answer</strong></p>
<p>The Transformer architecture, introduced in the seminal paper “Attention is All You Need” (Vaswani et al., 2017), has revolutionized the field of deep learning, particularly in natural language processing (NLP) and, more recently, computer vision and other domains. Understanding its historical context allows us to better predict its future trajectory.</p>
<p><strong>Historical Context &amp; Key Innovations:</strong></p>
<ul>
<li><p><strong>Sequence-to-Sequence Models &amp; RNN Limitations:</strong> Before Transformers, sequence-to-sequence tasks were largely dominated by Recurrent Neural Networks (RNNs) and their variants (LSTMs, GRUs). While effective, RNNs suffered from limitations like vanishing gradients, difficulty in parallelization due to their sequential nature, and challenges in capturing long-range dependencies. The attention mechanism was initially introduced to alleviate some of these limitations within the RNN framework, but Transformers took it to a new level.</p></li>
<li><p><strong>The Attention Mechanism:</strong> The core innovation of Transformers is the self-attention mechanism. This allows the model to weigh the importance of different parts of the input sequence when processing each element. Mathematically, self-attention can be represented as:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(Q\)</span> is the query matrix, <span class="math inline">\(K\)</span> is the key matrix, <span class="math inline">\(V\)</span> is the value matrix, and <span class="math inline">\(d_k\)</span> is the dimension of the keys. The <span class="math inline">\(\sqrt{d_k}\)</span> term is used to scale the dot products, preventing them from becoming too large, which can lead to vanishing gradients after the softmax operation. Multi-head attention further enhances this by allowing the model to learn different relationships between the input elements using multiple sets of <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> matrices.</p></li>
<li><p><strong>Parallelization &amp; Scalability:</strong> Transformers enable parallel processing of the input sequence, overcoming a major bottleneck of RNNs. This, combined with the attention mechanism, made it possible to train significantly larger models on massive datasets, leading to breakthroughs in various NLP tasks. Models like BERT, GPT, and T5 showcased the power of pre-training large Transformers on vast amounts of text data and then fine-tuning them for specific downstream tasks.</p></li>
</ul>
<p><strong>Future Directions &amp; Open Challenges:</strong></p>
<p>Given this context, I foresee the future of Transformers heading in several key directions:</p>
<ol type="1">
<li><p><strong>Efficiency &amp; Scalability:</strong></p>
<ul>
<li><p><strong>Sparse Attention:</strong> The quadratic complexity of self-attention (<span class="math inline">\(O(n^2)\)</span> with respect to sequence length <span class="math inline">\(n\)</span>) remains a major bottleneck for long sequences. Future research will focus on more efficient attention mechanisms, such as sparse attention, which aims to reduce the computational cost by attending only to a subset of the input sequence. Techniques like Longformer, Reformer, and BigBird exemplify this trend. Mathematically, this could involve approximating the attention matrix or using learnable sparsity patterns.</p></li>
<li><p><strong>Quantization &amp; Pruning:</strong> Model compression techniques like quantization (reducing the precision of weights and activations) and pruning (removing less important connections) will become increasingly important for deploying large Transformer models on resource-constrained devices. This could involve techniques like:</p>
<ul>
<li><strong>Quantization:</strong> Converting weights from FP32 to INT8 or lower. For example, a quantized weight <span class="math inline">\(w_q\)</span> can be represented as <span class="math inline">\(w_q = \text{round}(w / s)\)</span>, where <span class="math inline">\(w\)</span> is the original weight and <span class="math inline">\(s\)</span> is a scaling factor.</li>
<li><strong>Pruning:</strong> Setting weights below a certain magnitude threshold to zero. This can be represented as <span class="math inline">\(w' = w \cdot \mathbb{I}(|w| &gt; \tau)\)</span>, where <span class="math inline">\(w'\)</span> is the pruned weight, <span class="math inline">\(\mathbb{I}\)</span> is the indicator function, and <span class="math inline">\(\tau\)</span> is the threshold.</li>
</ul></li>
<li><p><strong>Hardware Acceleration:</strong> Developing specialized hardware architectures optimized for Transformer operations will be crucial. This includes ASICs (Application-Specific Integrated Circuits) designed specifically for matrix multiplication and attention calculations.</p></li>
</ul></li>
<li><p><strong>Data Efficiency &amp; Generalization:</strong></p>
<ul>
<li><p><strong>Few-Shot &amp; Zero-Shot Learning:</strong> While large-scale pre-training has been remarkably successful, Transformers still require a significant amount of data for fine-tuning. Future research will focus on improving data efficiency, enabling models to learn from very few examples (few-shot learning) or even generalize to unseen tasks without any task-specific training (zero-shot learning). Meta-learning techniques and advanced prompting strategies play a significant role here.</p></li>
<li><p><strong>Robustness &amp; Adversarial Training:</strong> Transformers are vulnerable to adversarial attacks (small, carefully crafted perturbations to the input that can cause the model to make incorrect predictions). Enhancing the robustness of Transformers against adversarial examples and other forms of noise is a critical area of research. This often involves adversarial training, where the model is trained on both clean and adversarially perturbed examples. The adversarial loss can be defined as:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{adv}} = \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\delta \in \Delta} \mathcal{L}(f(x + \delta), y) \right]
\]</span></p>
<p>where <span class="math inline">\(x\)</span> is the input, <span class="math inline">\(y\)</span> is the target, <span class="math inline">\(\mathcal{D}\)</span> is the data distribution, <span class="math inline">\(f\)</span> is the model, <span class="math inline">\(\delta\)</span> is the adversarial perturbation, <span class="math inline">\(\Delta\)</span> is the set of allowed perturbations, and <span class="math inline">\(\mathcal{L}\)</span> is the loss function.</p></li>
<li><p><strong>Causal Inference &amp; Reasoning:</strong> Integrating causal reasoning capabilities into Transformers is a major challenge. Current Transformers primarily focus on correlation, not causation. Future research will explore ways to incorporate causal knowledge and reasoning into the model architecture and training process. This might involve using causal graphs or interventions during training.</p></li>
</ul></li>
<li><p><strong>Multi-Modal Learning &amp; Integration:</strong></p>
<ul>
<li><p><strong>Vision-Language Models (VLMs):</strong> Extending Transformers to handle multiple modalities (e.g., text, images, audio, video) is a promising direction. VLMs like CLIP and DALL-E 2 have demonstrated the potential of this approach. Future research will focus on developing more powerful and general-purpose VLMs that can seamlessly integrate information from different modalities.</p></li>
<li><p><strong>Robotics &amp; Embodied AI:</strong> Transformers are increasingly being used in robotics and embodied AI to process sensor data and control robot actions. This requires developing Transformers that can handle continuous inputs and operate in real-time.</p></li>
</ul></li>
<li><p><strong>Beyond Attention:</strong></p>
<ul>
<li><strong>State Space Models:</strong> There’s growing evidence that alternatives to Attention may achieve similar or better results for certain sequence modelling tasks with lower computational complexity. For example, State Space Models have shown promise.</li>
</ul></li>
</ol>
<p><strong>Open Challenges:</strong></p>
<ul>
<li><strong>Interpretability:</strong> Understanding <em>why</em> Transformers make certain predictions remains a challenge. Developing methods for interpreting Transformer behavior is crucial for building trust and ensuring fairness. Techniques like attention visualization and probing are used, but more sophisticated approaches are needed.</li>
<li><strong>Bias &amp; Fairness:</strong> Transformers can inherit biases from the data they are trained on, leading to unfair or discriminatory outcomes. Developing methods for mitigating bias in Transformer models is essential.</li>
<li><strong>Long-Range Dependencies:</strong> While Transformers are better at capturing long-range dependencies than RNNs, they still struggle with very long sequences. Efficiently modeling long-range dependencies remains an open challenge.</li>
</ul>
<p>In summary, the future of Transformers lies in addressing the limitations of the current architecture, improving efficiency, enhancing robustness, and extending its capabilities to handle multiple modalities and complex reasoning tasks. The evolution will likely involve a combination of architectural innovations, training techniques, and hardware advancements.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a concise overview:</strong> “The Transformer architecture has been revolutionary, particularly in NLP, but also increasingly in vision and other domains. Its impact stems from overcoming limitations of previous sequence models like RNNs.”</p></li>
<li><p><strong>Explain the historical context:</strong> “Before Transformers, RNNs were dominant, but they struggled with vanishing gradients, parallelization, and long-range dependencies. The attention mechanism was a crucial stepping stone.”</p></li>
<li><p><strong>Dive into the attention mechanism:</strong> “The key innovation is self-attention, which allows the model to weigh the importance of different parts of the input. Mathematically, it can be represented as … [briefly state the attention formula]. The scaling factor is important to stabilize training.”</p></li>
<li><p><strong>Highlight the benefits of Transformers:</strong> “Transformers enabled parallel processing and facilitated the training of much larger models on massive datasets, leading to breakthroughs like BERT and GPT.”</p></li>
<li><p><strong>Transition to future directions:</strong> “Looking ahead, I see the future of Transformers focused on several key areas: efficiency, data efficiency, multi-modal learning, and exploring alternatives to attention itself.”</p></li>
<li><p><strong>Elaborate on efficiency:</strong> “A major bottleneck is the quadratic complexity of self-attention. Techniques like sparse attention are being developed to reduce this cost. Model compression techniques like quantization and pruning are also important for deployment.” [If the interviewer shows interest, you can briefly explain quantization and pruning.]</p></li>
<li><p><strong>Discuss data efficiency:</strong> “While pre-training is powerful, Transformers still need a lot of data for fine-tuning. Research is focusing on few-shot and zero-shot learning to improve data efficiency.”</p></li>
<li><p><strong>Mention robustness:</strong> “Transformers are vulnerable to adversarial attacks, so enhancing their robustness is crucial. Adversarial training is a common technique.”</p></li>
<li><p><strong>Move to multi-modal learning:</strong> “Extending Transformers to handle multiple modalities like images and audio is a promising direction. Vision-Language Models are a good example of this.”</p></li>
<li><p><strong>Mention Alternatives to Attention:</strong> Briefly mention the use of State Space Models that attempt to do similar with fewer computations.</p></li>
<li><p><strong>Address open challenges:</strong> “Despite their success, there are still open challenges, including interpretability, bias, and handling very long sequences efficiently.”</p></li>
<li><p><strong>Conclude with a summary:</strong> “In summary, the future of Transformers involves addressing current limitations, improving efficiency and robustness, and expanding their capabilities to handle more complex tasks. This will require a combination of architectural innovations, training techniques, and potentially new hardware.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the answer. Give the interviewer time to process the information.</li>
<li><strong>Use clear and concise language:</strong> Avoid jargon where possible.</li>
<li><strong>Check for understanding:</strong> Pause periodically and ask if the interviewer has any questions.</li>
<li><strong>Tailor your response to the interviewer’s level of expertise:</strong> If the interviewer seems unfamiliar with a particular concept, provide a simpler explanation. If they seem very knowledgeable, you can delve into more technical details.</li>
<li><strong>Don’t be afraid to say “I don’t know”:</strong> If you are unsure about something, it is better to be honest than to try to bluff your way through it. You can then say something like, “I don’t know the answer to that specifically, but I would approach the problem by…”</li>
<li><strong>Highlight practical applications:</strong> Whenever possible, connect your answer to real-world applications of Transformers.</li>
<li><strong>Express enthusiasm:</strong> Show that you are passionate about the field of deep learning and excited about the future of Transformers.</li>
</ul>
<p>When discussing the mathematical formula, write it out on a whiteboard (if available) and explain each component. Don’t just recite the formula; explain its purpose and the role of each variable. Say something like, “Q represents the queries, K the keys, and V the values. The softmax function normalizes the attention weights, and the scaling factor helps prevent vanishing gradients.” By providing context, you make the formula more accessible and demonstrate a deeper understanding.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>