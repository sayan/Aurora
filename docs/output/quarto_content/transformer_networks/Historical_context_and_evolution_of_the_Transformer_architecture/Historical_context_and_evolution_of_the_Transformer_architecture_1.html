<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-2.-describe-the-self-attention-mechanism-mathematically.-how-do-the-concepts-of-queries-keys-and-values-interact-and-what-is-the-role-of-scaled-dot-product-attention" class="level2">
<h2 class="anchored" data-anchor-id="question-2.-describe-the-self-attention-mechanism-mathematically.-how-do-the-concepts-of-queries-keys-and-values-interact-and-what-is-the-role-of-scaled-dot-product-attention">Question: 2. Describe the self-attention mechanism mathematically. How do the concepts of queries, keys, and values interact, and what is the role of scaled dot-product attention?</h2>
<p><strong>Best Answer</strong></p>
<p>The self-attention mechanism is a core component of the Transformer architecture, enabling the model to weigh the importance of different parts of the input sequence when processing it. Unlike recurrent neural networks, self-attention can capture long-range dependencies in a sequence with a fixed number of computations. This explanation will cover the mathematical details of self-attention, the roles of queries, keys, and values, and the purpose of scaled dot-product attention.</p>
<p><strong>Mathematical Formulation</strong></p>
<p>Given an input sequence, we first transform each element into three vectors: a query (<span class="math inline">\(Q\)</span>), a key (<span class="math inline">\(K\)</span>), and a value (<span class="math inline">\(V\)</span>). These are obtained by multiplying the input sequence by three different weight matrices, <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, and <span class="math inline">\(W_V\)</span>, respectively.</p>
<p><span class="math display">\[
Q = XW_Q \\
K = XW_K \\
V = XW_V
\]</span></p>
<p>Where <span class="math inline">\(X\)</span> is the input sequence represented as a matrix, and <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, <span class="math inline">\(W_V\)</span> are the learned weight matrices.</p>
<p>The self-attention mechanism computes attention weights by taking the dot product of the query matrix <span class="math inline">\(Q\)</span> with the key matrix <span class="math inline">\(K\)</span>. This dot product measures the similarity between each query and each key. The result is then scaled by the square root of the dimension of the key vectors (<span class="math inline">\(d_k\)</span>) and passed through a softmax function to obtain the attention weights.</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>Here’s a breakdown of each step:</p>
<ol type="1">
<li><p><strong>Dot Product of Queries and Keys:</strong> The dot product <span class="math inline">\(QK^T\)</span> calculates the similarity between each query and each key. This results in a matrix where each element <span class="math inline">\((i, j)\)</span> represents the similarity between the <span class="math inline">\(i\)</span>-th query and the <span class="math inline">\(j\)</span>-th key.</p>
<p><span class="math display">\[
\text{Similarity Matrix} = QK^T
\]</span></p></li>
<li><p><strong>Scaling:</strong> The similarity matrix is scaled down by dividing by <span class="math inline">\(\sqrt{d_k}\)</span>, where <span class="math inline">\(d_k\)</span> is the dimension of the key vectors. This scaling is crucial because the dot products can become large in magnitude, pushing the softmax function into regions where it has extremely small gradients. This can slow down learning.</p>
<p><span class="math display">\[
\text{Scaled Similarity Matrix} = \frac{QK^T}{\sqrt{d_k}}
\]</span></p></li>
<li><p><strong>Softmax:</strong> The scaled similarity matrix is passed through a softmax function. The softmax function converts the similarity scores into probabilities, ensuring that they sum to 1 along each row. This results in the attention weights.</p>
<p><span class="math display">\[
\text{Attention Weights} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)
\]</span></p></li>
<li><p><strong>Weighted Sum of Values:</strong> The attention weights are then used to compute a weighted sum of the value vectors <span class="math inline">\(V\)</span>. Each value vector is multiplied by its corresponding attention weight, and the results are summed to produce the output of the self-attention mechanism.</p>
<p><span class="math display">\[
\text{Output} = \text{Attention Weights} \cdot V = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p></li>
</ol>
<p><strong>Roles of Queries, Keys, and Values</strong></p>
<ul>
<li><p><strong>Queries (<span class="math inline">\(Q\)</span>)</strong>: Queries represent what we are looking for. Each query is compared against all keys to determine which values are most relevant. In the analogy of a database retrieval system, the query is the search term.</p></li>
<li><p><strong>Keys (<span class="math inline">\(K\)</span>)</strong>: Keys represent what is being indexed or referenced. They are compared against the queries to determine the relevance of each value. Continuing the database analogy, keys are the indexed terms in the database.</p></li>
<li><p><strong>Values (<span class="math inline">\(V\)</span>)</strong>: Values contain the actual information that is being retrieved. They are weighted by the attention weights and summed to produce the output. In the database analogy, values are the content associated with each indexed term.</p></li>
</ul>
<p>The interaction between these three components allows the model to attend to different parts of the input sequence and to focus on the most relevant information when making predictions.</p>
<p><strong>Role of Scaled Dot-Product Attention</strong></p>
<p>The scaled dot-product attention mechanism addresses the vanishing gradient problem that can arise when the dot products become too large. Without scaling, the softmax function can saturate, leading to small gradients and slow learning. By scaling the dot products by <span class="math inline">\(\sqrt{d_k}\)</span>, the variance of the dot products is reduced, preventing the softmax function from saturating.</p>
<p>Specifically, if <span class="math inline">\(q_i\)</span> and <span class="math inline">\(k_j\)</span> are the <span class="math inline">\(i\)</span>-th and <span class="math inline">\(j\)</span>-th rows of <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> respectively, and assuming that the components of <span class="math inline">\(q_i\)</span> and <span class="math inline">\(k_j\)</span> are independent random variables with mean 0 and variance 1, then the variance of the dot product <span class="math inline">\(q_i \cdot k_j\)</span> is <span class="math inline">\(d_k\)</span>. Scaling by <span class="math inline">\(\sqrt{d_k}\)</span> normalizes the variance to 1, stabilizing the gradients during training.</p>
<p><span class="math display">\[
\text{Var}(q_i \cdot k_j) = d_k
\]</span></p>
<p><strong>Benefits and Considerations</strong></p>
<ul>
<li><p><strong>Parallel Computation</strong>: Self-attention can be computed in parallel, unlike recurrent neural networks, which process the input sequence sequentially. This makes the Transformer architecture much faster to train and more suitable for large datasets.</p></li>
<li><p><strong>Long-Range Dependencies</strong>: Self-attention can capture long-range dependencies in a sequence with a fixed number of computations, addressing the vanishing gradient problem that can plague recurrent neural networks when dealing with long sequences.</p></li>
<li><p><strong>Quadratic Complexity</strong>: The computational complexity of self-attention is <span class="math inline">\(O(n^2d)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length and <span class="math inline">\(d\)</span> is the dimension of the queries, keys, and values. This quadratic complexity can be a bottleneck for very long sequences. Variations such as sparse attention and linear attention have been developed to address this issue.</p></li>
</ul>
<p>In summary, the self-attention mechanism is a powerful tool for capturing dependencies in sequential data. Its ability to process information in parallel and to attend to different parts of the input sequence makes it a key component of the Transformer architecture. The scaled dot-product attention mechanism ensures that the gradients remain stable during training, while the queries, keys, and values interact to produce a weighted representation of the input sequence.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<ol type="1">
<li><p><strong>Introduction</strong>:</p>
<ul>
<li>Start by defining self-attention as the core component of the Transformer architecture.</li>
<li>Emphasize its role in weighing the importance of different parts of the input.</li>
<li>Mention that it overcomes limitations of RNNs in capturing long-range dependencies.</li>
</ul>
<p><em>Example:</em> “Self-attention is a key mechanism in the Transformer, allowing the model to weigh different parts of the input sequence. Unlike RNNs, it efficiently captures long-range dependencies.”</p></li>
<li><p><strong>Queries, Keys, and Values</strong>:</p>
<ul>
<li>Introduce queries, keys, and values as transformations of the input sequence.</li>
<li>Explain how they are obtained using weight matrices.</li>
</ul>
<p><em>Example:</em> “We start by transforming the input into queries, keys, and values using learned weight matrices, <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, and <span class="math inline">\(W_V\)</span>. This projects the input into different representation spaces.”</p></li>
<li><p><strong>Mathematical Formulation</strong>:</p>
<ul>
<li>Present the attention formula.</li>
<li>Walk through each step, explaining the dot product, scaling, softmax, and weighted sum.</li>
<li>Use LaTeX notation for clarity.</li>
</ul>
<p><em>Example:</em> “The attention mechanism is defined as <span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span>. First, we compute the dot product of queries and keys, <span class="math inline">\(QK^T\)</span>, to measure similarity. We then scale by <span class="math inline">\(\sqrt{d_k}\)</span> to stabilize gradients and apply softmax to get attention weights. Finally, we compute a weighted sum of the values.”</p></li>
<li><p><strong>Role of Scaling</strong>:</p>
<ul>
<li>Explain the purpose of the scaling factor <span class="math inline">\(\sqrt{d_k}\)</span>.</li>
<li>Mention the vanishing gradient problem and how scaling helps to stabilize training.</li>
</ul>
<p><em>Example:</em> “The scaling factor <span class="math inline">\(\sqrt{d_k}\)</span> is crucial because the dot products can become large, causing the softmax function to saturate, which leads to small gradients. Scaling helps prevent this and stabilizes training.”</p></li>
<li><p><strong>Benefits and Considerations</strong>:</p>
<ul>
<li>Discuss the benefits of self-attention, such as parallel computation and capturing long-range dependencies.</li>
<li>Acknowledge the quadratic complexity and mention techniques to mitigate it.</li>
</ul>
<p><em>Example:</em> “Self-attention allows for parallel computation, which speeds up training significantly. It also effectively captures long-range dependencies. However, it has a quadratic complexity, <span class="math inline">\(O(n^2d)\)</span>, which can be a bottleneck for long sequences. Techniques like sparse attention address this issue.”</p></li>
<li><p><strong>Conclusion</strong>:</p>
<ul>
<li>Summarize the key points.</li>
<li>Reiterate the importance of self-attention in modern deep learning architectures.</li>
</ul>
<p><em>Example:</em> “In summary, self-attention is a powerful mechanism for capturing dependencies in sequential data. Its ability to process information in parallel and its effectiveness in capturing long-range dependencies make it a key component of the Transformer architecture.”</p></li>
</ol>
<p><strong>Communication Tips</strong></p>
<ul>
<li><strong>Pace</strong>: Speak clearly and at a moderate pace. Avoid rushing through mathematical details.</li>
<li><strong>Emphasis</strong>: Highlight key points such as the role of scaling and the benefits of parallel computation.</li>
<li><strong>Interaction</strong>: Encourage questions from the interviewer to ensure understanding.</li>
<li><strong>Visual Aids</strong>: If possible, use diagrams or visualizations to illustrate the self-attention mechanism. You can sketch these on a whiteboard, if available.</li>
<li><strong>Confidence</strong>: Demonstrate confidence in your understanding of the topic.</li>
<li><strong>Real-World Examples</strong>: If relevant, provide real-world examples of how self-attention is used in applications such as machine translation or natural language understanding.</li>
<li><strong>Mathematical Sections</strong>: When presenting mathematical sections, briefly explain the purpose and intuition behind each step before diving into the formulas. This helps the interviewer follow along and understand the underlying concepts.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>