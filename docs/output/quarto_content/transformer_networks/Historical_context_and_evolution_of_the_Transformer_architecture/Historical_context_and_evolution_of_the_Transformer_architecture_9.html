<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>historical_context_and_evolution_of_the_transformer_architecture_9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-10.-compare-transformer-architectures-with-their-predecessors-rnns-cnns-in-terms-of-handling-sequential-data.-under-what-circumstances-might-a-hybrid-architecture-be-advantageous" class="level2">
<h2 class="anchored" data-anchor-id="question-10.-compare-transformer-architectures-with-their-predecessors-rnns-cnns-in-terms-of-handling-sequential-data.-under-what-circumstances-might-a-hybrid-architecture-be-advantageous">Question: 10. Compare Transformer architectures with their predecessors (RNNs, CNNs) in terms of handling sequential data. Under what circumstances might a hybrid architecture be advantageous?</h2>
<p><strong>Best Answer</strong></p>
<p>Transformers have revolutionized sequential data processing, largely surpassing Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) in many applications. However, each architecture has unique strengths and weaknesses, making hybrid architectures a valuable consideration in certain scenarios.</p>
<p><strong>1. Comparison of Architectures:</strong></p>
<ul>
<li><p><strong>Recurrent Neural Networks (RNNs):</strong></p>
<ul>
<li><strong>Mechanism:</strong> RNNs process sequential data element by element, maintaining a hidden state that encapsulates information about past elements. Variants like LSTMs and GRUs address the vanishing gradient problem, enabling them to capture longer-range dependencies better than simple RNNs.</li>
<li><strong>Strengths:</strong> RNNs are inherently designed for sequential data. They are effective in tasks where the order of elements is crucial, such as time series prediction, natural language processing, and speech recognition.</li>
<li><strong>Weaknesses:</strong> The sequential processing of RNNs limits parallelization, making training slow, especially on long sequences. They also struggle with very long-range dependencies despite LSTM and GRU improvements, and are often outperformed by Transformers in tasks requiring attention across the entire sequence.</li>
<li><strong>Mathematical Representation (Illustrative LSTM):</strong>
<ul>
<li>Input: <span class="math inline">\(x_t\)</span> (input at time step t)</li>
<li>Hidden State: <span class="math inline">\(h_t\)</span> (hidden state at time step t)</li>
<li>Cell State: <span class="math inline">\(c_t\)</span> (cell state at time step t)</li>
<li>Equations: <span class="math display">\[
\begin{aligned}
f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f) \\
i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i) \\
\tilde{c}_t &amp;= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
c_t &amp;= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + b_o) \\
h_t &amp;= o_t \odot \tanh(c_t)
\end{aligned}
\]</span> where <span class="math inline">\(\sigma\)</span> is the sigmoid function and <span class="math inline">\(\odot\)</span> denotes element-wise multiplication.</li>
</ul></li>
</ul></li>
<li><p><strong>Convolutional Neural Networks (CNNs):</strong></p>
<ul>
<li><strong>Mechanism:</strong> CNNs apply convolutional filters to local segments of the input sequence, capturing local patterns and features.</li>
<li><strong>Strengths:</strong> CNNs excel at capturing local dependencies and are highly parallelizable. They are computationally efficient and effective for tasks like image recognition and some sequence modeling tasks where local features are important.</li>
<li><strong>Weaknesses:</strong> CNNs struggle to capture long-range dependencies effectively without stacking many layers or using dilated convolutions, which can increase complexity and computational cost. They are not inherently designed for sequential data in the same way as RNNs or Transformers, and may require additional mechanisms to incorporate sequence order.</li>
<li><strong>Mathematical Representation:</strong> <span class="math display">\[
y[i] = \sum_{k=0}^{K-1} x[i+k] \cdot w[k] + b
\]</span> where <span class="math inline">\(x\)</span> is the input sequence, <span class="math inline">\(w\)</span> is the convolutional filter of length <span class="math inline">\(K\)</span>, <span class="math inline">\(b\)</span> is the bias, and <span class="math inline">\(y\)</span> is the output. Multiple layers of convolution are often stacked.</li>
</ul></li>
<li><p><strong>Transformers:</strong></p>
<ul>
<li><strong>Mechanism:</strong> Transformers rely on self-attention mechanisms to weigh the importance of different parts of the input sequence when processing each element. This allows them to capture long-range dependencies effectively and process the entire sequence in parallel.</li>
<li><strong>Strengths:</strong> Transformers excel at capturing long-range dependencies and are highly parallelizable, leading to faster training times. They have achieved state-of-the-art results in various NLP tasks, as well as in computer vision and other domains.</li>
<li><strong>Weaknesses:</strong> Transformers have a quadratic computational complexity with respect to the sequence length, making them computationally expensive for very long sequences. They also require large amounts of data for training and may be less effective than RNNs or CNNs in scenarios with limited data or where local dependencies are paramount.</li>
<li><strong>Mathematical Representation (Self-Attention):</strong> <span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span> where <span class="math inline">\(Q\)</span> is the query matrix, <span class="math inline">\(K\)</span> is the key matrix, <span class="math inline">\(V\)</span> is the value matrix, and <span class="math inline">\(d_k\)</span> is the dimension of the keys.</li>
</ul></li>
</ul>
<p><strong>2. Circumstances Favoring Hybrid Architectures:</strong></p>
<p>Hybrid architectures can be advantageous in several scenarios:</p>
<ul>
<li><p><strong>Exploiting Local and Global Dependencies:</strong> When the data contains both important local features and long-range dependencies, a hybrid approach can combine the strengths of different architectures. For example:</p>
<ul>
<li><strong>CNN + Transformer:</strong> Use CNN layers to extract local features from the input sequence, and then feed these features into a Transformer to capture long-range dependencies. This can be useful in tasks such as speech recognition or video analysis.</li>
<li><strong>RNN + Transformer:</strong> Use an RNN to pre-process the input sequence and capture sequential information, then use a Transformer to model long-range dependencies between the RNN’s hidden states. This approach could be beneficial when sequential context and global attention are both crucial.</li>
</ul></li>
<li><p><strong>Reducing Computational Cost:</strong> For very long sequences, the quadratic complexity of Transformers can be a bottleneck. Hybrid architectures can help reduce this cost:</p>
<ul>
<li><strong>CNN + Transformer (with pooling):</strong> Use CNN layers with pooling to reduce the sequence length before feeding it into a Transformer. This can significantly reduce the computational cost of the Transformer while still allowing it to capture long-range dependencies.</li>
<li><strong>Sparse Transformers:</strong> While technically not a ‘hybrid’, it’s worth mentioning that sparse attention mechanisms can also alleviate computational cost. These restrict the attention to a subset of the input, reducing the quadratic complexity. Hybrid approaches could incorporate sparse attention within a transformer block, combined with CNN or RNN components elsewhere in the architecture.</li>
</ul></li>
<li><p><strong>Handling Multi-Modal Data:</strong> Hybrid architectures can effectively combine different modalities of data:</p>
<ul>
<li><strong>Text + Image:</strong> Use CNNs to process image data and Transformers to process text data, then fuse the representations to perform tasks such as image captioning or visual question answering.</li>
<li><strong>Time Series + Text:</strong> Use RNNs or CNNs to process time series data and Transformers to process associated text data, enabling tasks like predictive maintenance with contextual information from maintenance logs.</li>
</ul></li>
<li><p><strong>Improving Interpretability:</strong> Hybrid architectures can sometimes improve interpretability by allowing different components to focus on specific aspects of the data.</p>
<ul>
<li>For example, using a CNN to extract local features can make it easier to understand which features the model is attending to in the Transformer layers. Attention visualization from the Transformer layer, combined with the identified CNN features, can provide a more complete picture.</li>
</ul></li>
<li><p><strong>Small Datasets:</strong> In situations where only limited training data is available, the inductive bias of RNNs or CNNs can be helpful. Starting with pre-trained CNN or RNN layers, then fine-tuning with a Transformer on top, can provide a boost in performance compared to training a Transformer from scratch.</p></li>
</ul>
<p><strong>3. Examples:</strong></p>
<ul>
<li><strong>Vision Transformer (ViT) with CNN Stem:</strong> A CNN can be used as a “stem” to pre-process images into patch embeddings before feeding them into a Vision Transformer, which is a kind of hybrid architecture leveraging the local feature extraction capabilities of CNNs.</li>
<li><strong>Speech Recognition:</strong> Combine CNNs for acoustic feature extraction with Transformers for language modeling and sequence-to-sequence mapping.</li>
</ul>
<p>In summary, while Transformers have become the dominant architecture for many sequence modeling tasks, RNNs and CNNs still have valuable strengths. Hybrid architectures can be advantageous when dealing with complex data that contains both local and global dependencies, when computational cost is a concern, when handling multi-modal data, when interpretability is important, or when limited training data is available. The choice of architecture or hybrid architecture ultimately depends on the specific requirements of the task and the characteristics of the data.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to present this information in an interview:</p>
<ol type="1">
<li><p><strong>Start with a high-level comparison:</strong> “Transformers have become incredibly powerful for sequential data, but it’s important to remember the strengths of their predecessors, RNNs and CNNs. Each has its place, and hybrid architectures can leverage the best of all worlds.”</p></li>
<li><p><strong>Discuss RNNs:</strong></p>
<ul>
<li>“RNNs are inherently sequential, processing data step-by-step while maintaining a hidden state. LSTMs and GRUs address the vanishing gradient problem, but even they can struggle with very long sequences.”</li>
<li>“The sequential nature of RNNs makes them hard to parallelize, a major drawback compared to Transformers. A simplified view of the LSTM equations is:” Then present the equations concisely, focusing on their iterative nature. Avoid getting bogged down; highlight the key components (<span class="math inline">\(f_t, i_t, c_t, o_t, h_t\)</span>) and what they represent (forget gate, input gate, cell state, output gate, hidden state).</li>
<li>“Despite their limitations, RNNs can be effective when the data is inherently sequential and localized context is important.”</li>
</ul></li>
<li><p><strong>Discuss CNNs:</strong></p>
<ul>
<li>“CNNs excel at capturing local features through convolutional filters. They are highly parallelizable and computationally efficient.”</li>
<li>“However, capturing long-range dependencies requires stacking many layers, which can increase complexity. Unlike RNNs and Transformers, they don’t naturally account for sequence order without specific adaptations.”</li>
<li>“Think of CNNs as feature extractors that operate on local windows of the sequence.” Briefly show the convolution equation and explain how each element contributes to the calculation of the output.</li>
<li>“CNNs are valuable when local patterns are crucial, and the order is less important, or when combined with other architectures.”</li>
</ul></li>
<li><p><strong>Discuss Transformers:</strong></p>
<ul>
<li>“Transformers use self-attention to weigh the importance of different parts of the sequence, capturing long-range dependencies and enabling parallel processing.”</li>
<li>“This parallelization leads to faster training times and state-of-the-art results in many tasks, especially with large datasets.”</li>
<li>“The attention mechanism is key. Simplified, it can be represented as: <present the="" attention="" equation=""> <span class="math inline">\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span>. Here, we are calculating weights based on the relationship between the Queries and Keys and using these to weight the Values.</present></li>
<li>“The downside is the quadratic complexity with sequence length, making them expensive for very long inputs and requiring substantial training data.”</li>
</ul></li>
<li><p><strong>Transition to Hybrid Architectures:</strong></p>
<ul>
<li>“Given these strengths and weaknesses, hybrid architectures can be very advantageous. They allow us to combine the best aspects of each approach.”</li>
</ul></li>
<li><p><strong>Explain scenarios favoring hybrid architectures:</strong></p>
<ul>
<li>“One key scenario is when both local features and long-range dependencies are important. For example, using CNNs for local feature extraction and then feeding those features to a Transformer.”</li>
<li>“Another is reducing computational cost. CNNs can reduce the sequence length before the Transformer, or specialized Transformer architectures can provide sparsity.”</li>
<li>“Hybrid architectures are also great for multi-modal data. CNNs for images, Transformers for text, and then a fusion layer to combine the representations.”</li>
<li>“Interpretability is also a factor; some hybrid designs make it easier to understand what each component is focusing on.”</li>
<li>“Finally, with limited data, the inductive bias of CNNs or RNNs can give you a head start via transfer learning.”</li>
</ul></li>
<li><p><strong>Provide examples:</strong></p>
<ul>
<li>“A concrete example is using a CNN stem in a Vision Transformer to process image patches before the Transformer layers. Or in speech recognition where CNNs extract acoustic features.”</li>
</ul></li>
<li><p><strong>Concluding Statement:</strong></p>
<ul>
<li>“Ultimately, the choice depends on the specific task, the data characteristics, and the computational constraints. Understanding the strengths of each architecture is crucial for designing an effective solution, whether it’s a single architecture or a hybrid combination.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanations. Allow the interviewer time to process the information.</li>
<li><strong>Use visual aids:</strong> If possible, use diagrams or sketches to illustrate the architectures and their interactions.</li>
<li><strong>Check for understanding:</strong> Ask the interviewer if they have any questions or if they’d like you to elaborate on a specific point.</li>
<li><strong>Don’t be afraid to simplify:</strong> If the interviewer doesn’t have a deep technical background, tailor your explanation to their level of understanding. Focus on the high-level concepts rather than getting lost in the details.</li>
<li><strong>Show enthusiasm:</strong> Let your passion for the topic shine through. This will make your answer more engaging and memorable.</li>
<li><strong>Avoid jargon:</strong> While technical terms are necessary, try to explain them clearly and concisely. Avoid using overly complex language that could confuse the interviewer.</li>
<li><strong>Stay conversational:</strong> This isn’t a lecture; it’s a conversation. Engage with the interviewer and make eye contact.</li>
</ul>
<p>By following these guidelines, you can effectively communicate your understanding of Transformer architectures and hybrid approaches, demonstrating your senior-level expertise.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>