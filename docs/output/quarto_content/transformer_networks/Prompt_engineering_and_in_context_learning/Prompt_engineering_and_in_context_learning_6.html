<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>prompt_engineering_and_in_context_learning_6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-7.-how-would-you-experimentally-evaluate-the-effectiveness-of-a-given-prompt-design-what-metrics-and-evaluations-would-you-consider" class="level2">
<h2 class="anchored" data-anchor-id="question-7.-how-would-you-experimentally-evaluate-the-effectiveness-of-a-given-prompt-design-what-metrics-and-evaluations-would-you-consider">Question: 7. How would you experimentally evaluate the effectiveness of a given prompt design? What metrics and evaluations would you consider?</h2>
<p><strong>Best Answer</strong></p>
<p>Evaluating the effectiveness of a prompt design is crucial for optimizing the performance of large language models (LLMs) in various applications. A comprehensive evaluation strategy should incorporate both quantitative and qualitative metrics, as well as rigorous experimental designs. Here’s a breakdown of the key considerations:</p>
<p><strong>1. Quantitative Metrics:</strong></p>
<ul>
<li><strong>Accuracy/Correctness:</strong> This is often the most fundamental metric. It measures how accurately the LLM’s output matches the ground truth or expected result. This depends heavily on the type of task.
<ul>
<li>For classification tasks: Accuracy, Precision, Recall, F1-score, and AUC (Area Under the Curve) are standard.</li>
<li>For question answering: Exact Match (EM) and F1-score are commonly used. EM requires the generated answer to exactly match the reference answer, while F1-score measures the overlap between the generated and reference answers.</li>
<li>For text generation tasks: Metrics like BLEU (Bilingual Evaluation Understudy), ROUGE (Recall-Oriented Understudy for Gisting Evaluation), and METEOR (Metric for Evaluation of Translation with Explicit Ordering) can be used to assess the similarity between the generated text and reference text. However, these metrics have limitations in capturing semantic similarity and may require human evaluation.</li>
</ul></li>
<li><strong>Consistency:</strong> Measures how consistently the LLM produces similar outputs for similar inputs. Inconsistent behavior can be problematic in production settings.
<ul>
<li><em>Variance across multiple runs:</em> Run the same prompt multiple times with different random seeds (if the LLM supports it) and measure the variance in the outputs. Lower variance indicates better consistency. Mathematically, we can calculate the variance of a chosen metric <span class="math inline">\(M\)</span> (e.g., accuracy) across <span class="math inline">\(n\)</span> runs as: <span class="math display">\[Var(M) = \frac{1}{n-1}\sum_{i=1}^{n} (M_i - \bar{M})^2\]</span> where <span class="math inline">\(M_i\)</span> is the metric value for the <span class="math inline">\(i\)</span>-th run and <span class="math inline">\(\bar{M}\)</span> is the mean metric value.</li>
<li><em>Semantic Similarity:</em> Use embedding models (e.g., Sentence Transformers) to encode the outputs from multiple runs and calculate the cosine similarity between the embeddings. Higher cosine similarity indicates better semantic consistency.</li>
</ul></li>
<li><strong>Robustness:</strong> Evaluates how well the prompt design performs under noisy or adversarial inputs. This is especially important when the LLM is exposed to user-generated content.
<ul>
<li><em>Adversarial Attacks:</em> Introduce small perturbations to the input prompt (e.g., adding typos, paraphrasing) and measure the change in output quality.</li>
<li><em>Out-of-Distribution Data:</em> Test the prompt design on data that is different from the data used for training or fine-tuning the LLM.</li>
</ul></li>
<li><strong>Efficiency:</strong> Considers the computational resources required to generate the output, including latency and cost.
<ul>
<li><em>Latency:</em> Measure the time taken to generate the output for a given prompt.</li>
<li><em>Cost:</em> For paid LLM APIs, track the number of tokens consumed per prompt.</li>
<li>It is essential to balance accuracy with the need to minimise <span class="math inline">\(Cost(prompt)\)</span>, the API cost per prompt request. <span class="math display">\[Effectiveness = Accuracy - \lambda * Cost(prompt)\]</span> where <span class="math inline">\(\lambda\)</span> weights cost relative to accuracy.</li>
</ul></li>
</ul>
<p><strong>2. Qualitative Metrics:</strong></p>
<ul>
<li><strong>Relevance:</strong> Assess whether the LLM’s output is relevant to the input prompt and the intended task.</li>
<li><strong>Coherence:</strong> Evaluate the logical flow and readability of the generated text. Does it make sense? Is it well-structured?</li>
<li><strong>Fluency:</strong> Judge the naturalness and grammatical correctness of the output.</li>
<li><strong>Completeness:</strong> Determine whether the output provides a comprehensive answer to the question or fulfills the requirements of the task.</li>
<li><strong>User Satisfaction:</strong> Gather feedback from users on the quality and usefulness of the LLM’s output. This can be done through surveys, A/B testing, or user interviews.</li>
</ul>
<p><strong>3. Experimental Designs:</strong></p>
<ul>
<li><strong>A/B Testing:</strong> Compare the performance of two different prompt designs on the same task. Randomly assign users or inputs to one of the two prompts and measure the metrics of interest. Statistical significance tests (e.g., t-tests, chi-squared tests) can be used to determine if the differences in performance are statistically significant.</li>
<li><strong>Ablation Studies:</strong> Systematically remove or modify parts of the prompt to understand their impact on performance. For example, you could remove specific keywords, instructions, or examples from the prompt and measure the change in accuracy. This helps to identify the most important components of the prompt design.</li>
<li><strong>Controlled Experiments:</strong> Design experiments to isolate the effects of different prompt elements. This involves manipulating specific variables in the prompt (e.g., the number of examples, the type of instructions) and measuring their impact on performance while controlling for other factors.</li>
<li><strong>Human Evaluation:</strong> Involve human evaluators to assess the quality of the LLM’s output. Human evaluators can provide more nuanced feedback than automated metrics, especially for tasks that require creativity, common sense reasoning, or subjective judgment. Employ clear guidelines and scoring rubrics to ensure consistency and reliability in human evaluations.</li>
</ul>
<p><strong>4. Considerations for Specific Tasks:</strong></p>
<ul>
<li><strong>Code Generation:</strong> Evaluate the correctness and efficiency of the generated code. Metrics like pass@k (the probability of generating at least one correct solution within k attempts) and execution time are relevant.</li>
<li><strong>Summarization:</strong> Assess the informativeness, coherence, and conciseness of the generated summaries. Metrics like ROUGE and human evaluation are commonly used.</li>
<li><strong>Dialogue Generation:</strong> Evaluate the coherence, relevance, and engagingness of the generated dialogue. Metrics like BLEU, perplexity, and human evaluation are relevant.</li>
</ul>
<p><strong>5. Implementation Details:</strong></p>
<ul>
<li><strong>Dataset Selection:</strong> Choose a representative dataset that reflects the intended use case of the LLM. Ensure that the dataset is of high quality and contains sufficient examples to evaluate the prompt design effectively.</li>
<li><strong>Evaluation Infrastructure:</strong> Set up a robust evaluation pipeline that automates the process of running prompts, collecting metrics, and analyzing results. Use appropriate tools and libraries for data processing, metric calculation, and statistical analysis.</li>
<li><strong>Statistical Significance:</strong> When comparing different prompt designs, ensure that the results are statistically significant. Use appropriate statistical tests and report p-values and confidence intervals.</li>
</ul>
<p>In summary, a comprehensive evaluation of prompt design effectiveness requires a combination of quantitative metrics, qualitative assessments, and rigorous experimental designs. The specific metrics and evaluations should be tailored to the specific task and the intended use case of the LLM.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“Evaluating prompt design is critical for maximizing the performance of LLMs. A strong evaluation combines quantitative metrics, qualitative assessments, and controlled experiments.”</li>
</ul></li>
<li><strong>Quantitative Metrics (Focus on the most important ones first):</strong>
<ul>
<li>“On the quantitative side, accuracy is paramount. We can measure it with standard metrics like accuracy, precision, recall, and F1-score, depending on the task.”</li>
<li>“Consistency is also key, indicating how reliably the model produces similar outputs for similar inputs. We can quantify this by measuring variance across multiple runs or using semantic similarity metrics on the outputs.”</li>
<li>“Robustness matters, too, especially when dealing with potentially noisy or adversarial inputs. We can test this by introducing perturbations to the prompt and observing the impact on output quality.”</li>
<li>“Efficiency, which means latency and cost, is also important. Cost especially is about balancing accuracy with minimizing API costs.”</li>
</ul></li>
<li><strong>Explain One Formula (Optional, based on Interviewer’s Interest):</strong>
<ul>
<li>“For instance, when assessing consistency, we can calculate the variance of a chosen metric, say accuracy (briefly show the variance formula, but don’t get bogged down in derivation): <span class="math display">\[Var(M) = \frac{1}{n-1}\sum_{i=1}^{n} (M_i - \bar{M})^2\]</span>”</li>
<li>“This formula helps quantify the spread of results, giving a tangible measure of consistency.”</li>
</ul></li>
<li><strong>Qualitative Metrics:</strong>
<ul>
<li>“While numbers are important, qualitative aspects provide crucial context. Relevance, coherence, fluency, completeness, and user satisfaction tell us about the output’s usability and quality from a human perspective.”</li>
<li>“User satisfaction is extremely important, and that’s why surveys, A/B testing, or user interviews provide valuable insights into overall user experience.”</li>
</ul></li>
<li><strong>Experimental Designs:</strong>
<ul>
<li>“To isolate the impact of specific prompt elements, we use several experimental designs.”</li>
<li>“A/B testing allows us to compare two prompt designs head-to-head, using statistical tests to confirm if the observed performance difference is significant.”</li>
<li>“Ablation studies systematically remove parts of the prompt to understand their contribution, helping us refine and optimize the prompt design.”</li>
<li>“Controlled experiments manipulate prompt variables, measuring their effects on performance. This enables precise understanding of the design elements. Human evaluations, with clear guidelines, provide nuanced insights especially for creative and reasoning tasks.”</li>
</ul></li>
<li><strong>Task-Specific Considerations:</strong>
<ul>
<li>“The specific metrics and methods need to be tailored to the task. For code generation, we look at correctness and efficiency; for summarization, informativeness and conciseness are key; and for dialogue, coherence and engagingness are critical.”</li>
</ul></li>
<li><strong>Implementation Details (mention Briefly):</strong>
<ul>
<li>“Finally, reliable implementation includes careful dataset selection, a robust evaluation pipeline, and statistical significance testing. These steps ensure the evaluation is valid and reproducible.”</li>
</ul></li>
<li><strong>Concluding Remarks:</strong>
<ul>
<li>“In summary, a comprehensive evaluation strategy should incorporate quantitative metrics, qualitative assessments, and rigorous experimental designs. The specific metrics and evaluations should be tailored to the specific task and the intended use case of the LLM. This holistic approach is key to building effective and reliable prompt designs.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use Visual Aids (if available):</strong> If you’re in a virtual interview, consider sharing your screen to show example metrics or experimental setups.</li>
<li><strong>Check for Understanding:</strong> Pause occasionally to ask if the interviewer has any questions.</li>
<li><strong>Tailor to the Audience:</strong> Adjust the level of detail based on the interviewer’s background and the context of the discussion.</li>
<li><strong>Stay Confident:</strong> Speak clearly and confidently, demonstrating your expertise in the area.</li>
</ul>
<p><strong>Walking Through Mathematical Sections:</strong></p>
<ul>
<li><strong>Introduce the Purpose:</strong> Before diving into a formula, explain what you’re trying to quantify.</li>
<li><strong>Explain the Components:</strong> Briefly describe each variable in the formula.</li>
<li><strong>Avoid Derivation:</strong> Unless specifically asked, avoid getting bogged down in the mathematical derivation.</li>
<li><strong>Focus on Interpretation:</strong> Emphasize what the result of the calculation tells you about the prompt design.</li>
<li><strong>Offer to Elaborate:</strong> Let the interviewer know that you can provide more details if they’re interested.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>