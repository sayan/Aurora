<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>prompt_engineering_and_in_context_learning_8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-9.-when-deploying-prompt-based-systems-in-production-what-scalability-issues-might-arise-and-how-would-you-address-them" class="level2">
<h2 class="anchored" data-anchor-id="question-9.-when-deploying-prompt-based-systems-in-production-what-scalability-issues-might-arise-and-how-would-you-address-them">Question: 9. When deploying prompt-based systems in production, what scalability issues might arise, and how would you address them?</h2>
<p><strong>Best Answer</strong></p>
<p>Deploying prompt-based systems in production introduces several scalability challenges. These challenges stem from the computational intensity of large language models (LLMs), the variable nature of user prompts, and the need to maintain consistent performance under increasing load. Here’s a breakdown of potential issues and mitigation strategies:</p>
<p><strong>1. Response Time (Latency)</strong></p>
<ul>
<li><p><strong>Issue:</strong> LLMs are computationally expensive, and generating responses, especially for complex prompts, can take a significant amount of time. High latency leads to poor user experience and can limit the system’s throughput.</p></li>
<li><p><strong>Mitigation Strategies:</strong></p>
<ul>
<li><strong>Model Optimization:</strong>
<ul>
<li><strong>Model Distillation:</strong> Train a smaller, faster model to mimic the behavior of a larger, more accurate model. This reduces the computational burden per request.</li>
<li><strong>Quantization:</strong> Reduce the precision of the model’s weights (e.g., from 32-bit floating point to 8-bit integer). This reduces memory footprint and can improve inference speed.</li>
</ul></li>
<li><strong>Caching:</strong>
<ul>
<li><strong>Prompt Caching:</strong> Store the results of frequently used prompts. A simple key-value cache, where the prompt serves as the key and the LLM response as the value, can significantly reduce latency for repetitive queries. However, cache invalidation strategies (e.g., TTL-based) are essential.</li>
<li><strong>Semantic Caching:</strong> Instead of exact prompt matching, identify prompts with similar semantic meaning and reuse cached responses. This requires embedding the prompts and using a similarity metric (e.g., cosine similarity) to find close matches. Semantic caching introduces additional complexity but can greatly improve cache hit rate.</li>
</ul></li>
<li><strong>Asynchronous Processing:</strong> Offload prompt processing to a background queue. Return an immediate response to the user (e.g., a “processing” message) and notify them when the final result is ready. This decouples the user’s request from the LLM’s processing time.</li>
<li><strong>Hardware Acceleration:</strong> Utilize specialized hardware like GPUs or TPUs to accelerate LLM inference. These accelerators are designed for parallel processing and can significantly reduce response times.</li>
<li><strong>Request Batching:</strong> Process multiple prompts in a single batch to take advantage of the parallel processing capabilities of GPUs/TPUs. This amortizes the overhead of model loading and inference across multiple requests.</li>
<li><strong>Prompt Optimization:</strong> Rewriting prompts to be more concise and focused can reduce the LLM’s processing time. Techniques like “chain-of-thought” prompting can improve accuracy but also increase latency, so careful optimization is necessary.</li>
</ul></li>
</ul>
<p><strong>2. Computational Cost</strong></p>
<ul>
<li><p><strong>Issue:</strong> Running LLMs is expensive, especially at scale. The cost is typically driven by the number of tokens processed (input + output).</p></li>
<li><p><strong>Mitigation Strategies:</strong></p>
<ul>
<li><strong>Prompt Engineering:</strong> Design prompts that elicit desired responses with minimal token usage. Techniques include:
<ul>
<li><strong>Conciseness:</strong> Avoid unnecessary words or phrases.</li>
<li><strong>Structured Prompts:</strong> Use clear and well-defined formats to guide the LLM’s response.</li>
<li><strong>Few-Shot Learning:</strong> Provide a small number of examples in the prompt to improve accuracy with shorter output lengths.</li>
</ul></li>
<li><strong>Response Length Control:</strong> Limit the maximum length of the LLM’s response. This can be enforced through parameters like <code>max_tokens</code> in the LLM API.</li>
<li><strong>Model Selection:</strong> Choose the smallest model that meets the required accuracy and performance criteria. Larger models are generally more expensive to run.</li>
<li><strong>Rate Limiting:</strong> Implement rate limits to prevent abuse and control costs. This can be done on a per-user or per-IP address basis.</li>
<li><strong>Cost Monitoring:</strong> Track the cost of LLM usage closely to identify areas for optimization. Tools provided by LLM providers (e.g., OpenAI’s usage dashboard) can be helpful.</li>
<li><strong>Strategic Retries:</strong> Implement exponential backoff with jitter for retry attempts to avoid overwhelming the system during peak load. Define clear policies for handling failed requests and preventing infinite retry loops.</li>
</ul></li>
</ul>
<p><strong>3. Prompt Length Limitations</strong></p>
<ul>
<li><p><strong>Issue:</strong> Most LLMs have a maximum input length (e.g., 4096 tokens for GPT-3). Long prompts can be truncated, leading to loss of information and degraded performance.</p></li>
<li><p><strong>Mitigation Strategies:</strong></p>
<ul>
<li><strong>Prompt Summarization:</strong> Summarize long documents or conversations before feeding them to the LLM. Techniques like extractive summarization (selecting existing sentences) or abstractive summarization (generating new sentences) can be used.</li>
<li><strong>Information Retrieval:</strong> Instead of including the entire context in the prompt, retrieve relevant information from a database or knowledge base and include only the retrieved snippets in the prompt.</li>
<li><strong>Prompt Segmentation:</strong> Divide long prompts into smaller chunks and process them sequentially. Combine the results to generate the final output. This approach requires careful design to ensure consistency and coherence across chunks.</li>
<li><strong>Model Fine-tuning:</strong> Fine-tune a model on longer sequences to increase its maximum input length. This requires a significant amount of training data and computational resources.</li>
<li><strong>Truncation Strategies:</strong> Implement intelligent truncation strategies that preserve the most important information in the prompt when it exceeds the maximum length. For example, prioritize preserving the beginning and end of the prompt, as these often contain crucial instructions or context.</li>
</ul></li>
</ul>
<p><strong>4. Output Variability and Quality</strong></p>
<ul>
<li><p><strong>Issue:</strong> LLMs can generate different responses to the same prompt, especially with non-deterministic decoding strategies. This variability can be undesirable in production systems where consistency is important.</p></li>
<li><p><strong>Mitigation Strategies:</strong></p>
<ul>
<li><strong>Temperature Control:</strong> Reduce the temperature parameter in the LLM API. A lower temperature makes the model more deterministic and reduces the variability of the output. A temperature of 0 will typically produce the most deterministic output, but it may also lead to less creative or insightful responses.</li>
<li><strong>Top-p Sampling:</strong> Use top-p sampling (nucleus sampling) to limit the set of tokens the model can choose from. This can improve the quality and consistency of the output.</li>
<li><strong>Prompt Engineering:</strong> Craft prompts that are specific and unambiguous to reduce the ambiguity in the LLM’s response.</li>
<li><strong>Response Validation:</strong> Implement a validation step to check the LLM’s response against predefined criteria. If the response fails validation, re-prompt the model or use a fallback mechanism.</li>
<li><strong>Ensemble Methods:</strong> Combine the outputs of multiple LLMs or multiple runs of the same LLM to reduce variability and improve accuracy.</li>
<li><strong>Fine-tuning:</strong> Fine-tune a model on a specific task or domain to improve the consistency and quality of its output. The more specific the training data, the less variability the model will produce.</li>
</ul></li>
</ul>
<p><strong>5. Concurrency and Throughput</strong></p>
<ul>
<li><p><strong>Issue:</strong> Handling a large number of concurrent requests can overwhelm the system, leading to increased latency and reduced throughput.</p></li>
<li><p><strong>Mitigation Strategies:</strong></p>
<ul>
<li><strong>Load Balancing:</strong> Distribute traffic across multiple LLM instances to prevent any single instance from being overloaded.</li>
<li><strong>Auto-scaling:</strong> Automatically scale the number of LLM instances based on the current load. Cloud platforms like AWS, Azure, and GCP provide auto-scaling capabilities.</li>
<li><strong>Connection Pooling:</strong> Use connection pooling to reuse existing connections to the LLM service, reducing the overhead of establishing new connections for each request.</li>
<li><strong>Queueing:</strong> Use a message queue to buffer incoming requests and process them asynchronously. This can help to smooth out traffic spikes and prevent the system from being overwhelmed.</li>
</ul></li>
</ul>
<p><strong>6. Monitoring and Observability</strong></p>
<ul>
<li><p><strong>Issue:</strong> Without proper monitoring, it’s difficult to identify and address scalability issues.</p></li>
<li><p><strong>Mitigation Strategies:</strong></p>
<ul>
<li><strong>Metrics Collection:</strong> Collect metrics on response time, throughput, error rates, and resource utilization.</li>
<li><strong>Logging:</strong> Log all requests and responses for debugging and analysis.</li>
<li><strong>Alerting:</strong> Set up alerts to notify the team when critical metrics exceed predefined thresholds.</li>
<li><strong>Tracing:</strong> Use distributed tracing to track requests as they flow through the system.</li>
</ul></li>
</ul>
<p><strong>7. Model Updates and Versioning</strong></p>
<ul>
<li><p><strong>Issue:</strong> Updating LLMs can be disruptive and lead to inconsistencies if not managed properly.</p></li>
<li><p><strong>Mitigation Strategies:</strong></p>
<ul>
<li><strong>Blue/Green Deployments:</strong> Deploy the new model alongside the old model and gradually shift traffic to the new model.</li>
<li><strong>Canary Releases:</strong> Release the new model to a small percentage of users to monitor its performance before rolling it out to everyone.</li>
<li><strong>Versioning:</strong> Maintain multiple versions of the model and allow users to specify which version they want to use.</li>
<li><strong>Feature Flags:</strong> Use feature flags to enable or disable new features without redeploying the model.</li>
</ul></li>
</ul>
<p><strong>8. Security Considerations</strong></p>
<ul>
<li><p><strong>Issue:</strong> Prompt-based systems are vulnerable to prompt injection attacks, where malicious users craft prompts that can manipulate the LLM’s behavior.</p></li>
<li><p><strong>Mitigation Strategies:</strong></p>
<ul>
<li><strong>Prompt Sanitization:</strong> Sanitize user inputs to remove potentially malicious code or commands.</li>
<li><strong>Input Validation:</strong> Validate user inputs against predefined criteria to prevent unexpected or harmful inputs.</li>
<li><strong>Output Monitoring:</strong> Monitor the LLM’s output for signs of prompt injection attacks.</li>
<li><strong>Sandboxing:</strong> Run the LLM in a sandboxed environment to limit its access to system resources.</li>
<li><strong>Least Privilege:</strong> Grant the LLM only the necessary permissions to perform its tasks.</li>
</ul></li>
</ul>
<p>By addressing these challenges proactively, organizations can successfully deploy prompt-based systems in production and achieve the desired scalability, performance, and cost-effectiveness. The specific strategies employed will depend on the specific application, the characteristics of the LLM being used, and the available resources.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested approach to answer this question in an interview:</p>
<ol type="1">
<li><p><strong>Start with a high-level overview:</strong> “Deploying prompt-based systems at scale presents several challenges related to response time, cost, prompt length limitations, output consistency, and security. These stem from the computational demands of LLMs and the dynamic nature of user inputs.”</p></li>
<li><p><strong>Address Response Time (Latency):</strong> “One major issue is response time. LLMs can be slow, which impacts user experience. To mitigate this, we can use techniques like model distillation and quantization to reduce model size. Caching strategies, including both exact prompt caching and more advanced semantic caching, can also significantly reduce latency for frequent or similar queries. Asynchronous processing, hardware acceleration with GPUs/TPUs, and optimizing prompt structure are other vital methods.”</p>
<ul>
<li><em>Mathematical element:</em> If mentioning quantization, you could briefly touch on how it works, e.g., “Quantization involves mapping the original floating point values to a reduced set of discrete values. For example, we can use the following equation for linear quantization, <span class="math inline">\(Q = round(\frac{R}{S} + Z)\)</span>, where R is real value, S is scaling factor, and Z is zero point. By reducing the number of bits needed to represent each weight, we can reduce memory and computational requirements”. Explain that this reduces precision but can significantly increase speed.</li>
</ul></li>
<li><p><strong>Move onto Computational Cost:</strong> “Another significant concern is cost. LLMs are expensive to run, especially considering the number of tokens processed. We can employ prompt engineering techniques, such as creating concise prompts, structured prompts, and few-shot learning examples. Limiting the maximum response length and choosing a right-sized model are also important. Establishing rate limits and rigorous cost monitoring are crucial for managing expenses.”</p></li>
<li><p><strong>Discuss Prompt Length Limitations:</strong> “Many LLMs have input length limits. To address this, we can summarize the input, use information retrieval to only include relevant snippets in the prompt, or segment long prompts. In certain cases, fine-tuning the model on longer sequences or using smarter truncation methods are also valid approaches.”</p></li>
<li><p><strong>Address Output Variability:</strong> “Output variability is another concern, we want reliable, consistent results. Setting the temperature parameter to a lower value in the LLM APIs can make the output more predictable. Combining this with Top-p sampling or carefully engineering our prompts and validating the LLM output will lead to reduced variance.”</p></li>
<li><p><strong>Mention Concurrency and Throughput:</strong> “Concurrency and throughput become key at scale. Using Load Balancing to distribute traffic across multiple LLM instances is necessary to avoid overwhelming single instances. Using connection pooling to reuse existing connections also helps to reduce overhead of re-establishing new connections.”</p></li>
<li><p><strong>Highlight Monitoring and Observability:</strong> “Effective monitoring and observability are essential. We need to track metrics like response time, error rates, and resource usage. Centralized Logging, Alerting systems and Tracing are key elements to building observable LLM based systems.”</p></li>
<li><p><strong>Mention Security Considerations:</strong> “Finally, we need to be mindful of security vulnerabilities. Prompt injection attacks are a potential threat and need to be mitigated with input sanitization, validation, output monitoring, and sandboxing.”</p></li>
<li><p><strong>Summarize and conclude:</strong> “By proactively addressing these challenges through a combination of architectural, engineering, and prompt-based techniques, we can deploy robust, scalable, and cost-effective prompt-based systems in production.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Balance technical detail with clarity:</strong> Avoid overwhelming the interviewer with excessive jargon. Explain complex concepts in a clear and concise manner.</li>
<li><strong>Showcase your problem-solving skills:</strong> Frame the discussion around the problems that arise in production and the strategies you would use to solve them.</li>
<li><strong>Highlight practical experience:</strong> If you have experience deploying prompt-based systems in production, share concrete examples of the challenges you faced and how you overcame them.</li>
<li><strong>Engage the interviewer:</strong> Encourage questions and feedback. This shows that you are confident in your knowledge and willing to engage in a discussion.</li>
<li><strong>Be enthusiastic:</strong> Show your passion for the topic and your excitement about the potential of prompt-based systems.</li>
</ul>
<p>When describing mathmatical elements, explain each variable and the relationship between them without getting bogged down in too much detail. For example, if explaining the quantization equation, don’t provide the theory behind quatization. It is more important to indicate how reducing the bits needed to represent each weight leads to reducing memory and computional requirments.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>