<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>prompt_engineering_and_in_context_learning_9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-10.-how-do-you-approach-the-problem-of-prompt-sensitivity-where-small-changes-in-wording-lead-to-large-changes-in-outputs-and-what-methods-can-be-used-to-stabilize-performance" class="level2">
<h2 class="anchored" data-anchor-id="question-10.-how-do-you-approach-the-problem-of-prompt-sensitivity-where-small-changes-in-wording-lead-to-large-changes-in-outputs-and-what-methods-can-be-used-to-stabilize-performance">Question: 10. How do you approach the problem of prompt sensitivity where small changes in wording lead to large changes in outputs, and what methods can be used to stabilize performance?</h2>
<p><strong>Best Answer</strong></p>
<p>Prompt sensitivity, the phenomenon where minor alterations in prompt wording drastically change the output of a language model (LLM), is a critical challenge in prompt engineering and in-context learning. It stems from the intricate mapping between natural language inputs and the model’s learned representations and decision boundaries. This sensitivity can lead to unpredictable or unreliable results, hindering the practical application of LLMs. I address this problem through a multi-faceted approach encompassing sensitivity analysis, prompt ensembling, robust prompt design, and, when appropriate, controlled natural language techniques, combined with rigorous testing.</p>
<p>Here’s a breakdown of my approach:</p>
<ol type="1">
<li><p><strong>Understanding the Root Causes:</strong> The sensitivity arises from the fact that language models are trained on vast datasets, learning complex statistical relationships between words and concepts. Even synonymous phrases may be represented differently in the model’s embedding space. This leads to variations in activation patterns and ultimately different outputs. Formally, we can consider the model’s output <span class="math inline">\(y\)</span> as a function of the prompt <span class="math inline">\(x\)</span> and the model parameters <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[y = f(x; \theta)\]</span></p>
<p>Small changes in <span class="math inline">\(x\)</span>, denoted as <span class="math inline">\(\Delta x\)</span>, can lead to significant changes in <span class="math inline">\(y\)</span>, <span class="math inline">\(\Delta y\)</span>, due to the non-linear nature of <span class="math inline">\(f\)</span> and the complex landscape defined by <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><strong>Sensitivity Analysis:</strong> A crucial first step is to quantify the extent of the prompt sensitivity. This involves systematically varying the prompt, generating multiple outputs, and analyzing the variance. Key steps include:</p>
<ul>
<li><p><strong>Defining Variation Space:</strong> Identify key words, phrases, and structural elements in the prompt that are likely to influence the output. Create a set of alternative wordings or structures for each.</p></li>
<li><p><strong>Generating Outputs:</strong> For each variation, run the prompt through the LLM and record the output.</p></li>
<li><p><strong>Measuring Variance:</strong> Use metrics relevant to the task (e.g., BLEU score for translation, ROUGE score for summarization, accuracy for classification, or even custom metrics) to quantify the similarity or difference between the outputs. The variance of these metrics across the prompt variations provides a measure of sensitivity. A high variance indicates significant sensitivity. We might calculate the standard deviation <span class="math inline">\(\sigma_y\)</span> of the output metric <span class="math inline">\(y\)</span> across different prompts <span class="math inline">\(x_i\)</span>:</p>
<p><span class="math display">\[\sigma_y = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \bar{y})^2}\]</span></p>
<p>where <span class="math inline">\(y_i = f(x_i; \theta)\)</span> and <span class="math inline">\(\bar{y}\)</span> is the mean output.</p></li>
<li><p><strong>Visualization:</strong> Plot the outputs or the variance metrics to visually identify sensitive prompt components.</p></li>
</ul></li>
<li><p><strong>Prompt Ensembling:</strong> This technique leverages multiple prompt variations to produce a more stable and reliable output. The core idea is to reduce the impact of any single, potentially sensitive prompt by aggregating the outputs from several related prompts.</p>
<ul>
<li><p><strong>Generate a diverse set of prompts:</strong> Create several variations of the original prompt using different phrasing, synonyms, and sentence structures. Aim for semantic equivalence but surface-level diversity.</p></li>
<li><p><strong>Obtain predictions for each prompt:</strong> Run each prompt variation through the LLM. Let <span class="math inline">\(y_i = f(x_i; \theta)\)</span> be the output for prompt <span class="math inline">\(x_i\)</span>.</p></li>
<li><p><strong>Aggregate the predictions:</strong> Combine the predictions from each prompt using a suitable aggregation method. Common methods include:</p>
<ul>
<li><strong>Averaging:</strong> For numerical outputs, simply average the predictions: <span class="math inline">\(\hat{y} = \frac{1}{N} \sum_{i=1}^{N} y_i\)</span>.</li>
<li><strong>Voting:</strong> For classification tasks, use majority voting to determine the final class.</li>
<li><strong>Weighted Averaging:</strong> Assign weights to each prompt based on its perceived reliability or performance on a validation set. Let <span class="math inline">\(w_i\)</span> be the weight for prompt <span class="math inline">\(x_i\)</span>. Then, <span class="math inline">\(\hat{y} = \sum_{i=1}^{N} w_i y_i\)</span>, where <span class="math inline">\(\sum_{i=1}^{N} w_i = 1\)</span>. Weighting schemes could be based on prompt complexity, validation accuracy or other factors.</li>
<li><strong>Ensemble Decoding:</strong> For text generation, use techniques like beam search with diverse beam groups, where each group is initialized with the output from a different prompt.</li>
</ul></li>
</ul>
<p>Prompt ensembling effectively smooths out the response surface, reducing the impact of individual prompt sensitivities.</p></li>
<li><p><strong>Robust Prompt Design:</strong> This involves crafting prompts that are less susceptible to variations in wording. Strategies include:</p>
<ul>
<li><strong>Use clear and unambiguous language:</strong> Avoid jargon, idioms, and overly complex sentence structures. Be explicit about the desired output format and any constraints.</li>
<li><strong>Provide sufficient context:</strong> The more context you provide, the less the model has to rely on subtle cues in the prompt wording. Include relevant background information, examples, and constraints. This reduces ambiguity and guides the model towards the intended interpretation.</li>
<li><strong>Experiment with different prompt structures:</strong> Try different prompt templates (e.g., question-answering, instruction-following, role-playing) to see which one produces the most stable results. For example, framing a task as “Answer the following question…” might be more robust than a free-form request.</li>
<li><strong>Incorporate paraphrasing instructions:</strong> Explicitly instruct the model to paraphrase the input before processing it. This can help to normalize the input and reduce the impact of minor wording variations. For example, “First, paraphrase the following text to ensure clarity and remove ambiguity. Then, summarize the main points.”</li>
<li><strong>Few-shot learning:</strong> Include multiple examples of input-output pairs in the prompt. This provides the model with a clearer understanding of the desired behavior and reduces its reliance on subtle cues in the wording.</li>
</ul></li>
<li><p><strong>Controlled Natural Language (CNL):</strong> In situations where the task domain is well-defined and precision is paramount, consider using a controlled natural language. CNL is a subset of natural language with a restricted vocabulary, grammar, and semantics. This reduces ambiguity and ensures that the model interprets the prompt in a predictable way. However, CNL requires more effort to develop and use, and it may not be suitable for all tasks. This approach can involve a specific grammar which can be given as:</p>
<p><span class="math display">\[G = (N, T, P, S)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N\)</span> is a finite set of non-terminal symbols.</li>
<li><span class="math inline">\(T\)</span> is a finite set of terminal symbols (vocabulary).</li>
<li><span class="math inline">\(P\)</span> is a finite set of production rules, <span class="math inline">\(P: (N \cup T)^*N(N \cup T)^* \rightarrow (N \cup T)^*\)</span>.</li>
<li><span class="math inline">\(S\)</span> is the start symbol (<span class="math inline">\(S \in N\)</span>).</li>
</ul></li>
<li><p><strong>Fine-tuning for Robustness:</strong> For particularly sensitive tasks, consider fine-tuning the LLM on a dataset of prompt variations and corresponding desired outputs. This can make the model more robust to changes in wording. The objective of fine-tuning is to minimize a loss function <span class="math inline">\(L\)</span> over the fine-tuning dataset:</p>
<p><span class="math display">\[\min_{\theta} \sum_{(x_i, y_i) \in D} L(f(x_i; \theta), y_i)\]</span></p>
<p>where <span class="math inline">\(D\)</span> is the fine-tuning dataset consisting of prompt variations <span class="math inline">\(x_i\)</span> and their corresponding target outputs <span class="math inline">\(y_i\)</span>.</p></li>
<li><p><strong>Diverse Testing and Robustness Checks:</strong> Regardless of the techniques used, thorough testing is crucial. This involves evaluating the model’s performance on a diverse set of prompts, including:</p>
<ul>
<li><strong>Synonym variations:</strong> Replace key words with synonyms.</li>
<li><strong>Structural variations:</strong> Change the sentence structure.</li>
<li><strong>Negations and hedges:</strong> Introduce negations (“not”, “never”) and hedges (“maybe”, “possibly”).</li>
<li><strong>Adversarial prompts:</strong> Craft prompts designed to mislead the model.</li>
</ul>
<p>Monitor the model’s performance across these variations and identify any remaining sensitivities.</p></li>
</ol>
<p>By combining these approaches, I can significantly mitigate the problem of prompt sensitivity and build more robust and reliable LLM-powered applications. The specific techniques I employ will depend on the nature of the task, the resources available, and the desired level of robustness.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><strong>Start by Acknowledging the Problem:</strong>
<ul>
<li>“Prompt sensitivity is a significant challenge in working with large language models. It refers to the tendency of these models to produce drastically different outputs in response to small changes in the wording of the prompt.”</li>
<li>“This sensitivity can make it difficult to rely on LLMs for tasks where consistency and predictability are important.”</li>
</ul></li>
<li><strong>Introduce Your Multi-Faceted Approach:</strong>
<ul>
<li>“I approach this problem by combining several techniques, including sensitivity analysis, prompt ensembling, robust prompt design, and potentially controlled natural language. I also emphasize thorough testing.”</li>
</ul></li>
<li><strong>Explain Sensitivity Analysis:</strong>
<ul>
<li>“First, I perform sensitivity analysis to understand <em>how</em> sensitive the model is to different variations in the prompt.”</li>
<li>“This involves systematically varying the prompt, generating multiple outputs, and measuring the variance using task-specific metrics.”</li>
<li><em>(If asked for more detail):</em> “For instance, if we’re doing translation, we might use BLEU score. If we’re seeing large variations in the BLEU score across slight prompt changes, that indicates high sensitivity.”</li>
</ul></li>
<li><strong>Discuss Prompt Ensembling:</strong>
<ul>
<li>“Prompt ensembling is a powerful technique to stabilize the performance. The idea is to use multiple slightly different prompts and then aggregate the outputs.”</li>
<li>“This helps to reduce the impact of any single, potentially sensitive prompt.”</li>
<li><em>(If asked for more detail):</em> “We can combine the outputs through averaging for numerical tasks, voting for classification, or even more sophisticated methods like weighted averaging or ensemble decoding for text generation.”</li>
</ul></li>
<li><strong>Explain Robust Prompt Design:</strong>
<ul>
<li>“Another important aspect is designing prompts that are inherently more robust to variations in wording. This involves using clear and unambiguous language, providing sufficient context, and experimenting with different prompt structures.”</li>
<li>“For example, framing a task as ‘Answer the following question…’ might be more robust than a free-form request. We may also instruct the LLM to first paraphrase the input.”</li>
</ul></li>
<li><strong>Mention Controlled Natural Language (If Applicable):</strong>
<ul>
<li>“In some cases, especially where precision is critical, controlled natural language can be an option. This involves using a restricted subset of natural language with a well-defined vocabulary and grammar, which reduces ambiguity.”</li>
<li>“However, CNL requires more effort to set up and might not be suitable for all tasks.”</li>
</ul></li>
<li><strong>Address Fine-tuning (If Applicable/Relevant):</strong>
<ul>
<li>“For tasks that are particularly sensitive, fine-tuning the LLM on a dataset of prompt variations and their desired outputs can improve robustness.”</li>
<li><em>(If asked for more detail):</em> “The goal is to teach the model to be less reliant on the precise wording of the prompt and more focused on the underlying intent.”</li>
</ul></li>
<li><strong>Emphasize Testing:</strong>
<ul>
<li>“Regardless of the techniques used, rigorous testing is absolutely essential. This means evaluating the model on a diverse set of prompts, including synonym variations, structural variations, and even adversarial prompts designed to mislead the model.”</li>
</ul></li>
<li><strong>Summarize and Conclude:</strong>
<ul>
<li>“By combining these approaches – sensitivity analysis, prompt ensembling, robust prompt design, and thorough testing – I can significantly mitigate the problem of prompt sensitivity and build more reliable LLM-powered applications.”</li>
<li>“The specific techniques I use will depend on the details of the project, but this multi-faceted approach provides a solid framework for addressing this challenge.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to process the information.</li>
<li><strong>Use Visual Aids (If Possible):</strong> If you’re in a virtual interview, consider sharing your screen and showing a diagram or example.</li>
<li><strong>Check for Understanding:</strong> Pause periodically and ask the interviewer if they have any questions.</li>
<li><strong>Don’t Overwhelm with Math:</strong> Present the mathematical notations as illustrations, not the core of your answer. Explain the concepts in plain language first, and then introduce the equations to provide a more formal representation. For example: “We can formally represent the model’s output like this, where <span class="math inline">\(y\)</span> is the output, <span class="math inline">\(x\)</span> is the prompt, and theta is the parameters: <span class="math inline">\(y = f(x; \theta)\)</span>.”</li>
<li><strong>Tailor to the Audience:</strong> Adjust the level of detail based on the interviewer’s background and apparent level of understanding. If they seem less familiar with LLMs, focus on the high-level concepts and avoid technical jargon. If they are very knowledgeable, you can delve into more detail.</li>
<li><strong>Be Prepared to Explain Further:</strong> The interviewer may ask you to elaborate on any of the techniques you mention. Be ready to provide more specific examples or explanations.</li>
<li><strong>Stay Confident:</strong> You have a deep understanding of the topic, so communicate that confidence through your tone and body language.</li>
</ul>
<p>By following these guidelines, you can effectively communicate your expertise in prompt engineering and demonstrate your ability to address the challenge of prompt sensitivity.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>