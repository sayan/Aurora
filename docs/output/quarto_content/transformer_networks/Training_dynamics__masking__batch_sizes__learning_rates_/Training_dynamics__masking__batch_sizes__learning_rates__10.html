<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>training_dynamics__masking__batch_sizes__learning_rates__10</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-11.-describe-a-scenario-where-you-observed-or-suspect-an-issue-with-the-training-dynamics-due-to-improper-masking.-how-would-you-debug-and-resolve-such-an-issue" class="level2">
<h2 class="anchored" data-anchor-id="question-11.-describe-a-scenario-where-you-observed-or-suspect-an-issue-with-the-training-dynamics-due-to-improper-masking.-how-would-you-debug-and-resolve-such-an-issue">Question: 11. Describe a scenario where you observed or suspect an issue with the training dynamics due to improper masking. How would you debug and resolve such an issue?</h2>
<p><strong>Best Answer</strong></p>
<p>Improper masking during neural network training can severely disrupt training dynamics, leading to slow convergence, instability, or even complete failure to learn. Masking is crucial in various scenarios such as handling variable-length sequences (e.g., in NLP), dealing with missing data, or implementing attention mechanisms.</p>
<p>Here’s a scenario where I encountered issues with masking in a sequence-to-sequence model and how I debugged and resolved it:</p>
<p><strong>Scenario: Neural Machine Translation (NMT) with Attention</strong></p>
<p>I was working on a Neural Machine Translation (NMT) model using an encoder-decoder architecture with attention. The input sequences (source language) had varying lengths. To efficiently process these sequences in batches, I padded shorter sequences to the length of the longest sequence in the batch. A mask was then used to ignore these padded tokens during training and inference.</p>
<p><strong>The Problem:</strong></p>
<p>The model exhibited significantly worse performance than expected, even after extensive hyperparameter tuning. The training loss decreased very slowly, and the generated translations were often nonsensical or repetitive. I suspected that the masking mechanism was the culprit.</p>
<p><strong>Debugging and Resolution:</strong></p>
<p>Here’s a systematic approach I took to debug and resolve the masking issue:</p>
<ol type="1">
<li><p><strong>Verify Mask Generation Logic:</strong></p>
<ul>
<li><strong>Code Inspection:</strong> The first step was a thorough review of the code responsible for generating the masks. This involved checking the logic that determines which tokens should be masked. I specifically looked for off-by-one errors or incorrect conditions that might lead to some valid tokens being masked or padded tokens being included.</li>
<li><strong>Unit Tests:</strong> I wrote unit tests specifically for the mask generation function. These tests covered various edge cases, such as:
<ul>
<li>Empty sequences</li>
<li>Sequences with length 1</li>
<li>Sequences that are already at the maximum length (no padding needed)</li>
<li>Sequences where padding is significant</li>
</ul></li>
<li><strong>Visualization:</strong> I printed and visualized the masks alongside the input sequences to visually confirm that the masking was applied correctly. This was especially helpful to identify patterns in where the mask might be failing. For instance, I would print the input tensor and the corresponding mask tensor using <code>print(input_tensor.shape)</code>, <code>print(mask_tensor.shape)</code>, <code>print(input_tensor)</code>, <code>print(mask_tensor)</code>.</li>
<li>Mathematically, the mask should represent a binary tensor where: <span class="math display">\[
mask[i, j] =
\begin{cases}
  1 &amp; \text{if the j-th token in the i-th sequence is valid} \\
  0 &amp; \text{if the j-th token in the i-th sequence is padding}
\end{cases}
\]</span></li>
</ul></li>
<li><p><strong>Check Tensor Shapes and Broadcasting:</strong></p>
<ul>
<li><p><strong>Shape Mismatches:</strong> Masks need to have compatible shapes with the tensors they are applied to. In my case, I needed to ensure that the mask had the same shape as the input embeddings or the attention weights. Broadcasting issues can also cause subtle errors where the mask is not applied as intended. For example, if the input is <code>(batch_size, seq_len, embedding_dim)</code> and the mask is <code>(batch_size, seq_len)</code>, the mask might need to be reshaped to <code>(batch_size, seq_len, 1)</code> for proper broadcasting during element-wise multiplication.</p></li>
<li><p>Debugging code example to check the shape: ```python # Check shape of input and mask print(“Input shape:”, input_tensor.shape) print(“Mask shape:”, mask_tensor.shape)</p>
<p># Verify that mask can be broadcasted try: masked_input = input_tensor * mask_tensor except RuntimeError as e: print(“Broadcasting error:”, e) ```</p></li>
</ul></li>
<li><p><strong>Inspect Loss Propagation:</strong></p>
<ul>
<li><strong>Loss Function:</strong> Ensuring the loss function correctly incorporates the mask is crucial. In my case, I was using <code>torch.nn.CrossEntropyLoss</code> with the <code>ignore_index</code> parameter to ignore the padded tokens when calculating the loss. I verified that the <code>ignore_index</code> was set to the correct padding token ID.</li>
<li><strong>Gradient Analysis:</strong> I inspected the gradients to see if they were being propagated correctly through the masked regions. Ideally, the gradients in the masked regions should be close to zero. Tools like <code>torch.autograd.grad</code> can be used to examine the gradients w.r.t. the input.</li>
<li><strong>Example:</strong> If your sequences are represented as <span class="math inline">\(X = \{x_1, x_2, ..., x_T\}\)</span>, the loss function <span class="math inline">\(L\)</span> can be expressed as: <span class="math display">\[
  L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} mask_{i,t} \cdot log(P(y_{i,t}|x_{i,1}, ..., x_{i,t}))
  \]</span> where <span class="math inline">\(N\)</span> is the number of sequences in the batch, <span class="math inline">\(T\)</span> is the maximum sequence length, <span class="math inline">\(mask_{i,t}\)</span> is the mask value for the t-th token in the i-th sequence, and <span class="math inline">\(P(y_{i,t}|x_{i,1}, ..., x_{i,t})\)</span> is the probability of the target token given the input sequence.</li>
</ul></li>
<li><p><strong>Inspect Model Outputs for Edge Cases:</strong></p>
<ul>
<li><strong>Qualitative Analysis:</strong> I examined the model’s outputs for specific edge cases:
<ul>
<li>Short sequences: Did the model correctly translate very short input sequences?</li>
<li>Sequences with large amounts of padding: Did the model handle heavily padded sequences appropriately?</li>
<li>Sequences containing the padding token within the non-padded region: This could indicate an issue where the padding token was not being correctly identified.</li>
</ul></li>
<li><strong>Quantitative Analysis:</strong> I calculated metrics such as BLEU score separately for short and long sentences to see if there was a significant performance difference. A large discrepancy could point to masking problems in longer, padded sequences.</li>
</ul></li>
<li><p><strong>Attention Mechanism Debugging (Specific to the Scenario):</strong></p>
<p>Since I was using an attention mechanism, I paid special attention to how the mask was being applied in the attention calculations. The attention weights should ideally be zero for padded tokens, preventing them from influencing the context vector.</p>
<ul>
<li><strong>Attention Visualization:</strong> I visualized the attention weights to confirm that the model was not attending to the padded tokens. Heatmaps of the attention weights can be very informative. If I saw the model attending to padded positions, it indicated that the mask was not being correctly applied in the attention mechanism.</li>
<li><strong>Mathematical Representation:</strong> Let <span class="math inline">\(a_{ij}\)</span> be the attention weight between the <span class="math inline">\(i\)</span>-th encoder hidden state <span class="math inline">\(h_i\)</span> and the <span class="math inline">\(j\)</span>-th decoder hidden state <span class="math inline">\(s_j\)</span>. With masking, the attention weights are modified as follows: <span class="math display">\[
\tilde{a}_{ij} = a_{ij} \cdot mask_i
\]</span> where <span class="math inline">\(mask_i\)</span> is the mask for the <span class="math inline">\(i\)</span>-th encoder position. This ensures that the padded positions do not contribute to the context vector.</li>
</ul></li>
<li><p><strong>Experimentation:</strong></p>
<ul>
<li><strong>Simplified Model:</strong> I created a simplified version of the model with a smaller vocabulary and fewer layers to make debugging easier. This allowed me to isolate the masking issue from other potential problems in the model architecture.</li>
<li><strong>Different Masking Strategies:</strong> I experimented with different ways of applying the mask, such as:
<ul>
<li>Element-wise multiplication with the attention weights</li>
<li>Adding a large negative value to the attention weights before applying softmax (this effectively forces the attention weights for padded tokens to be zero after the softmax)</li>
</ul></li>
<li><strong>Masking at Different Layers:</strong> I tested applying the mask at different layers of the model (e.g., before the attention mechanism, after the attention mechanism).</li>
</ul></li>
</ol>
<p><strong>The Solution:</strong></p>
<p>In my case, the issue was a subtle broadcasting error in the attention mechanism. The mask was not being correctly broadcasted when calculating the attention weights, causing the model to attend to padded tokens. Reshaping the mask tensor to have the correct dimensions resolved the problem. After fixing the masking issue, the model’s performance improved dramatically, and it was able to generate much more accurate translations.</p>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li><strong>Masking is Critical:</strong> Proper masking is essential when dealing with variable-length sequences, missing data, or attention mechanisms.</li>
<li><strong>Systematic Debugging:</strong> A systematic approach to debugging masking issues is crucial. This includes verifying the mask generation logic, checking tensor shapes, inspecting loss propagation, and analyzing model outputs for edge cases.</li>
<li><strong>Visualization:</strong> Visualizing the masks, attention weights, and model outputs can provide valuable insights into masking-related problems.</li>
<li><strong>Unit Testing:</strong> Writing unit tests for the mask generation function can help catch subtle errors.</li>
<li><strong>Attention to Detail:</strong> Masking issues can be subtle and require careful attention to detail.</li>
<li><strong>Use debugger tools:</strong> Use debugger tools such as <code>pdb</code> to check values and shapes of your tensors during runtime.</li>
</ul>
<p>By following these steps, I was able to identify and resolve the masking issue in my NMT model, leading to a significant improvement in performance. The debugging process emphasized the importance of meticulous code review, targeted testing, and a deep understanding of the model’s architecture and data flow.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s how I would narrate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with the importance of masking:</strong> “Masking is a critical technique in many deep learning tasks, especially when dealing with variable-length sequences, missing data, or complex attention mechanisms. However, improper masking can severely hinder training.”</p></li>
<li><p><strong>Introduce the scenario:</strong> “Let me share an experience I had while working on a Neural Machine Translation (NMT) project. We used an encoder-decoder architecture with attention, and the input sequences had varying lengths, requiring padding and masking.”</p></li>
<li><p><strong>Describe the problem:</strong> “Initially, the model performed poorly, with slow loss reduction and nonsensical translations. I suspected that the masking mechanism was the culprit.”</p></li>
<li><p><strong>Explain the debugging process, focusing on the key steps:</strong></p>
<ul>
<li>“First, I meticulously reviewed the mask generation logic. I wrote unit tests to cover edge cases like empty sequences, sequences with maximum length, and so on. I’d also print the shapes and values of the mask tensors along with the corresponding input tensors to visually verify that the masking was correct.” Briefly show the equation if asked: “<span class="math inline">\(mask[i, j] = 1\)</span> if the j-th token in the i-th sequence is valid, <span class="math inline">\(0\)</span> otherwise.”</li>
<li>“Next, I checked for shape mismatches and broadcasting errors. The mask needs to have compatible dimensions with the tensors it’s applied to. Broadcasting issues can be tricky to spot.”</li>
<li>“Then, I inspected loss propagation. I made sure the loss function correctly ignored padded tokens and analyzed gradients to see if they were being propagated correctly through masked regions.” If asked, mention the loss function: “Something like: <span class="math inline">\(L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} mask_{i,t} \cdot log(P(y_{i,t}|x_{i,1}, ..., x_{i,t}))\)</span>”</li>
<li>“I also inspected model outputs for edge cases like very short sequences or heavily padded sequences to see how the masking was affecting them.”</li>
<li>“Because we were using attention, I paid special attention to how the mask was applied during attention calculations. I visualized the attention weights to ensure that the model wasn’t attending to padded tokens. Ideally you want the attention weight formula to be: <span class="math inline">\(\tilde{a}_{ij} = a_{ij} \cdot mask_i\)</span>.”</li>
<li>“Finally, I conducted experiments, creating a simplified model and testing different masking strategies to isolate the problem.”</li>
</ul></li>
<li><p><strong>Explain the solution and the impact:</strong> “In my case, it turned out to be a subtle broadcasting error in the attention mechanism. The mask wasn’t being correctly broadcasted, causing the model to attend to padded tokens. Correcting the tensor shapes resolved the issue, leading to a dramatic improvement in translation accuracy.”</p></li>
<li><p><strong>Summarize key takeaways:</strong> “This experience highlighted the importance of thorough testing, systematic debugging, and a deep understanding of the model architecture when dealing with masking. It also reinforced the value of visualizing intermediate results to identify subtle errors.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace:</strong> Don’t rush. Explain each step clearly and concisely.</li>
<li><strong>Engagement:</strong> Pause occasionally and ask the interviewer if they have any questions or want you to elaborate on a specific point.</li>
<li><strong>Math:</strong> When presenting equations, provide context and explain the symbols. Don’t just throw equations at them. Offer to elaborate if they’re interested in a deeper dive. If they don’t seem interested, move on.</li>
<li><strong>Confidence:</strong> Speak confidently, demonstrating that you have a solid understanding of the concepts and the debugging process.</li>
<li><strong>Real-World Focus:</strong> Frame your answer in terms of a real-world problem and how you solved it. This makes your response more relatable and demonstrates your practical skills.</li>
<li><strong>Storytelling:</strong> Structure your answer as a story with a clear beginning (problem), middle (debugging process), and end (solution). This will make your answer more engaging and memorable.</li>
<li><strong>Listen to interviewer cues:</strong> If the interviewer looks confused or asks clarifying questions, adjust your explanation accordingly.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>