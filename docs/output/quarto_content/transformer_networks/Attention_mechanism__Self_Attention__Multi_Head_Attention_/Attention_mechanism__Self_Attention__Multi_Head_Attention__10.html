<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>attention_mechanism__self_attention__multi_head_attention__10</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-11.-how-would-you-optimize-a-transformer-model-utilizing-attention-mechanisms-for-real-time-applications-where-low-latency-is-critical" class="level2">
<h2 class="anchored" data-anchor-id="question-11.-how-would-you-optimize-a-transformer-model-utilizing-attention-mechanisms-for-real-time-applications-where-low-latency-is-critical">Question: 11. How would you optimize a transformer model utilizing attention mechanisms for real-time applications where low latency is critical?</h2>
<p><strong>Best Answer</strong></p>
<p>Optimizing a Transformer model for real-time applications with stringent latency requirements involves a multi-faceted approach, focusing on both model-level and system-level optimizations. The key is to reduce computational complexity while maintaining acceptable accuracy. Here’s a detailed breakdown:</p>
<p><strong>1. Model Pruning:</strong></p>
<ul>
<li><strong>Concept:</strong> Model pruning aims to reduce the model’s size by removing redundant or less important weights. This directly decreases the number of computations required for inference.</li>
<li><strong>Techniques:</strong>
<ul>
<li><em>Weight Pruning:</em> Individual weights with low magnitudes are set to zero. This leads to a sparse weight matrix.</li>
<li><em>Neuron Pruning:</em> Entire neurons (along with their connections) are removed based on metrics like activation importance or gradient magnitude. This leads to a smaller model.</li>
</ul></li>
<li><strong>Mathematical Representation:</strong> Let <span class="math inline">\(W\)</span> be a weight matrix in the Transformer. Pruning involves creating a mask <span class="math inline">\(M\)</span> such that <span class="math inline">\(M_{ij} = 0\)</span> if the weight <span class="math inline">\(W_{ij}\)</span> is pruned and <span class="math inline">\(M_{ij} = 1\)</span> otherwise. The pruned weight matrix <span class="math inline">\(W'\)</span> is then given by: <span class="math display">\[W' = W \odot M\]</span> where <span class="math inline">\(\odot\)</span> represents element-wise multiplication.</li>
<li><strong>Importance:</strong> Reduces the number of parameters, therefore reducing memory footprint and computational cost.</li>
</ul>
<p><strong>2. Quantization:</strong></p>
<ul>
<li><p><strong>Concept:</strong> Quantization reduces the precision of the model’s weights and activations, typically from 32-bit floating-point numbers (float32) to 8-bit integers (int8) or even lower.</p></li>
<li><p><strong>Techniques:</strong></p>
<ul>
<li><em>Post-Training Quantization:</em> The model is quantized after it has been fully trained. This is simpler to implement but may lead to some accuracy loss.</li>
<li><em>Quantization-Aware Training:</em> The model is trained with quantization in mind, simulating the quantization effects during training. This can recover much of the accuracy lost due to quantization.</li>
</ul></li>
<li><p><strong>Mathematical Representation:</strong> Quantization can be represented as a mapping <span class="math inline">\(Q: \mathbb{R} \rightarrow \mathbb{Z}\)</span>. A simplified uniform quantization can be written as:</p>
<p><span class="math display">\[q = round(\frac{r}{S} + Z)\]</span> where <span class="math inline">\(r\)</span> is the real value, <span class="math inline">\(S\)</span> is the scale factor, <span class="math inline">\(Z\)</span> is the zero-point, and <span class="math inline">\(q\)</span> is the quantized value. De-quantization is then: <span class="math display">\[\hat{r} = S(q - Z)\]</span> where <span class="math inline">\(\hat{r}\)</span> is the de-quantized value (approximation of r).</p></li>
<li><p><strong>Importance:</strong> Reduces memory usage and can significantly speed up computation on hardware that is optimized for integer arithmetic.</p></li>
</ul>
<p><strong>3. Efficient Attention Approximations:</strong></p>
<ul>
<li><p><strong>Concept:</strong> The standard self-attention mechanism in Transformers has a computational complexity of <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length. This can become a bottleneck for long sequences. Efficient attention mechanisms aim to reduce this complexity.</p></li>
<li><p><strong>Techniques:</strong></p>
<ul>
<li><em>Sparse Attention:</em> Only attend to a subset of the input sequence, instead of attending to all positions. Examples include:
<ul>
<li><em>Fixed Patterns:</em> Attend to fixed patterns of locations.</li>
<li><em>Learnable Patterns:</em> Learn which locations to attend to.</li>
</ul></li>
<li><em>Linear Attention:</em> Approximates the attention mechanism to achieve linear complexity <span class="math inline">\(O(n)\)</span>. Examples include:
<ul>
<li><em>Linformer:</em> Projects the key and value matrices to a lower-dimensional space before computing the attention.</li>
<li><em>Performer:</em> Uses random feature maps to approximate the attention kernel.</li>
</ul></li>
<li><em>Longformer:</em> Combines a sliding window approach with global attention to handle longer sequences.</li>
</ul></li>
<li><p><strong>Mathematical Representation:</strong> In standard attention, we have:</p>
<p><span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
<p>where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> are the query, key, and value matrices, respectively, and <span class="math inline">\(d_k\)</span> is the dimension of the key vectors. In linear attention methods (e.g., using kernel functions):</p>
<p><span class="math display">\[Attention(Q, K, V) \approx normalize(\phi(Q)\phi(K)^T)V\]</span></p>
<p>where <span class="math inline">\(\phi(\cdot)\)</span> is a feature map that allows for linear-time computation.</p></li>
<li><p><strong>Importance:</strong> Significantly reduces the computational cost of the attention mechanism, enabling faster inference, especially for long sequences.</p></li>
</ul>
<p><strong>4. Knowledge Distillation:</strong></p>
<ul>
<li><p><strong>Concept:</strong> Train a smaller, faster “student” model to mimic the behavior of a larger, more accurate “teacher” model.</p></li>
<li><p><strong>Techniques:</strong> The student model is trained to predict not only the correct labels but also the “soft” probabilities predicted by the teacher model. The “soft” probabilities contain more information than the hard labels, which helps the student model learn more effectively.</p></li>
<li><p><strong>Mathematical Representation:</strong> The loss function for knowledge distillation typically includes two terms:</p>
<p><span class="math display">\[L = \alpha L_{CE}(y, p_s) + (1 - \alpha) L_{KL}(p_t, p_s)\]</span></p>
<p>where <span class="math inline">\(L_{CE}\)</span> is the cross-entropy loss between the true labels <span class="math inline">\(y\)</span> and the student’s predictions <span class="math inline">\(p_s\)</span>, <span class="math inline">\(L_{KL}\)</span> is the Kullback-Leibler divergence between the teacher’s predictions <span class="math inline">\(p_t\)</span> and the student’s predictions <span class="math inline">\(p_s\)</span>, and <span class="math inline">\(\alpha\)</span> is a weighting factor.</p></li>
<li><p><strong>Importance:</strong> Allows for compressing the knowledge from a large model into a smaller one, achieving a better trade-off between accuracy and latency.</p></li>
</ul>
<p><strong>5. Hardware Acceleration:</strong></p>
<ul>
<li><strong>Concept:</strong> Leverage specialized hardware to accelerate the computations involved in the Transformer model.</li>
<li><strong>Techniques:</strong>
<ul>
<li><em>GPUs (Graphics Processing Units):</em> GPUs are well-suited for parallel computations and can significantly speed up matrix multiplications, which are a core operation in Transformers.</li>
<li><em>TPUs (Tensor Processing Units):</em> TPUs are custom-designed hardware accelerators specifically for machine learning workloads. They offer even greater performance than GPUs for certain tasks.</li>
<li><em>FPGAs (Field-Programmable Gate Arrays):</em> FPGAs can be customized to implement specific operations in hardware, offering the potential for very high performance.</li>
<li><em>Optimized Libraries:</em> Use optimized libraries (e.g., cuBLAS, cuDNN for NVIDIA GPUs) to leverage hardware-specific optimizations.</li>
</ul></li>
<li><strong>Importance:</strong> Provides the most significant speedups, especially when combined with model-level optimizations.</li>
</ul>
<p><strong>6. Parallel Processing &amp; Batching:</strong></p>
<ul>
<li><strong>Concept:</strong> Parallelize computations across multiple cores or devices, and process multiple input sequences in batches to improve throughput.</li>
<li><strong>Techniques:</strong>
<ul>
<li><em>Data Parallelism:</em> Distribute the data across multiple devices and train the model in parallel.</li>
<li><em>Model Parallelism:</em> Distribute the model across multiple devices, with each device responsible for a portion of the model’s computation.</li>
<li><em>Batching:</em> Process multiple input sequences in a single batch, which can improve the utilization of the hardware.</li>
</ul></li>
<li><strong>Importance:</strong> Improves throughput and reduces latency by leveraging parallel processing capabilities. However, larger batch sizes can sometimes increase latency for individual requests.</li>
</ul>
<p><strong>7. Operator Fusion:</strong></p>
<ul>
<li><strong>Concept:</strong> Combine multiple operations into a single kernel to reduce memory access and kernel launch overhead.</li>
<li><strong>Techniques:</strong> Merge operations like layer normalization, activation functions, and matrix multiplications into a single fused kernel.</li>
<li><strong>Importance:</strong> Reduces kernel launch overhead and memory access, leading to improved performance.</li>
</ul>
<p><strong>8. Dynamic Batching:</strong></p>
<ul>
<li><strong>Concept:</strong> Adjust the batch size dynamically based on the current workload to optimize for both throughput and latency.</li>
<li><strong>Techniques:</strong> Increase the batch size when the workload is low to improve throughput, and decrease the batch size when the workload is high to reduce latency.</li>
<li><strong>Importance:</strong> Provides a balance between throughput and latency, adapting to the changing workload conditions.</li>
</ul>
<p><strong>9. Trade-offs:</strong></p>
<p>It’s crucial to understand the trade-offs between accuracy and latency. Aggressively optimizing for latency can lead to a reduction in accuracy. The optimal balance will depend on the specific application and its requirements. Regular evaluation and monitoring are necessary to ensure that the model meets both the latency and accuracy goals.</p>
<p>In summary, optimizing Transformer models for real-time applications requires a combination of model-level optimizations (pruning, quantization, efficient attention, distillation) and system-level optimizations (hardware acceleration, parallel processing, operator fusion). Careful consideration of the trade-offs between accuracy and latency is essential for achieving the desired performance.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested way to present this information in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Big Picture:</strong> “To optimize a Transformer for real-time low-latency applications, I’d focus on both reducing computational complexity within the model itself and leveraging system-level optimizations. The key is to find the right balance between speed and accuracy, as aggressive optimization can sometimes hurt performance.”</p></li>
<li><p><strong>Introduce Model Pruning:</strong> “One important approach is model pruning. This involves removing redundant connections or entire neurons from the model. Mathematically, it’s like applying a mask to the weight matrices: <span class="math inline">\(&lt;W' = W \odot M&gt;\)</span>. This reduces the model size and computation.”</p></li>
<li><p><strong>Discuss Quantization:</strong> “Next, I’d consider quantization, which reduces the precision of the model’s weights and activations. For example, we might move from float32 to int8. This significantly cuts down on memory usage and can speed up computations on specialized hardware. The quantization process can be thought of as this: <span class="math inline">\(q = round(\frac{r}{S} + Z)\)</span> where we move from a real number <span class="math inline">\(r\)</span> to the integer <span class="math inline">\(q\)</span>. We can then recover the real number as <span class="math inline">\(\hat{r} = S(q - Z)\)</span>.”</p></li>
<li><p><strong>Explain Efficient Attention:</strong> “A major bottleneck in Transformers is the self-attention mechanism, with a complexity of O(n^2). Efficient attention approximations are crucial. Techniques like sparse attention and linear attention reduce this complexity. For example, linear attention approximates: <span class="math display">\[Attention(Q, K, V) \approx normalize(\phi(Q)\phi(K)^T)V\]</span> where <span class="math inline">\(\phi(\cdot)\)</span> is a feature map that allows for linear-time computation.” (Don’t delve too deeply into the math here unless specifically asked; just highlight the key idea of reducing complexity).</p></li>
<li><p><strong>Knowledge Distillation:</strong> “We can also use knowledge distillation, where we train a smaller ‘student’ model to mimic a larger, more accurate ‘teacher’ model. The student learns to reproduce the teacher’s outputs.”</p></li>
<li><p><strong>Highlight Hardware Acceleration:</strong> “Leveraging hardware acceleration with GPUs or TPUs is crucial for real-time performance. These devices are optimized for the matrix multiplications that form the core of Transformer computations.”</p></li>
<li><p><strong>Mention Other Techniques:</strong> “Other techniques include parallel processing and operator fusion, which further optimize the model’s performance at the system level.”</p></li>
<li><p><strong>Address Trade-offs:</strong> “It’s important to remember that there’s a trade-off between accuracy and latency. We need to carefully evaluate and monitor the model to ensure it meets both performance goals.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to absorb the information.</li>
<li><strong>Use Visual Aids (If Possible):</strong> If you’re in a remote interview, consider sharing a simple diagram or equation to illustrate key concepts.</li>
<li><strong>Check for Understanding:</strong> Periodically ask the interviewer if they have any questions or if they’d like you to elaborate on a particular point.</li>
<li><strong>Be Prepared to Dive Deeper:</strong> The interviewer may ask you to go into more detail about a specific technique. Be ready to provide more technical information if needed.</li>
<li><strong>Stay Practical:</strong> Always connect the techniques back to the real-world application and the goal of reducing latency.</li>
<li><strong>Modulate Detail:</strong> If the interviewer seems unfamiliar with some of the more advanced concepts, avoid overwhelming them. Focus on the high-level ideas and avoid getting bogged down in technical details. If they are well-versed, you can dig deeper.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>