<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>attention_mechanism__self_attention__multi_head_attention__7</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-8.-can-you-provide-an-example-of-how-attention-mechanisms-have-been-adapted-for-computer-vision-tasks-what-modifications-are-needed-compared-to-nlp-applications" class="level2">
<h2 class="anchored" data-anchor-id="question-8.-can-you-provide-an-example-of-how-attention-mechanisms-have-been-adapted-for-computer-vision-tasks-what-modifications-are-needed-compared-to-nlp-applications">Question: 8. Can you provide an example of how attention mechanisms have been adapted for computer vision tasks? What modifications are needed compared to NLP applications?</h2>
<p><strong>Best Answer</strong></p>
<p>Attention mechanisms, initially prominent in Natural Language Processing (NLP), have found significant success in computer vision. A key adaptation is the <strong>Vision Transformer (ViT)</strong>, which demonstrates how self-attention can be effectively applied to image recognition.</p>
<p>Here’s a breakdown:</p>
<ol type="1">
<li><p><strong>From Images to Tokens (Patches):</strong></p>
<ul>
<li><p>In NLP, the input consists of sequences of words (tokens). To adapt attention to vision, an image is divided into a grid of fixed-size patches. Each patch is then linearly embedded to form a “visual token.”</p></li>
<li><p>Mathematically, let an image <span class="math inline">\(X \in \mathbb{R}^{H \times W \times C}\)</span>, where <span class="math inline">\(H\)</span> is the height, <span class="math inline">\(W\)</span> is the width, and <span class="math inline">\(C\)</span> is the number of channels. We divide <span class="math inline">\(X\)</span> into <span class="math inline">\(N = \frac{H}{P} \times \frac{W}{P}\)</span> patches, where <span class="math inline">\(P\)</span> is the patch size. Each patch <span class="math inline">\(X_i \in \mathbb{R}^{P \times P \times C}\)</span> is then flattened and linearly projected to a <span class="math inline">\(D\)</span>-dimensional embedding space:</p>
<p><span class="math display">\[
z_i = E x_i, \quad \text{where } E \in \mathbb{R}^{(P^2C) \times D}
\]</span></p>
<p>Here, <span class="math inline">\(x_i\)</span> is the flattened patch <span class="math inline">\(X_i\)</span> and <span class="math inline">\(z_i\)</span> is the corresponding token embedding. These <span class="math inline">\(z_i\)</span> become the input to the Transformer encoder.</p></li>
</ul></li>
<li><p><strong>Positional Embeddings:</strong></p>
<ul>
<li><p>Since the Transformer architecture is permutation-invariant, positional embeddings are added to the patch embeddings to retain spatial information. These can be learned or fixed (e.g., sinusoidal).</p></li>
<li><p>The final input to the Transformer encoder is thus:</p>
<p><span class="math display">\[
z_0 = [z_1, z_2, ..., z_N] + E_{pos}, \quad E_{pos} \in \mathbb{R}^{N \times D}
\]</span></p>
<p>Where <span class="math inline">\(E_{pos}\)</span> are the positional embeddings.</p></li>
</ul></li>
<li><p><strong>Transformer Encoder:</strong></p>
<ul>
<li><p>The core of ViT is the standard Transformer encoder, consisting of alternating layers of multi-headed self-attention (MSA) and multilayer perceptron (MLP) blocks.</p></li>
<li><p>The self-attention mechanism computes attention weights based on the relationships between different patches. Given a set of queries <span class="math inline">\(Q\)</span>, keys <span class="math inline">\(K\)</span>, and values <span class="math inline">\(V\)</span>, the attention weights are computed as:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(d_k\)</span> is the dimension of the keys. Multi-Head Attention (MHA) runs this in parallel <span class="math inline">\(h\)</span> times and concatenates the results:</p>
<p><span class="math display">\[
\text{MHA}(Q, K, V) = \text{Concat}(head_1, ..., head_h)W^O
\]</span> where <span class="math inline">\(head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span> and <span class="math inline">\(W^O\)</span> is a learned projection.</p></li>
</ul></li>
<li><p><strong>Modifications Compared to NLP:</strong></p>
<ul>
<li><p><strong>Input Representation:</strong> The primary difference lies in how the input is represented. In NLP, tokens are discrete words from a vocabulary. In ViT, tokens are embeddings of image patches, which are continuous representations.</p></li>
<li><p><strong>Positional Information:</strong> While positional embeddings are also used in NLP, their interpretation differs slightly. In vision, they explicitly encode the spatial arrangement of patches.</p></li>
<li><p><strong>Computational Cost:</strong> Self-attention has a quadratic complexity with respect to the number of tokens, <span class="math inline">\(O(N^2)\)</span>, where <span class="math inline">\(N\)</span> is the number of tokens (patches). This can be a bottleneck for high-resolution images. Therefore, techniques such as hierarchical attention or sparse attention are often employed to reduce computational costs.</p></li>
<li><p><strong>Hybrid Architectures:</strong> In practice, many successful vision models combine convolutional layers with attention mechanisms. Convolutional layers can efficiently extract low-level features, while attention mechanisms capture long-range dependencies. This helps to leverage the strengths of both approaches.</p></li>
</ul></li>
<li><p><strong>Why is it important</strong></p>
<ul>
<li>Attention allows networks to focus on the relevant parts of the image, this leads to improved efficiency and performance.</li>
<li>Attention models can capture global dependencies, unlike CNNs which are inherently local.</li>
</ul></li>
<li><p><strong>Techniques</strong></p>
<ul>
<li>Vision Transformer(ViT)</li>
<li>Swin Transformer</li>
<li>Convolutional Block Attention Module (CBAM)</li>
</ul></li>
</ol>
<p><strong>Real-World Considerations:</strong></p>
<ul>
<li><strong>Patch Size Selection:</strong> The choice of patch size impacts performance. Smaller patch sizes capture finer details but increase the computational cost. Larger patch sizes are more efficient but may miss important local features.</li>
<li><strong>Pre-training:</strong> ViTs often benefit from pre-training on large datasets (e.g., ImageNet) to learn general visual representations. Fine-tuning on specific downstream tasks then allows the model to adapt to the target domain.</li>
<li><strong>Hardware Requirements:</strong> Training ViTs can be computationally demanding, requiring significant GPU resources. Optimizations such as mixed-precision training and distributed training are often necessary.</li>
</ul>
<p>In summary, ViTs demonstrate how attention mechanisms can be successfully adapted for computer vision by treating image patches as tokens and leveraging the Transformer architecture. Modifications compared to NLP primarily involve adapting the input representation, handling positional information, and addressing the computational cost associated with high-resolution images. The combination of CNNs and transformers is also a common trend for achieving state-of-the-art results.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Big Picture:</strong></p>
<ul>
<li>“Attention mechanisms, initially successful in NLP, have been effectively adapted for computer vision. A prominent example is the Vision Transformer, or ViT.” (This sets the stage and provides context.)</li>
</ul></li>
<li><p><strong>Explain the Core Adaptation - Image Patches as Tokens:</strong></p>
<ul>
<li>“The key idea is to treat image patches as ‘visual tokens,’ similar to words in a sentence. We divide the image into a grid of patches, and then embed each patch into a vector representation.” (Explain the analogy to NLP tokens.)</li>
</ul></li>
<li><p><strong>Walk Through the Math (but keep it high-level):</strong></p>
<ul>
<li>“Mathematically, if we have an image X of size H x W x C, we split it into patches. Each patch is flattened and linearly projected using a matrix E. This results in a set of ‘token embeddings’ that represent the image.” (Avoid getting bogged down in minute details. Focus on the transformation.)</li>
<li>“We can define the equation <span class="math display">\[z_i = E x_i, \quad \text{where } E \in \mathbb{R}^{(P^2C) \times D}\]</span>.</li>
</ul></li>
<li><p><strong>Discuss Positional Embeddings:</strong></p>
<ul>
<li>“Because the Transformer architecture is permutation-invariant, we add positional embeddings to encode the spatial arrangement of the patches. This is crucial for the model to understand the structure of the image.” (Explain why positional embeddings are necessary.)</li>
<li>“We can add the positional embedding via the equation <span class="math display">\[z_0 = [z_1, z_2, ..., z_N] + E_{pos}, \quad E_{pos} \in \mathbb{R}^{N \times D}\]</span>”</li>
</ul></li>
<li><p><strong>Explain Transformer Encoder</strong></p>
<ul>
<li>“The embeddings are passed to the tranformer encoder module where the self-attention mechanism is the core. It computes attention weights based on the relationships between different patches using the equation <span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span>”</li>
<li>“Multi-Head Attention (MHA) runs this in parallel and concatenates the results, <span class="math display">\[ \text{MHA}(Q, K, V) = \text{Concat}(head_1, ..., head_h)W^O\]</span>”</li>
</ul></li>
<li><p><strong>Highlight the Differences from NLP:</strong></p>
<ul>
<li>“The main differences from NLP are in the input representation. In NLP, we have discrete word tokens. In ViT, we have continuous embeddings of image patches. Positional information is also crucial to capture the spatial arrangement of the patches.” (Focus on the key distinctions.)</li>
</ul></li>
<li><p><strong>Address Computational Cost &amp; Hybrid Architectures:</strong></p>
<ul>
<li>“Self-attention has quadratic complexity, which can be a bottleneck for high-resolution images. To mitigate this, techniques like hierarchical attention or sparse attention are used. Also, it’s common to combine convolutional layers with attention mechanisms to leverage the strengths of both.” (Show awareness of real-world challenges and solutions.)</li>
</ul></li>
<li><p><strong>Discuss Practical Considerations (Optional, depending on time):</strong></p>
<ul>
<li>“The choice of patch size, pre-training strategies, and hardware requirements are important considerations when implementing ViTs.” (If the interviewer seems interested in implementation details, briefly touch on these points.)</li>
</ul></li>
<li><p><strong>Conclude with a Summary:</strong></p>
<ul>
<li>“In summary, ViTs successfully adapt attention mechanisms for computer vision by treating image patches as tokens and using the Transformer architecture. While there are differences compared to NLP, the core principles of attention remain the same.” (Reinforce the key takeaway.)</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow time for the interviewer to process the information.</li>
<li><strong>Check for Understanding:</strong> After explaining a complex concept, ask, “Does that make sense?” or “Are there any questions about that?”</li>
<li><strong>Use Visual Aids (If Possible):</strong> If you’re in a virtual interview, consider sharing your screen and sketching a simple diagram to illustrate the patch embedding process.</li>
<li><strong>Be Flexible:</strong> If the interviewer interrupts with a specific question, address it directly and then return to your prepared explanation.</li>
<li><strong>Stay Enthusiastic:</strong> Show genuine interest in the topic. Your passion will be contagious.</li>
<li><strong>Be Honest About Limitations:</strong> If there’s something you don’t know, admit it. For example, “I’m not an expert on all the variations of sparse attention, but I understand the general principle…”</li>
</ul>
<p>By following these guidelines, you can effectively demonstrate your knowledge of attention mechanisms in computer vision and showcase your senior-level expertise.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>