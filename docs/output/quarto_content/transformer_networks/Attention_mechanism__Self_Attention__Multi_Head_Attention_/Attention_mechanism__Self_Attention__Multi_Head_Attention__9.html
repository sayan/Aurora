<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>attention_mechanism__self_attention__multi_head_attention__9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-10.-explain-the-potential-relationship-and-differences-between-convolutional-networks-and-attention-mechanisms.-in-what-scenarios-might-one-be-preferred-over-the-other" class="level2">
<h2 class="anchored" data-anchor-id="question-10.-explain-the-potential-relationship-and-differences-between-convolutional-networks-and-attention-mechanisms.-in-what-scenarios-might-one-be-preferred-over-the-other">Question: 10. Explain the potential relationship and differences between convolutional networks and attention mechanisms. In what scenarios might one be preferred over the other?</h2>
<p><strong>Best Answer</strong></p>
<p>Convolutional Neural Networks (CNNs) and attention mechanisms are powerful tools in deep learning, particularly in areas like computer vision and natural language processing. While they approach feature extraction and pattern recognition differently, they can also be combined or viewed as complementary techniques.</p>
<p><strong>Convolutional Neural Networks (CNNs):</strong></p>
<ul>
<li><strong>Core Principle:</strong> CNNs operate based on the principle of <em>convolution</em>, which involves applying a set of learnable filters (kernels) to local regions of the input data. These filters extract features such as edges, textures, or more complex patterns.</li>
<li><strong>Key Characteristics:</strong>
<ul>
<li><p><strong>Local Receptive Fields:</strong> Each neuron in a convolutional layer processes information only from a small, local region of the input. This region is defined by the size of the filter.</p></li>
<li><p><strong>Translation Invariance/Equivariance:</strong> CNNs are naturally translation invariant (or equivariant, depending on pooling) because the same filter is applied across the entire input. This means that if a pattern is detected in one part of the image/sequence, it will be detected regardless of its location.</p></li>
<li><p><strong>Hierarchical Feature Extraction:</strong> CNNs typically consist of multiple convolutional layers, each extracting increasingly complex features. Lower layers might detect edges, while higher layers might detect objects or scenes.</p></li>
<li><p><strong>Parameter Sharing:</strong> Convolutional filters are shared across the entire input, reducing the number of learnable parameters and improving generalization.</p></li>
<li><p><strong>Formally:</strong> A convolutional layer’s output can be represented as:</p>
<p><span class="math display">\[y[i,j] = \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} x[i+m, j+n] \cdot w[m, n] + b\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(x\)</span> is the input feature map</li>
<li><span class="math inline">\(w\)</span> is the convolutional kernel of size <span class="math inline">\(k_h \times k_w\)</span></li>
<li><span class="math inline">\(b\)</span> is the bias term</li>
<li><span class="math inline">\(y\)</span> is the output feature map at location <span class="math inline">\((i, j)\)</span></li>
</ul></li>
</ul></li>
<li><strong>Strengths:</strong>
<ul>
<li>Efficient processing of grid-like data (images, audio).</li>
<li>Effective in capturing local patterns and spatial hierarchies.</li>
<li>Translation invariance is highly beneficial for tasks where the location of a feature is not critical.</li>
<li>Relatively computationally efficient compared to attention mechanisms for certain tasks.</li>
</ul></li>
<li><strong>Weaknesses:</strong>
<ul>
<li>Limited ability to capture long-range dependencies directly, especially in early layers. The receptive field grows with depth, but capturing truly global context can require very deep networks.</li>
<li>Fixed receptive fields may not be optimal for all tasks.</li>
<li>Can be less effective for sequence data where relationships between distant elements are crucial.</li>
</ul></li>
</ul>
<p><strong>Attention Mechanisms:</strong></p>
<ul>
<li><strong>Core Principle:</strong> Attention mechanisms allow the model to focus on the most relevant parts of the input when making a decision. They compute a weighted sum of the input features, where the weights represent the importance of each feature.</li>
<li><strong>Key Characteristics:</strong>
<ul>
<li><p><strong>Adaptive Receptive Fields:</strong> Attention mechanisms can dynamically adjust their receptive field based on the input. This allows them to focus on relevant information regardless of its location.</p></li>
<li><p><strong>Global Context:</strong> Attention mechanisms consider the entire input sequence or image when computing the attention weights, enabling them to capture long-range dependencies effectively.</p></li>
<li><p><strong>Variable-Length Inputs:</strong> Attention mechanisms can handle variable-length inputs, making them suitable for tasks like machine translation.</p></li>
<li><p><strong>Interpretability:</strong> Attention weights can provide insights into which parts of the input the model is focusing on.</p></li>
<li><p><strong>Self-Attention (or Intra-Attention):</strong> A specific type of attention where the input sequence attends to itself, allowing the model to capture relationships between different parts of the same sequence. The Transformer architecture relies heavily on self-attention.</p></li>
<li><p><strong>Formally (Self-Attention):</strong></p>
<ol type="1">
<li><p><strong>Compute Query, Key, and Value:</strong> Given an input sequence <span class="math inline">\(X \in \mathbb{R}^{n \times d}\)</span>, project it into three matrices:</p>
<p><span class="math display">\[Q = XW_Q, \quad K = XW_K, \quad V = XW_V\]</span></p>
<p>where <span class="math inline">\(W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}\)</span> are learnable projection matrices.</p></li>
<li><p><strong>Compute Attention Weights:</strong> <span class="math display">\[Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span> Here, <span class="math inline">\(\frac{QK^T}{\sqrt{d_k}}\)</span> calculates the compatibility scores between the query and key, scaled by <span class="math inline">\(\sqrt{d_k}\)</span> to prevent vanishing gradients. The softmax normalizes these scores into probabilities (attention weights). <span class="math inline">\(V\)</span> is a matrix of values to be weighted by the attention score.</p></li>
</ol></li>
</ul></li>
<li><strong>Strengths:</strong>
<ul>
<li>Excellent for capturing long-range dependencies.</li>
<li>Adaptive receptive fields improve performance on tasks with complex relationships.</li>
<li>Handles variable-length inputs effectively.</li>
<li>Provides interpretability through attention weights.</li>
<li>Offers flexibility for various tasks (translation, image captioning, etc.)</li>
</ul></li>
<li><strong>Weaknesses:</strong>
<ul>
<li>Higher computational cost, especially for long sequences. The complexity is often <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length.</li>
<li>Can be more prone to overfitting if not regularized properly.</li>
<li>May require more data to train effectively compared to CNNs for certain tasks.</li>
<li>Less inherent translation invariance compared to CNNs.</li>
</ul></li>
</ul>
<p><strong>Relationship and Hybrid Approaches:</strong></p>
<p>CNNs and attention mechanisms can be combined in various ways:</p>
<ul>
<li><strong>Attention after CNNs:</strong> CNNs can be used for initial feature extraction, and then attention mechanisms can be applied to these features to capture long-range dependencies. This is common in image captioning, where a CNN extracts visual features and an attention-based RNN generates the caption.</li>
<li><strong>Attention within CNNs:</strong> Attention mechanisms can be integrated into convolutional layers to dynamically weight the importance of different feature maps or spatial locations. This can improve the ability of CNNs to focus on relevant information. Examples: Squeeze-and-Excitation Networks (SENet), CBAM (Convolutional Block Attention Module).</li>
<li><strong>Combining CNNs and Transformers:</strong> Approaches are emerging that integrate CNNs with Transformers, attempting to leverage the strengths of both. For example, using a CNN for initial feature extraction from images before feeding them into a Transformer encoder.</li>
</ul>
<p><strong>Scenarios for Preference:</strong></p>
<ul>
<li><strong>CNNs:</strong>
<ul>
<li>Image classification: when translation invariance and local feature extraction are crucial.</li>
<li>Object detection: initial feature extraction.</li>
<li>Audio processing: when local patterns are important.</li>
<li>Tasks where computational efficiency is a primary concern.</li>
</ul></li>
<li><strong>Attention Mechanisms:</strong>
<ul>
<li>Machine translation: capturing long-range dependencies between words.</li>
<li>Image captioning: focusing on relevant regions of the image when generating the caption.</li>
<li>Natural language understanding: modeling relationships between different parts of a sentence or document.</li>
<li>Tasks involving variable-length sequences.</li>
<li>Tasks where global context is essential.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong></p>
<p>CNNs and attention mechanisms are complementary tools that can be used together to build powerful deep learning models. CNNs excel at capturing local patterns and translation invariance, while attention mechanisms excel at capturing long-range dependencies and adapting receptive fields. The choice between them depends on the specific task and the nature of the data. Hybrid approaches that combine the strengths of both are often the most effective.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this in an interview:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“CNNs and attention mechanisms are both essential components in deep learning, serving different purposes in feature extraction and relationship modeling. CNNs focus on local patterns, while attention mechanisms allow models to focus on the most relevant parts of the input, even over long distances.”</li>
</ul></li>
<li><strong>Explain CNNs:</strong>
<ul>
<li>“CNNs use convolutional filters to extract features from local regions of the input, making them efficient for grid-like data like images and audio. Their key strength is translation invariance, meaning they can recognize patterns regardless of their location.”</li>
<li>“The output of a convolutional layer can be described by the equation: [briefly explain the convolution equation]. Essentially, each output location is a weighted sum of the inputs within the filter’s receptive field.”</li>
<li>“However, CNNs can struggle with long-range dependencies, especially in early layers. The receptive field has to grow over many layers to capture global context.”</li>
</ul></li>
<li><strong>Transition to Attention Mechanisms:</strong>
<ul>
<li>“Attention mechanisms, on the other hand, excel at capturing long-range dependencies. They allow the model to dynamically focus on the most relevant parts of the input when making a decision.”</li>
<li>“Unlike CNNs, attention mechanisms have adaptive receptive fields, which can be adjusted based on the input. This is particularly useful for tasks where relationships between distant elements are crucial.”</li>
</ul></li>
<li><strong>Explain Self-Attention (if appropriate, based on the interviewer’s knowledge):</strong>
<ul>
<li>“A key type of attention is self-attention, where the input attends to itself. This is fundamental to the Transformer architecture.”</li>
<li>“In self-attention, the input is projected into Query, Key, and Value matrices. The attention weights are calculated by taking the softmax of (Query times Key transpose), scaled by the square root of the dimension. This is then multiplied by the Value matrix to obtain the attention-weighted representation.”</li>
<li>“The softmax part is important because it normalizes these scores into probabilities (attention weights). This helps the model decide what elements in the input are most relevant.”</li>
<li>“If the interviewer probes about Multi-Head Attention, explain that Multi-Head Attention simply runs the attention mechanism multiple times with different learned projections (different Q, K, V matrices), and concatenates the outputs, allowing the model to capture different aspects of the relationships.”</li>
</ul></li>
<li><strong>Compare Strengths and Weaknesses:</strong>
<ul>
<li>“CNNs are computationally efficient and good for translation invariance, but struggle with long-range dependencies. Attention mechanisms excel at capturing long-range dependencies and have adaptive receptive fields but are computationally more expensive.”</li>
</ul></li>
<li><strong>Discuss Hybrid Approaches:</strong>
<ul>
<li>“In practice, it’s common to combine CNNs and attention mechanisms. For example, using CNNs for initial feature extraction and then applying attention to capture long-range relationships.”</li>
<li>“We can also integrate attention <em>within</em> CNNs – as seen in Squeeze-and-Excitation Networks – to dynamically weight feature maps.”</li>
</ul></li>
<li><strong>Provide Examples:</strong>
<ul>
<li>“For image classification, CNNs are often preferred due to their efficiency and ability to capture local features. For machine translation, attention mechanisms are crucial for capturing relationships between words across the entire sentence.”</li>
</ul></li>
<li><strong>Conclude Summarizing Key Points:</strong>
<ul>
<li>“In summary, CNNs and attention mechanisms are complementary tools. CNNs excel at local pattern recognition, while attention mechanisms are strong at capturing long-range dependencies. The best approach often involves combining the strengths of both.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use visuals (if possible):</strong> If you’re in an in-person interview, consider drawing diagrams or using visual aids to illustrate the concepts. Even a simple sketch of a convolutional filter or an attention mechanism can be helpful.</li>
<li><strong>Check for understanding:</strong> Pause periodically and ask the interviewer if they have any questions. This shows that you’re engaged and want to ensure they’re following along.</li>
<li><strong>Don’t be afraid to simplify:</strong> If the interviewer seems less familiar with the technical details, adjust your explanation accordingly. Focus on the core concepts and avoid getting bogged down in unnecessary jargon.</li>
<li><strong>Demonstrate practical knowledge:</strong> Whenever possible, provide real-world examples of how CNNs and attention mechanisms are used in different applications.</li>
<li><strong>Be confident:</strong> Speak clearly and confidently, demonstrating your expertise in the subject matter.</li>
<li><strong>Be open to questions:</strong> The interviewer may ask follow-up questions to test your understanding. Be prepared to answer them thoughtfully and honestly. If you don’t know the answer, it’s okay to say so, but try to explain your reasoning or suggest possible approaches.</li>
<li><strong>Highlight Tradeoffs:</strong> When comparing the two techniques, consistently emphasize the tradeoffs in terms of computational cost, data requirements, and the types of relationships they are best suited to model.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>