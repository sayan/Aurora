<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>attention_mechanism__self_attention__multi_head_attention__1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-2.-walk-me-through-the-detailed-computation-steps-in-self-attention.-how-are-the-queries-keys-and-values-generated-and-used" class="level2">
<h2 class="anchored" data-anchor-id="question-2.-walk-me-through-the-detailed-computation-steps-in-self-attention.-how-are-the-queries-keys-and-values-generated-and-used">Question: 2. Walk me through the detailed computation steps in self-attention. How are the queries, keys, and values generated and used?</h2>
<p><strong>Best Answer</strong></p>
<p>The self-attention mechanism, a cornerstone of transformers, allows a model to attend to different parts of the input sequence when processing each element. Here’s a detailed breakdown of the computation steps:</p>
<ol type="1">
<li><p><strong>Input Embedding:</strong></p>
<ul>
<li>We start with an input sequence, which is typically a sequence of word embeddings. Let’s denote this input sequence as <span class="math inline">\(X = [x_1, x_2, ..., x_n]\)</span>, where each <span class="math inline">\(x_i \in \mathbb{R}^{d_{model}}\)</span> and <span class="math inline">\(n\)</span> is the sequence length, and <span class="math inline">\(d_{model}\)</span> is the embedding dimension.</li>
</ul></li>
<li><p><strong>Linear Projections (Generating Q, K, V):</strong></p>
<ul>
<li>The input <span class="math inline">\(X\)</span> is linearly transformed into three different representations: Queries (Q), Keys (K), and Values (V). This is done by multiplying <span class="math inline">\(X\)</span> with three different weight matrices: <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, and <span class="math inline">\(W_V\)</span>.</li>
<li><span class="math display">\[Q = XW_Q\]</span></li>
<li><span class="math display">\[K = XW_K\]</span></li>
<li><span class="math display">\[V = XW_V\]</span></li>
<li>Where <span class="math inline">\(W_Q, W_K, W_V \in \mathbb{R}^{d_{model} \times d_k}\)</span> and <span class="math inline">\(Q, K, V \in \mathbb{R}^{n \times d_k}\)</span>. <span class="math inline">\(d_k\)</span> is the dimension of the key (and query) vectors. It’s common to set <span class="math inline">\(d_k\)</span> smaller than <span class="math inline">\(d_{model}\)</span> for computational efficiency. The value matrix V usually has dimension <span class="math inline">\(d_v\)</span>, it is common to set <span class="math inline">\(d_v = d_k\)</span>.</li>
<li>Each row of <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> represents the query, key, and value vector for the corresponding input element. So, <span class="math inline">\(q_i\)</span>, <span class="math inline">\(k_i\)</span>, and <span class="math inline">\(v_i\)</span> are the query, key, and value vectors associated with <span class="math inline">\(x_i\)</span>.</li>
</ul></li>
<li><p><strong>Calculating Attention Scores:</strong></p>
<ul>
<li>The attention scores determine how much importance each element in the input sequence should have when representing the current element. These scores are computed by taking the dot product of the query vector of the current element (<span class="math inline">\(q_i\)</span>) with the key vectors of all other elements (<span class="math inline">\(k_j\)</span>).</li>
<li><span class="math display">\[Attention \ Scores = QK^T\]</span></li>
<li>Each element <span class="math inline">\(e_{ij}\)</span> in the resulting <span class="math inline">\(Attention \ Scores\)</span> matrix represents the unnormalized attention score between the i-th query and the j-th key. So, <span class="math inline">\(e_{ij} = q_i \cdot k_j\)</span>.</li>
</ul></li>
<li><p><strong>Scaled Dot-Product Attention:</strong></p>
<ul>
<li>To prevent the dot products from growing too large, which can push the softmax function into regions with extremely small gradients, we scale the attention scores by the square root of the dimension of the key vectors (<span class="math inline">\(d_k\)</span>). This scaling helps stabilize training.</li>
<li><span class="math display">\[Scaled \ Attention \ Scores = \frac{QK^T}{\sqrt{d_k}}\]</span></li>
</ul></li>
<li><p><strong>Softmax:</strong></p>
<ul>
<li>The scaled attention scores are then passed through a softmax function to obtain attention weights. These weights represent the probability distribution over the input sequence, indicating the relative importance of each element.</li>
<li><span class="math display">\[Attention \ Weights = softmax(\frac{QK^T}{\sqrt{d_k}})\]</span></li>
<li>The softmax is applied row-wise, meaning each query’s attention scores over all keys are normalized independently.</li>
</ul></li>
<li><p><strong>Weighted Sum:</strong></p>
<ul>
<li>Finally, the attention weights are used to compute a weighted sum of the value vectors. This weighted sum produces the output representation for each element in the input sequence. The attention weights determine how much each value vector contributes to this output.</li>
<li><span class="math display">\[Output = Attention \ Weights \cdot V\]</span></li>
<li>Formally: <span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></li>
</ul></li>
<li><p><strong>Multi-Head Attention (Extension):</strong></p>
<ul>
<li>To allow the model to capture different aspects of the relationships between elements, the self-attention mechanism is often extended to multi-head attention. In multi-head attention, the input is linearly transformed into multiple sets of Q, K, and V (each set is a “head”), and the self-attention mechanism is applied independently to each head. The outputs of all heads are then concatenated and linearly transformed to produce the final output.</li>
<li><span class="math display">\[MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\]</span></li>
<li><span class="math display">\[where \ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\]</span></li>
<li>Where <span class="math inline">\(W_i^Q \in \mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W_i^K \in \mathbb{R}^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W_i^V \in \mathbb{R}^{d_{model} \times d_v}\)</span>, and <span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_{model}}\)</span>. <span class="math inline">\(h\)</span> is the number of heads.</li>
</ul></li>
</ol>
<p><strong>Why is this important?</strong></p>
<p>Self-attention is crucial because it allows the model to capture long-range dependencies in the input sequence. Unlike recurrent neural networks (RNNs), which process the input sequentially, self-attention can attend to any part of the input sequence directly. This makes it possible to model relationships between distant elements in the sequence more effectively. The scaling factor is also crucial for stable training. Multi-head attention further enhances the model’s ability to capture different types of relationships.</p>
<p><strong>Real-world considerations:</strong></p>
<ul>
<li><strong>Computational Complexity:</strong> Self-attention has a quadratic computational complexity with respect to the sequence length (<span class="math inline">\(O(n^2)\)</span>). For long sequences, this can be a bottleneck. Techniques like sparse attention or using linear approximations to the attention mechanism are used to mitigate this.</li>
<li><strong>Implementation Details:</strong> Efficient matrix multiplication libraries (e.g., optimized BLAS or cuBLAS on GPUs) are crucial for implementing self-attention efficiently.</li>
<li><strong>Padding:</strong> When processing batches of sequences, padding is often used to make all sequences the same length. It’s important to mask the padding tokens so they don’t contribute to the attention scores. This is typically done by setting the attention scores for padding tokens to <span class="math inline">\(-\infty\)</span> before applying the softmax.</li>
<li><strong>Memory Requirements:</strong> Attention matrices can consume a lot of memory, especially for large sequences. Memory-efficient attention mechanisms have been proposed to address this, such as gradient checkpointing.</li>
<li><strong>Positional Encoding:</strong> Since self-attention is permutation-invariant (it doesn’t inherently capture the order of the input sequence), positional encodings are added to the input embeddings to provide information about the position of each element.</li>
</ul>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to explain this in an interview:</p>
<ol type="1">
<li><p><strong>Start with a high-level overview:</strong> “Self-attention is a mechanism that allows a model to attend to different parts of the input sequence when processing each element, enabling it to capture long-range dependencies.”</p></li>
<li><p><strong>Explain the Q, K, V generation:</strong> “First, the input sequence is transformed into three sets of vectors: Queries (Q), Keys (K), and Values (V). This is done through linear projections, where the input is multiplied by learned weight matrices: <span class="math inline">\(Q = XW_Q\)</span>, <span class="math inline">\(K = XW_K\)</span>, <span class="math inline">\(V = XW_V\)</span>.”</p></li>
<li><p><strong>Describe the attention score calculation:</strong> “The attention scores are computed by taking the dot product of the query vectors with the key vectors: <span class="math inline">\(Attention \ Scores = QK^T\)</span>. This gives us a measure of similarity between each pair of input elements.”</p></li>
<li><p><strong>Explain the scaling and softmax:</strong> “To stabilize training and prevent the softmax from saturating, we scale the attention scores by the square root of the key dimension: <span class="math inline">\(Scaled \ Attention \ Scores = \frac{QK^T}{\sqrt{d_k}}\)</span>. Then, we apply the softmax function to obtain attention weights: <span class="math inline">\(Attention \ Weights = softmax(\frac{QK^T}{\sqrt{d_k}})\)</span>.”</p></li>
<li><p><strong>Detail the weighted sum:</strong> “Finally, we compute a weighted sum of the value vectors, using the attention weights as coefficients: <span class="math inline">\(Output = Attention \ Weights \cdot V\)</span>. This gives us the output representation for each element, which is a weighted combination of the value vectors, weighted by the attention that element has given each of the value vectors.” You can also write down the whole equation as: <span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p></li>
<li><p><strong>Address Multi-Head Attention (if applicable):</strong> “To capture different types of relationships, we often use multi-head attention. The input is transformed into multiple sets of Q, K, and V, we perform the attention mechanism and then concat these heads.”</p></li>
<li><p><strong>Emphasize the importance:</strong> “This mechanism is crucial because it allows the model to capture long-range dependencies in the input sequence, unlike RNNs. This is essential for tasks like machine translation and text summarization.”</p></li>
<li><p><strong>Discuss real-world considerations (if asked):</strong> “Some practical considerations include the quadratic complexity, memory requirements, and the need for masking padding tokens. Techniques like sparse attention and gradient checkpointing are used to mitigate these challenges.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation.</li>
<li><strong>Use visual aids:</strong> If possible, draw a diagram of the self-attention mechanism on a whiteboard or share a diagram on a screen.</li>
<li><strong>Check for understanding:</strong> After explaining each step, ask the interviewer if they have any questions.</li>
<li><strong>Explain the math clearly:</strong> Write down the equations and explain each term. Don’t assume the interviewer knows the notation.</li>
<li><strong>Focus on the intuition:</strong> Explain <em>why</em> each step is done, not just <em>what</em> is done. For example, explain why scaling is important.</li>
<li><strong>Connect to real-world applications:</strong> Mention specific tasks where self-attention is used, such as machine translation or text summarization.</li>
<li><strong>Adapt to the interviewer’s level:</strong> If the interviewer seems unfamiliar with the topic, simplify your explanation. If they seem knowledgeable, you can go into more detail.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>