<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>attention_mechanism__self_attention__multi_head_attention__0</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-1.-can-you-explain-the-basic-idea-behind-the-self-attention-mechanism-and-its-importance-in-sequence-modeling" class="level2">
<h2 class="anchored" data-anchor-id="question-1.-can-you-explain-the-basic-idea-behind-the-self-attention-mechanism-and-its-importance-in-sequence-modeling">Question: 1. Can you explain the basic idea behind the self-attention mechanism and its importance in sequence modeling?</h2>
<p><strong>Best Answer</strong></p>
<p>The self-attention mechanism is a crucial component in modern sequence modeling, particularly in architectures like Transformers. It allows the model to attend to different parts of the input sequence when processing each element, effectively weighing their importance in the representation of that element. This is a significant departure from traditional recurrent neural networks (RNNs) which process sequences sequentially, making it challenging to capture long-range dependencies.</p>
<p>Here’s a breakdown of the key aspects:</p>
<ul>
<li><p><strong>Core Idea</strong>: At its heart, self-attention is about computing a weighted sum of the values associated with each position in the input sequence. The weights determine how much attention should be paid to each position when calculating the representation of a specific position. These weights are dynamically learned based on the relationships between the different parts of the input sequence.</p></li>
<li><p><strong>Mathematical Formulation:</strong></p>
<ol type="1">
<li><p><strong>Input Representation</strong>: Given an input sequence, we first represent each token (word, sub-word, etc.) as a vector. Let <span class="math inline">\(X \in \mathbb{R}^{n \times d}\)</span> be the input matrix, where <span class="math inline">\(n\)</span> is the sequence length and <span class="math inline">\(d\)</span> is the dimension of each token embedding.</p></li>
<li><p><strong>Linear Transformations</strong>: We then transform these embeddings into three different representations: queries (<span class="math inline">\(Q\)</span>), keys (<span class="math inline">\(K\)</span>), and values (<span class="math inline">\(V\)</span>). These are obtained by multiplying the input matrix by three different weight matrices:</p>
<p><span class="math display">\[
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\]</span></p>
<p>Where <span class="math inline">\(W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}\)</span> are the weight matrices that are learned during training, and <span class="math inline">\(d_k\)</span> is the dimension of the queries, keys, and values. Often, <span class="math inline">\(d_k\)</span> is chosen such that <span class="math inline">\(d_k = d/h\)</span>, where <span class="math inline">\(h\)</span> is the number of heads (more on this in multi-head attention).</p></li>
<li><p><strong>Attention Weights</strong>: The attention weights are calculated by taking the dot product of the query matrix <span class="math inline">\(Q\)</span> with the key matrix <span class="math inline">\(K\)</span>, scaling the result, and then applying a softmax function. This produces a matrix of weights, indicating the importance of each position in the sequence with respect to every other position.</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>The scaling factor <span class="math inline">\(\sqrt{d_k}\)</span> is used to stabilize training. Without it, the dot products can become very large, pushing the softmax function into regions where the gradients are extremely small, hindering learning. This issue becomes more pronounced as <span class="math inline">\(d_k\)</span> increases.</p></li>
<li><p><strong>Weighted Sum</strong>: Finally, the attention weights are used to compute a weighted sum of the value matrix <span class="math inline">\(V\)</span>. This weighted sum represents the output of the self-attention mechanism for each position in the sequence.</p></li>
</ol></li>
<li><p><strong>Importance in Sequence Modeling</strong>:</p>
<ul>
<li><p><strong>Capturing Long-Range Dependencies</strong>: Self-attention allows each position in the sequence to directly attend to any other position, regardless of the distance between them. This makes it much easier to capture long-range dependencies compared to RNNs, where information needs to be passed sequentially through the network. In RNNs, the information about the beginning of the sequence might be significantly diluted by the time the network processes the end of the sequence, especially for long sequences.</p></li>
<li><p><strong>Parallelization</strong>: Unlike RNNs, self-attention can be computed in parallel for all positions in the sequence. This significantly speeds up training, especially on modern hardware like GPUs and TPUs. RNNs, by their sequential nature, limit parallelization.</p></li>
<li><p><strong>Interpretability</strong>: The attention weights provide some degree of interpretability. By examining the attention weights, we can see which parts of the input sequence the model is attending to when processing a particular element. This can provide insights into the model’s reasoning process.</p></li>
<li><p><strong>Multi-Head Attention</strong>: A common extension of self-attention is multi-head attention. Instead of performing a single self-attention calculation, the input is transformed into multiple sets of queries, keys, and values. Each set is then used to compute a separate attention output, and the results are concatenated and linearly transformed to produce the final output.</p>
<p><span class="math display">\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\]</span></p>
<p>where <span class="math inline">\(\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span>, and <span class="math inline">\(W_i^Q, W_i^K, W_i^V, W^O\)</span> are learnable parameters. Multi-head attention allows the model to capture different types of relationships between elements in the sequence, which improves its overall performance.</p></li>
</ul></li>
<li><p><strong>Advantages over RNNs</strong>:</p>
<ul>
<li>Handles long-range dependencies more effectively.</li>
<li>Enables parallel computation, leading to faster training.</li>
<li>Provides some interpretability through attention weights.</li>
</ul></li>
<li><p><strong>Real-World Considerations:</strong></p>
<ul>
<li><strong>Computational Complexity</strong>: The computational complexity of self-attention is <span class="math inline">\(O(n^2d)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length. This can be a bottleneck for very long sequences. Techniques like sparse attention or linear attention have been developed to reduce this complexity.</li>
<li><strong>Memory Usage</strong>: The attention matrices can consume a significant amount of memory, especially for long sequences. Gradient checkpointing is often used to reduce memory usage during training, at the cost of increased computation time (recomputing activations during backpropagation).</li>
<li><strong>Positional Encoding</strong>: Since self-attention is permutation-equivariant (i.e., it doesn’t inherently account for the order of the input sequence), positional encodings are often added to the input embeddings to provide information about the position of each element in the sequence. These encodings can be learned or fixed (e.g., sinusoidal functions).</li>
<li><strong>Causal (Masked) Self-Attention</strong>: In autoregressive models (e.g., language models), it’s crucial to prevent the model from attending to future tokens when predicting the current token. This is achieved through masked self-attention, where the attention weights for future tokens are set to <span class="math inline">\(-\infty\)</span> before applying the softmax function.</li>
</ul></li>
</ul>
<p>In summary, the self-attention mechanism is a powerful tool for sequence modeling. It allows models to capture long-range dependencies, be parallelized, and provide some interpretability. While it has its own challenges (e.g., computational complexity), it has become a fundamental building block in many state-of-the-art sequence models.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to articulate this answer during an interview:</p>
<ol type="1">
<li><strong>Start with the Basics</strong>:
<ul>
<li>“The self-attention mechanism is designed to weigh the importance of different parts of an input sequence when processing each element. It’s a core component of the Transformer architecture and allows the model to capture long-range dependencies.”</li>
<li>“Unlike RNNs, which process sequences sequentially, self-attention allows the model to attend to <em>any</em> part of the input sequence directly.”</li>
</ul></li>
<li><strong>Explain the Math (Keep it High-Level Initially)</strong>:
<ul>
<li>“At a high level, the mechanism involves transforming the input into queries, keys, and values. We compute attention weights based on the relationship between queries and keys, and then use these weights to compute a weighted sum of the values.”</li>
<li>“More formally, we start with the input sequence <span class="math inline">\(X\)</span>. We multiply it with weight matrices to get Query (<span class="math inline">\(Q\)</span>), Key (<span class="math inline">\(K\)</span>), and Value (<span class="math inline">\(V\)</span>) matrices.”</li>
<li>“Then, the attention is calculated as softmax of <span class="math inline">\(QK^T\)</span> divided by the square root of the dimension of key, the whole result multiplied with <span class="math inline">\(V\)</span>.”</li>
<li>If the interviewer seems interested in more depth, you can provide the explicit formulas and explain the role of each term. But avoid diving too deep into the math unless prompted.</li>
</ul></li>
<li><strong>Highlight Key Advantages</strong>:
<ul>
<li>“The main advantages are the ability to capture long-range dependencies more effectively, its parallelizable nature which speeds up training, and a degree of interpretability through the attention weights.”</li>
<li>“Unlike RNNs where information has to flow sequentially, self-attention allows for direct connections between any two tokens.”</li>
</ul></li>
<li><strong>Explain Multi-Head Attention</strong>:
<ul>
<li>“A common extension is multi-head attention, where we perform the self-attention mechanism multiple times with different learned projections of the input. This allows the model to capture different types of relationships between elements in the sequence.”</li>
<li>“So you can conceptualize it as each head focusing on a different aspect of the relationship between the tokens.”</li>
</ul></li>
<li><strong>Discuss Real-World Considerations</strong>:
<ul>
<li>“While self-attention is powerful, there are challenges. The computational complexity is <span class="math inline">\(O(n^2d)\)</span>, which can be a bottleneck for long sequences. This has led to research into sparse and linear attention mechanisms.”</li>
<li>“Memory usage can also be a concern, especially with large models and long sequences. Techniques like gradient checkpointing are used to mitigate this.”</li>
<li>“Because self-attention is permutation-equivariant, we often use positional encodings to provide information about the order of the sequence.”</li>
</ul></li>
<li><strong>Be Prepared for Follow-Up Questions</strong>:
<ul>
<li>Anticipate questions about the computational complexity, memory usage, and techniques for mitigating these issues.</li>
<li>Be ready to discuss the differences between self-attention and other attention mechanisms (e.g., attention in encoder-decoder models).</li>
<li>Think about how self-attention is used in various architectures like Transformers, BERT, GPT, etc.</li>
</ul></li>
</ol>
<p><strong>Communication Tips</strong>:</p>
<ul>
<li><strong>Pace yourself</strong>: Don’t rush through the explanation.</li>
<li><strong>Use clear and concise language</strong>: Avoid jargon unless necessary.</li>
<li><strong>Check for understanding</strong>: Pause occasionally to ask if the interviewer has any questions.</li>
<li><strong>Tailor your answer</strong>: Adapt your explanation to the interviewer’s level of expertise. If they are unfamiliar with the concept, start with the basics and build up from there. If they are familiar, you can dive into more technical details.</li>
<li><strong>Show enthusiasm</strong>: Demonstrate your passion for the topic.</li>
<li><strong>Don’t be afraid to say “I don’t know”</strong>: If you are unsure about something, it is better to be honest than to bluff. You can follow up by saying that you would be happy to look into it further.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>