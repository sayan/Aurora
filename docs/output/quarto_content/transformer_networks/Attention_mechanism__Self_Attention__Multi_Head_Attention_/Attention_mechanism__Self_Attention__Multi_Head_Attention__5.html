<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>attention_mechanism__self_attention__multi_head_attention__5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-6.-how-does-positional-encoding-integrate-with-self-attention-mechanisms-and-what-alternatives-exist-to-the-classic-sinusoidal-or-learned-positional-encodings" class="level2">
<h2 class="anchored" data-anchor-id="question-6.-how-does-positional-encoding-integrate-with-self-attention-mechanisms-and-what-alternatives-exist-to-the-classic-sinusoidal-or-learned-positional-encodings">Question: 6. How does positional encoding integrate with self-attention mechanisms, and what alternatives exist to the classic sinusoidal or learned positional encodings?</h2>
<p><strong>Best Answer</strong></p>
<p>Positional encoding is a crucial component in architectures that utilize self-attention mechanisms, such as Transformers, particularly when processing sequential data. The self-attention mechanism, by design, is permutation-invariant; it processes the input sequence as a set and does not inherently account for the order of elements. Therefore, positional encoding is introduced to inject information about the position of each element in the sequence, enabling the model to understand and utilize the order of the data.</p>
<p><strong>Why Positional Encoding is Necessary</strong></p>
<p>Consider a sequence of words “the cat sat on the mat”. Without positional information, the self-attention mechanism would treat “the cat sat” and “cat the sat” identically, leading to a loss of crucial sequential information. Positional encodings provide a unique “fingerprint” for each position, allowing the model to differentiate between elements based on their location in the sequence.</p>
<p><strong>Classic Sinusoidal Positional Encoding</strong></p>
<p>Vaswani et al.&nbsp;(2017) introduced sinusoidal positional encodings in the original Transformer paper. These encodings use sine and cosine functions of different frequencies to create a unique positional vector for each position in the sequence. The positional encoding <span class="math inline">\(PE\)</span> for position <span class="math inline">\(pos\)</span> and dimension <span class="math inline">\(i\)</span> is defined as:</p>
<p><span class="math display">\[
PE(pos, 2i) = sin(\frac{pos}{10000^{2i/d_{model}}})
\]</span></p>
<p><span class="math display">\[
PE(pos, 2i+1) = cos(\frac{pos}{10000^{2i/d_{model}}})
\]</span></p>
<p>where: - <span class="math inline">\(pos\)</span> is the position in the input sequence. - <span class="math inline">\(i\)</span> is the dimension index. - <span class="math inline">\(d_{model}\)</span> is the dimensionality of the positional encoding (and the model’s embedding dimension).</p>
<p>The intuition behind using sine and cosine functions is that they provide a range of frequencies, allowing the model to attend to different relative positions. Additionally, linear combinations of these sinusoidal functions can represent relative positions, enabling the model to generalize to sequence lengths not seen during training. We can demonstrate that for any fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear transformation of <span class="math inline">\(PE_{pos}\)</span>. This can be shown using trigonometric identities. This property allows the model to easily attend to relative positions.</p>
<p><strong>Integration with Self-Attention</strong></p>
<p>Positional encodings are typically added directly to the input embeddings before they are fed into the self-attention layers:</p>
<p><span class="math display">\[
X_{encoded} = X_{embeddings} + PE
\]</span></p>
<p>where: - <span class="math inline">\(X_{embeddings}\)</span> are the input embeddings. - <span class="math inline">\(PE\)</span> is the positional encoding matrix. - <span class="math inline">\(X_{encoded}\)</span> is the combined embedding with positional information.</p>
<p>This combined input is then used to compute the query (<span class="math inline">\(Q\)</span>), key (<span class="math inline">\(K\)</span>), and value (<span class="math inline">\(V\)</span>) matrices, which are used in the self-attention mechanism:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<p>where <span class="math inline">\(d_k\)</span> is the dimensionality of the keys. The inclusion of positional information in <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> allows the attention mechanism to weigh the importance of different positions in the sequence.</p>
<p><strong>Learned Positional Encoding</strong></p>
<p>Instead of using predefined functions, positional encodings can also be learned during training. In this approach, a positional embedding matrix is initialized randomly and updated along with the other model parameters during training. Learned positional encodings can potentially adapt to the specific characteristics of the dataset and task.</p>
<p><strong>Alternatives to Sinusoidal and Learned Positional Encodings</strong></p>
<ol type="1">
<li><p><strong>Relative Positional Encoding:</strong></p>
<ul>
<li><p>Instead of encoding absolute positions, relative positional encodings encode the distance between tokens. This is particularly useful when the absolute position is less important than the relative relationships between elements.</p></li>
<li><p>One way to implement relative positional encoding is to modify the attention mechanism directly. The attention score between tokens <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is computed as:</p>
<p><span class="math display">\[
Attention_{ij} = Q_iK_j^T + R_{i-j}
\]</span></p>
<p>where <span class="math inline">\(R_{i-j}\)</span> is the relative positional encoding for the distance <span class="math inline">\(i-j\)</span>. This adds positional information directly into the attention weights.</p></li>
</ul></li>
<li><p><strong>Position-Aware Self-Attention:</strong></p>
<ul>
<li><p>This approach integrates positional information directly into the self-attention mechanism. Shaw et al.&nbsp;(2018) proposed a modification to the self-attention formula that includes relative position embeddings:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax(\frac{QK^T + S}{\sqrt{d_k}})V
\]</span></p>
<p>where <span class="math inline">\(S_{ij} = a_{clip(i-j, -k, k)}\)</span> and <span class="math inline">\(a\)</span> is a learned embedding for each of the relative positions. <span class="math inline">\(clip\)</span> ensures the relative position is within the bounds of a predefined window <span class="math inline">\([-k, k]\)</span>.</p></li>
</ul></li>
<li><p><strong>Recurrent Neural Networks (RNNs):</strong></p>
<ul>
<li>While not strictly positional encoding, RNNs inherently process sequential data in order. The hidden state at each time step contains information about the previous elements in the sequence, effectively encoding positional information. However, RNNs suffer from limitations such as difficulty in capturing long-range dependencies.</li>
</ul></li>
<li><p><strong>Convolutional Neural Networks (CNNs):</strong></p>
<ul>
<li>Similar to RNNs, CNNs also process data sequentially through the use of kernels that slide over the input sequence, which implicitly encode positional information based on the kernel size and stride.</li>
</ul></li>
<li><p><strong>Complex Embeddings:</strong></p>
<ul>
<li>Some approaches use complex numbers to represent positional information. For example, each position <span class="math inline">\(p\)</span> could be associated with a complex number <span class="math inline">\(e^{ip\theta}\)</span> for some fixed frequency <span class="math inline">\(\theta\)</span>.</li>
</ul></li>
</ol>
<p><strong>Real-World Considerations</strong></p>
<ul>
<li><strong>Sequence Length:</strong> For very long sequences, the sinusoidal encodings might start to repeat, and learned encodings may not generalize well if the model is trained on shorter sequences. Relative positional encodings can be more effective in these cases.</li>
<li><strong>Computational Cost:</strong> Some positional encoding methods, such as adding learned embeddings for all possible relative positions, can significantly increase the model’s memory footprint, especially for long sequences.</li>
<li><strong>Task Dependence:</strong> The choice of positional encoding method can depend on the specific task. For tasks where absolute position is critical (e.g., machine translation), sinusoidal or learned encodings might be suitable. For tasks where relative position is more important (e.g., document summarization), relative positional encodings might be a better choice.</li>
</ul>
<p>In summary, positional encoding is essential for self-attention mechanisms to effectively process sequential data. While sinusoidal encodings are a common choice due to their simplicity and generalization properties, learned positional encodings and relative positional encodings offer alternative solutions that can be more suitable for specific tasks and sequence lengths. These various approaches each have different trade-offs in terms of computational cost, generalization ability, and suitability for different tasks, and are thus important to understand when designing sequence processing models.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested approach for presenting this answer in an interview, emphasizing clarity and depth without overwhelming the interviewer:</p>
<ol type="1">
<li><strong>Start with the Importance:</strong>
<ul>
<li>Begin by stating the core problem: “Self-attention mechanisms are permutation-invariant, meaning they don’t inherently understand sequence order. Therefore, positional encoding is crucial for injecting information about the position of each element.” This immediately establishes the context and significance.</li>
</ul></li>
<li><strong>Explain Sinusoidal Encodings Clearly:</strong>
<ul>
<li>Introduce sinusoidal positional encodings: “The original Transformer paper used sinusoidal positional encodings, which employ sine and cosine functions of different frequencies.”</li>
<li>Present the equations: “The positional encoding for position <span class="math inline">\(pos\)</span> and dimension <span class="math inline">\(i\)</span> is defined by these formulas…” Write the two formulas for <span class="math inline">\(PE(pos, 2i)\)</span> and <span class="math inline">\(PE(pos, 2i+1)\)</span>.</li>
<li>Explain the rationale: “The use of sine and cosine functions with different frequencies allows the model to attend to various relative positions. Crucially, this also allows the model to attend to relative positions, and generalize to unseen sequence lengths.”</li>
<li>“For any fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear transformation of <span class="math inline">\(PE_{pos}\)</span>” and mention that this can be proved using trigonometric identities.</li>
</ul></li>
<li><strong>Illustrate Integration with Self-Attention:</strong>
<ul>
<li>Explain how the encodings are combined with input embeddings: “Positional encodings are added directly to the input embeddings using the formula <span class="math inline">\(X_{encoded} = X_{embeddings} + PE\)</span>.”</li>
<li>Relate it to the attention mechanism: “This combined input is then used to compute the query, key, and value matrices, influencing how the attention mechanism weighs different positions.”</li>
</ul></li>
<li><strong>Introduce Learned Encodings Concisely:</strong>
<ul>
<li>“Instead of fixed functions, we can also <em>learn</em> positional encodings. These are initialized randomly and updated during training. This can adapt better to the specific dataset.”</li>
</ul></li>
<li><strong>Discuss Alternatives Systematically:</strong>
<ul>
<li>Present the alternatives: “There are several alternatives to these classic methods, including…”</li>
<li>Explain Relative Positional Encoding: “Relative positional encodings encode the <em>distance</em> between tokens instead of absolute positions. The attention score can be modified as: <span class="math inline">\(Attention_{ij} = Q_iK_j^T + R_{i-j}\)</span>, where <span class="math inline">\(R_{i-j}\)</span> is the relative positional encoding.”</li>
<li>Mention Position-Aware Self-Attention: “Another approach is position-aware self-attention, where positional information is integrated directly into the attention mechanism.”</li>
</ul></li>
<li><strong>Address Real-World Considerations:</strong>
<ul>
<li>“When choosing a positional encoding method, several factors come into play.”</li>
<li>Mention sequence length, computational cost, and task dependence, giving examples: “For very long sequences, relative encodings may be more effective. Some methods can be computationally expensive. For machine translation absolute position may matter more than document summarization.”</li>
</ul></li>
<li><strong>Communication Tips:</strong>
<ul>
<li><strong>Pace Yourself:</strong> Speak clearly and at a moderate pace, especially when explaining the mathematical details.</li>
<li><strong>Visual Aids (if possible):</strong> If you are in a virtual interview, consider having a slide or document prepared with the key equations. You can ask if it’s okay to share your screen briefly.</li>
<li><strong>Check for Understanding:</strong> After presenting a complex section, pause and ask, “Does that make sense?” or “Would you like me to elaborate on any part of that?” This shows engagement and ensures the interviewer is following along.</li>
<li><strong>Avoid Jargon:</strong> While demonstrating expertise is important, avoid unnecessary jargon. Explain concepts in a straightforward manner.</li>
<li><strong>Be Prepared to Go Deeper:</strong> The interviewer might ask follow-up questions about specific aspects, so be ready to provide more detail or examples.</li>
</ul></li>
</ol>
<p>By following this approach, you can deliver a comprehensive and insightful answer that showcases your expertise in positional encoding and self-attention mechanisms, while also demonstrating strong communication skills.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>