<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>attention_mechanism__self_attention__multi_head_attention__8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-9.-in-multi-head-attention-after-computing-attention-for-all-heads-how-are-the-outputs-combined-and-what-design-considerations-come-into-play-regarding-dimensionality" class="level2">
<h2 class="anchored" data-anchor-id="question-9.-in-multi-head-attention-after-computing-attention-for-all-heads-how-are-the-outputs-combined-and-what-design-considerations-come-into-play-regarding-dimensionality">Question: 9. In multi-head attention, after computing attention for all heads, how are the outputs combined and what design considerations come into play regarding dimensionality?</h2>
<p><strong>Best Answer</strong></p>
<p>Multi-head attention enhances the standard self-attention mechanism by allowing the model to attend to information from different representation subspaces at different positions. After computing the attention outputs for each head, a specific process is followed to combine these outputs into a unified representation. This combination process and the related dimensionality design considerations are crucial for the model’s performance.</p>
<p><strong>Detailed Explanation</strong></p>
<ol type="1">
<li><p><strong>Attention Calculation in Each Head:</strong></p>
<p>In multi-head attention, the input is projected into multiple sets of query (<span class="math inline">\(Q\)</span>), key (<span class="math inline">\(K\)</span>), and value (<span class="math inline">\(V\)</span>) matrices. For each head <span class="math inline">\(i\)</span>, we have:</p>
<p><span class="math display">\[
Q_i = XW_i^Q, \quad K_i = XW_i^K, \quad V_i = XW_i^V
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the input, and <span class="math inline">\(W_i^Q\)</span>, <span class="math inline">\(W_i^K\)</span>, and <span class="math inline">\(W_i^V\)</span> are the projection matrices for head <span class="math inline">\(i\)</span>. The attention output for each head is then calculated as:</p>
<p><span class="math display">\[
\text{Attention}_i = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i
\]</span></p>
<p>Here, <span class="math inline">\(d_k\)</span> is the dimension of the keys (<span class="math inline">\(K_i\)</span>), and the scaling by <span class="math inline">\(\sqrt{d_k}\)</span> prevents the softmax from becoming too peaked, which can hinder learning.</p></li>
<li><p><strong>Concatenation of Heads:</strong></p>
<p>After computing the attention outputs for each head, the outputs are concatenated along the last dimension (usually the feature dimension). Suppose we have <span class="math inline">\(h\)</span> heads, and each head produces an output of dimension <span class="math inline">\(d_v\)</span>. Then, the concatenated output will have a dimension of <span class="math inline">\(h \cdot d_v\)</span>. Mathematically:</p>
<p><span class="math display">\[
\text{Concatenated Output} = \text{Concat}(\text{Attention}_1, \text{Attention}_2, ..., \text{Attention}_h)
\]</span></p></li>
<li><p><strong>Linear Transformation:</strong></p>
<p>Following concatenation, a linear transformation is applied to project the concatenated output back to the desired output dimension. This involves multiplying the concatenated output by a weight matrix <span class="math inline">\(W^O\)</span>:</p>
<p><span class="math display">\[
\text{Final Output} = \text{Concatenated Output} \cdot W^O
\]</span></p>
<p>Here, <span class="math inline">\(W^O\)</span> is a learned weight matrix that maps the concatenated dimension (<span class="math inline">\(h \cdot d_v\)</span>) back to the model’s desired output dimension (<span class="math inline">\(d_{\text{model}}\)</span>). So, <span class="math inline">\(W^O\)</span> has dimensions <span class="math inline">\((h \cdot d_v) \times d_{\text{model}}\)</span>.</p></li>
<li><p><strong>Dimensionality Considerations:</strong></p>
<ul>
<li><p><strong>Maintaining Dimensional Consistency:</strong> It is crucial to ensure that the input and output dimensions of the multi-head attention layer are consistent with the rest of the network. This often means that the output dimension <span class="math inline">\(d_{\text{model}}\)</span> is equal to the input dimension of <span class="math inline">\(X\)</span>. This consistency allows the multi-head attention layer to be easily integrated into deeper architectures, such as the Transformer, where residual connections are used.</p></li>
<li><p><strong>Dimensionality Reduction/Expansion Trade-offs:</strong> The choice of the number of heads (<span class="math inline">\(h\)</span>) and the dimension of each head (<span class="math inline">\(d_v\)</span>) involves a trade-off. One can choose to reduce the dimensionality in each head (i.e., <span class="math inline">\(d_v &lt; d_{\text{model}}\)</span>) to reduce the computational cost. However, this may limit the representation capacity of each head. Conversely, increasing the number of heads can allow the model to capture more diverse relationships in the data, but it also increases the computational cost.</p></li>
<li><p><strong>Computational Complexity:</strong> The computational complexity of multi-head attention is <span class="math inline">\(O(n^2d)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length and <span class="math inline">\(d\)</span> is the dimensionality. The number of heads affects the constant factor in this complexity, but not the overall order. Therefore, choosing an appropriate number of heads and the dimension of each head is essential for balancing performance and computational efficiency.</p></li>
<li><p><strong>Expressiveness:</strong> Each head can learn different attention patterns. More heads allow for more diverse patterns, potentially capturing more complex relationships. However, there is a point of diminishing returns, where adding more heads does not significantly improve performance. This depends on the complexity of the data and the task.</p></li>
<li><p><strong>Overfitting:</strong> A large number of heads, each with a large dimension, can lead to overfitting, especially if the training dataset is small. Regularization techniques, such as dropout, are often used to mitigate this.</p></li>
</ul></li>
</ol>
<p><strong>Real-World Considerations</strong></p>
<ul>
<li><strong>Implementation Details:</strong> In practice, the projection matrices <span class="math inline">\(W_i^Q\)</span>, <span class="math inline">\(W_i^K\)</span>, <span class="math inline">\(W_i^V\)</span>, and <span class="math inline">\(W^O\)</span> are often implemented using linear layers in deep learning frameworks (e.g., PyTorch, TensorFlow). These layers automatically handle the weight initialization and optimization during training.</li>
<li><strong>Optimization:</strong> The choice of optimizer (e.g., Adam, SGD) and learning rate can significantly affect the training of multi-head attention layers. It is common to use learning rate scheduling techniques (e.g., warm-up followed by decay) to improve convergence.</li>
<li><strong>Hardware Constraints:</strong> The size of the input sequence and the dimensionality of the attention layers can be limited by the available memory on the GPU or TPU. Techniques such as gradient accumulation and mixed-precision training can be used to overcome these limitations.</li>
<li><strong>Specialized Architectures:</strong> There are variations of multi-head attention, such as grouped query attention or sparse attention, that aim to reduce the computational cost while maintaining performance. These architectures are particularly useful for very long sequences.</li>
</ul>
<p>In summary, combining the outputs of multi-head attention involves concatenating the attention outputs from each head and then applying a linear transformation to project the concatenated output back to the desired dimension. The design considerations regarding dimensionality involve balancing computational cost, representation capacity, and the risk of overfitting. These choices depend on the specific task, dataset, and hardware constraints.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Purpose of Multi-Head Attention:</strong></p>
<ul>
<li>“Multi-head attention is designed to allow the model to attend to different aspects of the input at different positions, capturing a richer set of relationships than single-head attention.”</li>
</ul></li>
<li><p><strong>Explain the Attention Calculation in Each Head:</strong></p>
<ul>
<li>“First, the input is projected into multiple query, key, and value spaces, one set for each head. So, for each head, we have query, key, and value matrices, which are obtained by multiplying the input by respective weight matrices.”</li>
<li>“Mathematically, we can represent this as <span class="math inline">\(Q_i = XW_i^Q\)</span>, <span class="math inline">\(K_i = XW_i^K\)</span>, and <span class="math inline">\(V_i = XW_i^V\)</span>, where <span class="math inline">\(X\)</span> is the input, and <span class="math inline">\(W_i^Q\)</span>, <span class="math inline">\(W_i^K\)</span>, and <span class="math inline">\(W_i^V\)</span> are the projection matrices for head <span class="math inline">\(i\)</span>.” <em>Write this on the whiteboard if available.</em></li>
<li>“Then, for each head, attention scores are computed, usually using scaled dot-product attention: <span class="math inline">\(\text{Attention}_i = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i\)</span>.” <em>Write this on the whiteboard if available.</em></li>
<li>“The scaling by <span class="math inline">\(\sqrt{d_k}\)</span> is important to prevent the softmax from becoming too peaked when <span class="math inline">\(d_k\)</span> is large, which can hinder learning.”</li>
</ul></li>
<li><p><strong>Describe the Concatenation Process:</strong></p>
<ul>
<li>“After computing the attention output for each head, these outputs are concatenated along the feature dimension. So, if we have <span class="math inline">\(h\)</span> heads, each producing an output of dimension <span class="math inline">\(d_v\)</span>, the concatenated output will have a dimension of <span class="math inline">\(h \cdot d_v\)</span>.”</li>
<li>“In mathematical terms: <span class="math inline">\(\text{Concatenated Output} = \text{Concat}(\text{Attention}_1, \text{Attention}_2, ..., \text{Attention}_h)\)</span>.” <em>Write this on the whiteboard if available.</em></li>
</ul></li>
<li><p><strong>Explain the Linear Transformation:</strong></p>
<ul>
<li>“Following concatenation, a linear transformation is applied to project the concatenated output back to the desired output dimension. This is done by multiplying the concatenated output by a weight matrix <span class="math inline">\(W^O\)</span>.”</li>
<li>“So, the final output is <span class="math inline">\(\text{Final Output} = \text{Concatenated Output} \cdot W^O\)</span>.” <em>Write this on the whiteboard if available.</em></li>
<li>“Here, <span class="math inline">\(W^O\)</span> has dimensions <span class="math inline">\((h \cdot d_v) \times d_{\text{model}}\)</span>, where <span class="math inline">\(d_{\text{model}}\)</span> is the model’s desired output dimension.”</li>
</ul></li>
<li><p><strong>Discuss Dimensionality Considerations (Most Important Part):</strong></p>
<ul>
<li>“There are several important considerations when designing the dimensions of the multi-head attention layer. First, maintaining dimensional consistency with the rest of the network is key. Typically, you want the output dimension to match the input dimension.”</li>
<li>“There’s a trade-off between the number of heads and the dimension of each head. Reducing the dimensionality in each head can reduce computational cost, but it might limit the representation capacity. Increasing the number of heads allows the model to capture more diverse relationships, but also increases the computational cost and the risk of overfitting. Finding the right balance is crucial.”</li>
<li>“The number of heads impacts the expressiveness of the model; each head can learn different attention patterns. But there’s a point of diminishing returns. More heads aren’t always better and can increase overfitting, especially on smaller datasets.”</li>
</ul></li>
<li><p><strong>Mention Real-World Considerations (If Time Permits):</strong></p>
<ul>
<li>“In practice, these projection matrices are implemented using linear layers in deep learning frameworks. Optimization is crucial, and techniques like learning rate scheduling are often employed.”</li>
<li>“Hardware limitations, such as GPU memory, can also influence the choice of dimensionality. Techniques like gradient accumulation or mixed-precision training might be necessary.”</li>
<li>“There are also specialized architectures, like grouped query attention, designed to improve efficiency for very long sequences.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Explain the concepts step by step. Avoid rushing through the mathematical notations.</li>
<li><strong>Use Visual Aids:</strong> If a whiteboard is available, use it to illustrate the mathematical notations and the flow of data.</li>
<li><strong>Check for Understanding:</strong> Pause after each major point and ask if the interviewer has any questions.</li>
<li><strong>Focus on Trade-offs:</strong> Emphasize the trade-offs involved in dimensionality design, such as the balance between computational cost, representation capacity, and the risk of overfitting.</li>
<li><strong>Be Practical:</strong> Relate the concepts to real-world implementation details and optimization techniques.</li>
</ul>
<p>By following these steps, you can deliver a comprehensive and clear explanation of multi-head attention, demonstrating your expertise in the topic.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>