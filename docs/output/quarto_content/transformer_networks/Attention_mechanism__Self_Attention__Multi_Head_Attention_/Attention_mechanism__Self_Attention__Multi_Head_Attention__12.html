<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>attention_mechanism__self_attention__multi_head_attention__12</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-13.-can-you-describe-a-scenario-where-the-self-attention-mechanism-might-fail-or-perform-suboptimally-what-strategies-might-you-consider-to-mitigate-these-issues" class="level2">
<h2 class="anchored" data-anchor-id="question-13.-can-you-describe-a-scenario-where-the-self-attention-mechanism-might-fail-or-perform-suboptimally-what-strategies-might-you-consider-to-mitigate-these-issues">Question: 13. Can you describe a scenario where the self-attention mechanism might fail or perform suboptimally? What strategies might you consider to mitigate these issues?</h2>
<p><strong>Best Answer</strong></p>
<p>The self-attention mechanism, while powerful, is not without limitations. Several scenarios can lead to its failure or suboptimal performance. These primarily revolve around computational complexity with long sequences, difficulties capturing positional information, and potential biases in attention weights.</p>
<p><strong>1. Computational Complexity with Long Sequences:</strong></p>
<p>The core of self-attention lies in calculating attention weights between every pair of tokens in a sequence. Given a sequence of length <span class="math inline">\(n\)</span>, the computational complexity is <span class="math inline">\(O(n^2)\)</span>. This quadratic scaling becomes a bottleneck for very long sequences, such as those encountered in document summarization, long-form question answering, or processing entire books. The memory requirements also grow quadratically, limiting the sequence length that can be processed.</p>
<ul>
<li><p><strong>Why it matters:</strong> Training and inference become prohibitively expensive for long sequences.</p></li>
<li><p><strong>Mathematical Explanation:</strong> The attention mechanism calculates attention weights as follows:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> are the query, key, and value matrices, respectively, and <span class="math inline">\(d_k\)</span> is the dimensionality of the keys. The <span class="math inline">\(QK^T\)</span> term explicitly shows the <span class="math inline">\(O(n^2)\)</span> complexity. Each dot product between a query vector and all the key vectors in the sequence contributes to this quadratic scaling.</p></li>
</ul>
<p><strong>2. Difficulty in Modeling Positional Information:</strong></p>
<p>The original self-attention mechanism is permutation-invariant. This means the output is the same regardless of the order of the input tokens. While positional embeddings are added to inject positional information, they might not fully capture complex positional relationships, especially when the model needs to understand hierarchical or long-range dependencies reliant on precise token order. Relative positional encoding and learned positional encodings are proposed to better represent positional information.</p>
<ul>
<li><strong>Why it matters:</strong> Natural language is highly dependent on word order. Without accurate positional information, understanding sentence structure, logical flow, and relationships between entities becomes difficult.</li>
<li><strong>Example:</strong> Consider the phrases “man bites dog” and “dog bites man”. Without positional understanding, a model might incorrectly interpret these phrases as having the same meaning.</li>
</ul>
<p><strong>3. Overemphasis on Certain Tokens/Lack of Diversity in Attention:</strong></p>
<p>In some cases, the attention mechanism may overly focus on a small subset of tokens, ignoring other potentially relevant parts of the sequence. This can lead to a lack of diversity in the information aggregated by the attention mechanism, resulting in suboptimal representations. This can also lead to the model being brittle and sensitive to specific inputs.</p>
<ul>
<li><strong>Why it matters:</strong> Over-reliance on a few tokens can limit the model’s ability to capture the full context and nuances of the input sequence.</li>
<li><strong>Mitigation Strategy:</strong> Techniques like attention dropout can introduce noise to the attention weights, encouraging the model to attend to a wider range of tokens.</li>
</ul>
<p><strong>4. Vanishing Attention for Long-Range Dependencies:</strong></p>
<p>While self-attention is designed to capture long-range dependencies, in extremely long sequences, the attention weights can become diluted, making it difficult for the model to effectively attend to distant tokens. The softmax function can result in very small attention weights for many tokens, effectively diminishing their contribution.</p>
<ul>
<li><strong>Why it matters:</strong> Many NLP tasks require understanding relationships between distant parts of a document, such as resolving coreference, identifying argumentative structures, or summarizing long texts.</li>
<li><strong>Mathematical Explanation:</strong> As sequence length increases, the softmax function applied to the scaled dot-product attention scores can become very peaked, with a few tokens receiving almost all the attention and the rest receiving negligible attention.</li>
</ul>
<p><strong>Mitigation Strategies:</strong></p>
<p>To address these limitations, several strategies have been developed:</p>
<ol type="1">
<li><p><strong>Sparse Attention Mechanisms:</strong> These techniques reduce the computational complexity by only attending to a subset of the tokens. Examples include:</p>
<ul>
<li><strong>Windowed Attention:</strong> Attending only to tokens within a fixed-size window around each token.</li>
<li><strong>Strided Attention:</strong> Attending to tokens at regular intervals.</li>
<li><strong>Longformer:</strong> Combines windowed attention, dilated sliding window attention, and global attention for specific tokens. Reduces complexity from <span class="math inline">\(O(n^2)\)</span> to <span class="math inline">\(O(n)\)</span>.</li>
<li><strong>BigBird:</strong> Uses random attention, global attention, and window attention to approximate full attention with <span class="math inline">\(O(n)\)</span> complexity.</li>
</ul></li>
<li><p><strong>Attention Masking:</strong> Preventing the model from attending to certain tokens, such as padding tokens or tokens in the future (in causal language modeling). This helps focus attention on relevant parts of the sequence and improves efficiency.</p></li>
<li><p><strong>Positional Encoding Refinements:</strong> Employing more sophisticated positional encoding schemes, such as:</p>
<ul>
<li><strong>Relative Positional Encodings:</strong> Encoding the relative distances between tokens rather than absolute positions. This allows the model to better generalize to sequences of different lengths.</li>
<li><strong>Learned Positional Encodings:</strong> Learning the positional embeddings directly from the data, allowing the model to adapt the positional representations to the specific task.</li>
</ul></li>
<li><p><strong>Combining Attention with Other Architectures:</strong> Hybrid models that combine self-attention with other architectural components, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs).</p>
<ul>
<li><strong>CNNs:</strong> Can capture local dependencies efficiently and provide a strong inductive bias for translational invariance.</li>
<li><strong>RNNs:</strong> Can process sequential data in a step-by-step manner, capturing temporal dependencies.</li>
</ul></li>
<li><p><strong>Attention Dropout:</strong> Applying dropout to the attention weights during training can encourage the model to attend to a wider range of tokens and prevent over-reliance on a few specific tokens. This is a regularization technique.</p></li>
<li><p><strong>Kernel Methods for Attention (e.g., Transformers with Gaussian Kernels):</strong> Replacing the dot-product attention with kernel functions can provide more flexible and robust attention mechanisms. These can also be combined with other techniques like low-rank approximations to reduce computational complexity.</p></li>
<li><p><strong>Linearized Attention:</strong> Approximating the attention mechanism with linear computations to achieve linear complexity w.r.t sequence length. Examples: Linformer, Performer</p></li>
</ol>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to present this information in an interview:</p>
<ol type="1">
<li><p><strong>Start with a concise summary:</strong> “While self-attention is a powerful mechanism, it has limitations, especially with long sequences and positional information. These limitations can lead to suboptimal performance or even failure in certain scenarios.”</p></li>
<li><p><strong>Address each limitation one by one:</strong></p>
<ul>
<li>“One major issue is the <span class="math inline">\(O(n^2)\)</span> computational complexity. This arises because each token attends to every other token, making it impractical for very long sequences. The core formula, <span class="math inline">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span>, clearly shows the quadratic relationship with sequence length.”
<ul>
<li><em>Communication Tip:</em> When presenting the equation, briefly explain each term (<span class="math inline">\(Q, K, V\)</span>) and emphasize how the <span class="math inline">\(QK^T\)</span> term drives the complexity. Don’t dive into excessive detail unless asked.</li>
</ul></li>
<li>“Another challenge is accurately modeling positional information. While positional embeddings are used, the mechanism is fundamentally permutation-invariant and struggles with complex hierarchical structures where the precise order matters.” Give a simple “man bites dog” example.</li>
<li>“Sometimes, the model can overemphasize certain tokens, ignoring others. This lack of diversity in attention can limit the model’s understanding of the full context.”</li>
<li>“Finally, in extremely long sequences, attention weights can become diluted, making it difficult to capture long-range dependencies.”</li>
</ul></li>
<li><p><strong>Transition to mitigation strategies:</strong> “To address these issues, several strategies have been developed. These can broadly be categorized as methods for reducing computational complexity, improving positional encoding, and encouraging more diverse attention.”</p></li>
<li><p><strong>Describe the mitigation strategies:</strong></p>
<ul>
<li>“Sparse attention mechanisms, like Longformer and BigBird, reduce the complexity to approximately <span class="math inline">\(O(n)\)</span> by attending only to a subset of tokens using techniques like windowed attention and dilated sliding windows.” Briefly describe Longformer/BigBird ideas.</li>
<li>“Attention masking prevents the model from attending to irrelevant tokens like padding.”</li>
<li>“More sophisticated positional encoding schemes, such as relative positional encodings, can better capture positional relationships.”</li>
<li>“Hybrid models combine attention with CNNs or RNNs to leverage their respective strengths.”</li>
<li>“Attention dropout can regularize the attention weights and prevent over-reliance on a few tokens.”</li>
</ul></li>
<li><p><strong>Conclude with a summary:</strong> “By carefully considering these limitations and employing appropriate mitigation strategies, we can leverage the power of self-attention while avoiding its pitfalls.”</p></li>
</ol>
<ul>
<li><strong>Communication Tips:</strong>
<ul>
<li>Use a clear and structured approach.</li>
<li>Pace yourself. Don’t rush through the explanation.</li>
<li>Use visual aids (if available) to illustrate the attention mechanism and different mitigation strategies.</li>
<li>Be prepared to answer follow-up questions about specific techniques or applications.</li>
<li>Avoid jargon unless you are confident that the interviewer understands it.</li>
<li>Show enthusiasm for the topic.</li>
<li>Relate your answers to real-world applications or projects whenever possible. This demonstrates practical experience and a deeper understanding of the subject.</li>
</ul></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>