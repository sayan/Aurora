<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>key_differences_between_rnn__cnn_based_models_and_transformers_6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-7.-discuss-the-training-challenges-associated-with-each-of-these-models.-how-do-issues-like-vanishing-gradients-overfitting-or-computational-costs-manifest-in-rnns-cnns-and-transformers" class="level2">
<h2 class="anchored" data-anchor-id="question-7.-discuss-the-training-challenges-associated-with-each-of-these-models.-how-do-issues-like-vanishing-gradients-overfitting-or-computational-costs-manifest-in-rnns-cnns-and-transformers">Question: 7. Discuss the training challenges associated with each of these models. How do issues like vanishing gradients, overfitting, or computational costs manifest in RNNs, CNNs, and Transformers?</h2>
<p><strong>Best Answer</strong></p>
<p>Training deep learning models, whether they are Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), or Transformers, comes with its own set of challenges. These challenges often manifest as vanishing gradients, overfitting, or high computational costs. Let’s examine each of these models and the specific challenges they face:</p>
<section id="recurrent-neural-networks-rnns" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-networks-rnns">1. Recurrent Neural Networks (RNNs)</h3>
<ul>
<li><strong>Vanishing Gradients:</strong>
<ul>
<li><strong>Problem:</strong> In standard RNNs, the gradient signal can diminish exponentially as it is backpropagated through time. This makes it difficult for the network to learn long-range dependencies, as the weights in earlier layers receive little to no update.</li>
<li><strong>Mathematical Explanation:</strong> During backpropagation through time (BPTT), the gradients are computed by multiplying the derivatives through each time step. If these derivatives are consistently less than 1, repeated multiplication causes the gradient to shrink towards zero. <span class="math display">\[
\frac{\partial L}{\partial W} = \sum_{t=1}^{T} \frac{\partial L}{\partial y_t} \frac{\partial y_t}{\partial h_t} \frac{\partial h_t}{\partial h_{t-1}} \cdots \frac{\partial h_1}{\partial W}
\]</span> Where L is the loss, <span class="math inline">\(W\)</span> represents the weights, <span class="math inline">\(y_t\)</span> is the output at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(h_t\)</span> is the hidden state at time <span class="math inline">\(t\)</span>. The term <span class="math inline">\(\frac{\partial h_t}{\partial h_{t-1}}\)</span> contains the repeated multiplication.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit):</strong> These architectures introduce gating mechanisms that allow the network to selectively remember or forget information over long sequences. The gates help to maintain a more stable gradient flow. For example, LSTMs use input, forget, and output gates to control the cell state.</li>
<li><strong>Gradient Clipping:</strong> This technique involves scaling the gradients when their norm exceeds a predefined threshold, preventing them from becoming excessively large and contributing to instability.</li>
<li><strong>Initialization Strategies:</strong> Using appropriate weight initialization techniques (e.g., Xavier/Glorot or He initialization) can help to keep the initial gradients within a reasonable range.</li>
</ul></li>
</ul></li>
<li><strong>Exploding Gradients:</strong>
<ul>
<li><strong>Problem:</strong> Though less common than vanishing gradients, exploding gradients occur when the gradients become excessively large during training, leading to unstable updates and potentially divergence.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Gradient Clipping:</strong> The most common solution, where gradients exceeding a certain threshold are scaled down.</li>
<li><strong>Regularization:</strong> L1 or L2 regularization can help prevent weights from growing too large.</li>
</ul></li>
</ul></li>
<li><strong>Overfitting:</strong>
<ul>
<li><strong>Problem:</strong> RNNs can overfit to the training data, particularly when the model is complex or the dataset is small.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Dropout:</strong> Randomly dropping out neurons during training can prevent the network from relying too heavily on specific features.</li>
<li><strong>Regularization (L1/L2):</strong> Adding regularization terms to the loss function penalizes large weights and encourages simpler models.</li>
<li><strong>Early Stopping:</strong> Monitoring the performance on a validation set and stopping training when the validation loss starts to increase.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="convolutional-neural-networks-cnns" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-networks-cnns">2. Convolutional Neural Networks (CNNs)</h3>
<ul>
<li><strong>Overfitting:</strong>
<ul>
<li><strong>Problem:</strong> CNNs, especially deep ones with a large number of parameters, are prone to overfitting, especially when the training dataset is relatively small. The network can memorize the training examples rather than learning generalizable features.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Data Augmentation:</strong> Increasing the size of the training dataset by applying various transformations to the existing images (e.g., rotations, translations, flips, and scaling).</li>
<li><strong>Dropout:</strong> Randomly dropping out neurons during training.</li>
<li><strong>Regularization (L1/L2):</strong> Adding regularization terms to the loss function.</li>
<li><strong>Batch Normalization:</strong> Normalizing the activations within each batch can help to stabilize training and reduce overfitting.</li>
<li><strong>Early Stopping:</strong> Monitoring performance on a validation set.</li>
</ul></li>
</ul></li>
<li><strong>Computational Costs:</strong>
<ul>
<li><strong>Problem:</strong> Deep CNNs can be computationally expensive to train, especially with high-resolution images and large batch sizes. The number of parameters and the complexity of the convolutional operations contribute to this cost.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Smaller Kernel Sizes:</strong> Using smaller convolutional kernels reduces the number of parameters and computations.</li>
<li><strong>Strided Convolutions and Pooling:</strong> Using strided convolutions or pooling layers (e.g., max pooling) reduces the spatial dimensions of the feature maps, decreasing the computational load.</li>
<li><strong>Depthwise Separable Convolutions:</strong> These convolutions reduce the number of parameters compared to standard convolutions by separating the spatial and channel-wise computations. MobileNet uses this extensively.</li>
<li><strong>Model Compression Techniques:</strong> Techniques such as pruning (removing less important connections) and quantization (reducing the precision of weights) can reduce the model size and computational requirements.</li>
<li><strong>Distributed Training:</strong> Distributing the training workload across multiple GPUs or machines can significantly speed up the training process.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="transformers" class="level3">
<h3 class="anchored" data-anchor-id="transformers">3. Transformers</h3>
<ul>
<li><strong>Computational Costs:</strong>
<ul>
<li><strong>Problem:</strong> The self-attention mechanism in Transformers has a quadratic complexity with respect to the sequence length <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length. This makes training Transformers on long sequences computationally expensive and memory-intensive. This complexity arises because each token needs to attend to every other token in the sequence.</li>
<li><strong>Mathematical Explanation:</strong> The attention mechanism calculates attention weights between each pair of tokens. This involves computing a score matrix of size <span class="math inline">\((n \times n)\)</span>, where each element represents the attention score between two tokens. This quadratic scaling is a major bottleneck for long sequences. <span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span> Where <span class="math inline">\(Q\)</span> is the query matrix, <span class="math inline">\(K\)</span> is the key matrix, <span class="math inline">\(V\)</span> is the value matrix, and <span class="math inline">\(d_k\)</span> is the dimension of the keys. The <span class="math inline">\(QK^T\)</span> operation results in the <span class="math inline">\(n \times n\)</span> matrix, where <span class="math inline">\(n\)</span> is the sequence length.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Sparse Attention:</strong> Instead of attending to all tokens, only attend to a subset of tokens based on certain criteria. This reduces the computational complexity. Examples include:
<ul>
<li><strong>Local Attention:</strong> Attending only to a fixed window of tokens around each token.</li>
<li><strong>Global Attention:</strong> Attending to a small set of global tokens for the entire sequence.</li>
<li><strong>Longformer:</strong> Combines local and global attention.</li>
</ul></li>
<li><strong>Linear Attention:</strong> Approximates the attention mechanism with linear complexity <span class="math inline">\(O(n)\)</span>. Reformer does this.</li>
<li><strong>Knowledge Distillation:</strong> Training a smaller, more efficient model to mimic the behavior of a larger Transformer model.</li>
<li><strong>Mixed Precision Training:</strong> Using lower precision (e.g., FP16) for computations can reduce memory usage and speed up training.</li>
<li><strong>Gradient Checkpointing:</strong> Reduces memory consumption by recomputing activations during the backward pass instead of storing them.</li>
</ul></li>
</ul></li>
<li><strong>Overfitting:</strong>
<ul>
<li><strong>Problem:</strong> Transformers, especially large ones with billions of parameters, are prone to overfitting if not trained carefully.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Data Augmentation:</strong> While less common for text data than images, techniques like back-translation and synonym replacement can be used.</li>
<li><strong>Regularization (Weight Decay):</strong> Adding a weight decay term to the loss function.</li>
<li><strong>Dropout:</strong> Applying dropout to the attention weights or the feedforward layers.</li>
<li><strong>Early Stopping:</strong> Monitoring the validation loss and stopping training when it starts to increase.</li>
<li><strong>Pre-training:</strong> Training the model on a large, general-purpose dataset before fine-tuning it on a specific task. This helps the model learn general language representations and reduces the risk of overfitting to the smaller task-specific dataset.</li>
</ul></li>
</ul></li>
<li><strong>Vanishing Gradients:</strong>
<ul>
<li><strong>Problem:</strong> While Transformers mitigate the vanishing gradient problem compared to standard RNNs due to the self-attention mechanism providing direct connections between all tokens, very deep Transformers can still suffer from vanishing gradients.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Residual Connections:</strong> Transformers heavily rely on residual connections, which help the gradient flow more easily through the network.</li>
<li><strong>Layer Normalization:</strong> Normalizing the activations within each layer can stabilize training and improve gradient flow.</li>
<li><strong>Careful Initialization:</strong> Using proper initialization techniques can mitigate the issue.</li>
</ul></li>
</ul></li>
</ul>
<p>In summary, each of these models has its own unique set of training challenges. Understanding these challenges and the various techniques to mitigate them is crucial for successfully training these models and achieving state-of-the-art performance on various tasks.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a General Overview:</strong></p>
<ul>
<li>“Each of these models – RNNs, CNNs, and Transformers – presents unique training challenges due to their architecture. These challenges often manifest as vanishing gradients, overfitting, or high computational costs. I’ll discuss each model and how these issues arise and are addressed.”</li>
</ul></li>
<li><p><strong>Discuss RNNs:</strong></p>
<ul>
<li>“RNNs are particularly susceptible to the vanishing gradient problem because of how gradients are backpropagated through time. As the gradient signal passes through multiple time steps, it can diminish exponentially. The problem occurs when the derivatives used in backpropagation are consistently less than 1. When these small derivatives are multiplied across many time steps, the gradient shrinks drastically, preventing the earlier layers from learning effectively.”</li>
<li><em>Optionally, write the gradient equation on the whiteboard to illustrate the repeated multiplication, but only if prompted or if the interviewer seems very technically focused.</em>
<ul>
<li>“Here’s the equation for BPTT. Notice the product of partial derivatives, which shrinks towards zero if the derivatives are less than 1.”</li>
</ul></li>
<li>“LSTM and GRU networks mitigate this by introducing gating mechanisms to better control the flow of information and maintain a stable gradient. Additionally, gradient clipping can prevent exploding gradients.”</li>
<li>“RNNs are also prone to overfitting, so dropout, regularization, and early stopping are common techniques used to combat that.”</li>
</ul></li>
<li><p><strong>Discuss CNNs:</strong></p>
<ul>
<li>“CNNs, especially deep networks, tend to overfit when the training dataset is small. Data augmentation, dropout, regularization, batch normalization, and early stopping are commonly used to address this.”</li>
<li>“Deep CNNs can also be computationally expensive to train. Techniques to mitigate this include using smaller kernels, strided convolutions/pooling, depthwise separable convolutions, model compression, and distributed training.”</li>
</ul></li>
<li><p><strong>Discuss Transformers:</strong></p>
<ul>
<li>“Transformers face challenges primarily due to the computational cost of the self-attention mechanism, which scales quadratically with the sequence length. This complexity stems from the attention mechanism’s need to compute attention weights between each pair of tokens. For long sequences, this becomes very expensive.”</li>
<li><em>Consider briefly showing the attention equation if the interviewer is engaged and technically focused.</em>
<ul>
<li>“This equation illustrates the matrix multiplication that leads to the quadratic complexity.”</li>
</ul></li>
<li>“To address this, techniques like sparse attention and linear attention have been developed to reduce the complexity. Also, knowledge distillation helps to create smaller, more efficient models.”</li>
<li>“Transformers can overfit, which is mitigated using data augmentation, regularization, dropout, early stopping, and pre-training.”</li>
<li>“While Transformers are better at handling vanishing gradients compared to RNNs, they can still occur in very deep architectures. Residual connections and layer normalization help to maintain gradient flow.”</li>
</ul></li>
<li><p><strong>Concluding Remarks:</strong></p>
<ul>
<li>“In summary, each model presents unique training challenges. Understanding these challenges and applying the appropriate mitigation techniques is essential for successful model training and achieving high performance.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you’re in a virtual interview, consider sharing your screen to show relevant diagrams or equations. If you’re in person and there’s a whiteboard, use it to illustrate key concepts.</li>
<li><strong>Check for Understanding:</strong> Pause occasionally to ask if the interviewer has any questions or wants you to elaborate on a specific point.</li>
<li><strong>Adapt to the Interviewer’s Level:</strong> If the interviewer seems less technically inclined, focus on the high-level concepts and avoid getting bogged down in the mathematical details. If they seem very knowledgeable, you can delve deeper into the technical aspects.</li>
<li><strong>Be Confident:</strong> Speak clearly and confidently, demonstrating your expertise in the subject matter.</li>
<li><strong>Stay Practical:</strong> Connect the theoretical aspects to real-world considerations whenever possible.</li>
<li><strong>Enthusiasm:</strong> Show enthusiasm for the topic.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>