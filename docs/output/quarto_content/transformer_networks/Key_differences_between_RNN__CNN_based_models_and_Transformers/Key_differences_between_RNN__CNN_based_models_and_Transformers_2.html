<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>key_differences_between_rnn__cnn_based_models_and_transformers_2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-3.-mathematically-how-do-the-convolution-operation-in-cnns-recurrence-in-rnns-and-self-attention-mechanisms-in-transformers-differ-in-terms-of-complexity-and-operation" class="level2">
<h2 class="anchored" data-anchor-id="question-3.-mathematically-how-do-the-convolution-operation-in-cnns-recurrence-in-rnns-and-self-attention-mechanisms-in-transformers-differ-in-terms-of-complexity-and-operation">Question: 3. Mathematically, how do the convolution operation in CNNs, recurrence in RNNs, and self-attention mechanisms in Transformers differ in terms of complexity and operation?</h2>
<p><strong>Best Answer</strong></p>
<p>The core difference between Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers lies in how they process input data, particularly with respect to capturing spatial or temporal dependencies. This is reflected in their mathematical formulations and computational complexities.</p>
<p><strong>1. Convolutional Neural Networks (CNNs)</strong></p>
<ul>
<li><p><strong>Operation:</strong> CNNs employ convolution operations to extract local features from input data. A convolution involves sliding a filter (or kernel) across the input, performing element-wise multiplication and summation.</p>
<p><span class="math display">\[
(f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) d\tau
\]</span></p>
<p>In the discrete case, for a 2D image, this becomes:</p>
<p><span class="math display">\[
(I * K)(i, j) = \sum_{m} \sum_{n} I(i-m, j-n) K(m, n)
\]</span></p>
<p>Where <span class="math inline">\(I\)</span> is the input image, and <span class="math inline">\(K\)</span> is the convolution kernel.</p></li>
<li><p><strong>Mathematical Formulation:</strong> The output feature map <span class="math inline">\(O\)</span> of a convolutional layer can be represented as:</p>
<p><span class="math display">\[
O_{i,j,k} = \sigma\left(\sum_{c=1}^{C_{in}} \sum_{m=0}^{F_H-1} \sum_{n=0}^{F_W-1} I_{i+m, j+n, c} \cdot K_{m, n, c, k} + b_k\right)
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(O_{i,j,k}\)</span> is the value at position <span class="math inline">\((i, j)\)</span> in the <span class="math inline">\(k\)</span>-th feature map of the output.</li>
<li><span class="math inline">\(\sigma\)</span> is an activation function (e.g., ReLU).</li>
<li><span class="math inline">\(C_{in}\)</span> is the number of input channels.</li>
<li><span class="math inline">\(F_H\)</span> and <span class="math inline">\(F_W\)</span> are the height and width of the filter.</li>
<li><span class="math inline">\(I_{i+m, j+n, c}\)</span> is the input value at position <span class="math inline">\((i+m, j+n)\)</span> in the <span class="math inline">\(c\)</span>-th input channel.</li>
<li><span class="math inline">\(K_{m, n, c, k}\)</span> is the kernel weight at position <span class="math inline">\((m, n)\)</span> for the <span class="math inline">\(c\)</span>-th input channel and <span class="math inline">\(k\)</span>-th output feature map.</li>
<li><span class="math inline">\(b_k\)</span> is the bias term for the <span class="math inline">\(k\)</span>-th output feature map.</li>
</ul></li>
<li><p><strong>Complexity:</strong> The computational complexity of a convolutional layer is approximately <span class="math inline">\(O(N \cdot F_H \cdot F_W \cdot C_{in} \cdot C_{out})\)</span>, where <span class="math inline">\(N\)</span> is the number of positions in the output feature map (<span class="math inline">\(N = H_{out} \cdot W_{out}\)</span>), <span class="math inline">\(F_H\)</span> and <span class="math inline">\(F_W\)</span> are the filter dimensions, <span class="math inline">\(C_{in}\)</span> is the number of input channels, and <span class="math inline">\(C_{out}\)</span> is the number of output channels. Importantly, <span class="math inline">\(F_H\)</span> and <span class="math inline">\(F_W\)</span> are typically small (e.g., 3x3 or 5x5), making the operation relatively efficient for local feature extraction. The convolution operation can be parallelized effectively across different positions in the input.</p></li>
</ul>
<p><strong>2. Recurrent Neural Networks (RNNs)</strong></p>
<ul>
<li><p><strong>Operation:</strong> RNNs process sequential data by maintaining a hidden state that is updated at each time step based on the current input and the previous hidden state.</p></li>
<li><p><strong>Mathematical Formulation:</strong> The update equations for a basic RNN are as follows:</p>
<p><span class="math display">\[
h_t = \sigma(W_{ih}x_t + b_{ih} + W_{hh}h_{t-1} + b_{hh})
\]</span> <span class="math display">\[
y_t = W_{hy}h_t + b_{hy}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x_t\)</span> is the input at time step <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(h_t\)</span> is the hidden state at time step <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(y_t\)</span> is the output at time step <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(W_{ih}\)</span>, <span class="math inline">\(W_{hh}\)</span>, and <span class="math inline">\(W_{hy}\)</span> are the input-to-hidden, hidden-to-hidden, and hidden-to-output weight matrices, respectively.</li>
<li><span class="math inline">\(b_{ih}\)</span> and <span class="math inline">\(b_{hy}\)</span> are the bias vectors.</li>
<li><span class="math inline">\(\sigma\)</span> is an activation function (e.g., tanh).</li>
</ul></li>
<li><p><strong>Complexity:</strong> The computational complexity of an RNN for a sequence of length <span class="math inline">\(T\)</span> is <span class="math inline">\(O(T \cdot (d^2 + d \cdot i + d \cdot o))\)</span>, where <span class="math inline">\(d\)</span> is the hidden state dimension, <span class="math inline">\(i\)</span> is the input dimension, and <span class="math inline">\(o\)</span> is the output dimension. The key aspect here is the <em>sequential</em> processing, which means that the computation at time step <span class="math inline">\(t\)</span> depends on the result from time step <span class="math inline">\(t-1\)</span>. This makes parallelization difficult. Furthermore, RNNs can suffer from vanishing or exploding gradients, especially for long sequences, making them difficult to train. More complex variants like LSTMs and GRUs address these gradient issues, but at the cost of increased computational complexity per time step.</p></li>
</ul>
<p><strong>3. Transformers (Self-Attention)</strong></p>
<ul>
<li><p><strong>Operation:</strong> Transformers rely on self-attention mechanisms to capture relationships between all positions in the input sequence in parallel. This allows them to model long-range dependencies more effectively than RNNs.</p></li>
<li><p><strong>Mathematical Formulation:</strong> The core of the Transformer is the self-attention mechanism. Given a sequence of input embeddings, <span class="math inline">\(X = [x_1, x_2, ..., x_n]\)</span>, the self-attention mechanism computes a weighted sum of these embeddings, where the weights are determined by the relationships between them. This is typically done through a scaled dot-product attention:</p>
<ol type="1">
<li><p><strong>Compute Queries, Keys, and Values:</strong> <span class="math display">\[
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\]</span> where <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, and <span class="math inline">\(W_V\)</span> are learnable weight matrices that project the input embeddings into query, key, and value spaces.</p></li>
<li><p><strong>Compute Attention Weights:</strong> <span class="math display">\[
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span> where <span class="math inline">\(d_k\)</span> is the dimension of the keys (and queries), and the scaling factor <span class="math inline">\(\sqrt{d_k}\)</span> prevents the dot products from becoming too large, which can lead to vanishing gradients after the softmax. The <span class="math inline">\(softmax\)</span> function normalizes the attention scores to produce weights between 0 and 1.</p></li>
<li><p><strong>Multi-Head Attention:</strong> Transformers often use multi-head attention, where the attention mechanism is applied multiple times in parallel with different learned linear projections:</p>
<p><span class="math display">\[
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
\]</span> <span class="math display">\[
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
\]</span> where <span class="math inline">\(W_i^Q\)</span>, <span class="math inline">\(W_i^K\)</span>, <span class="math inline">\(W_i^V\)</span>, and <span class="math inline">\(W^O\)</span> are learnable parameter matrices, and <span class="math inline">\(h\)</span> is the number of heads.</p></li>
</ol></li>
<li><p><strong>Complexity:</strong> The computational complexity of the self-attention mechanism is <span class="math inline">\(O(n^2 \cdot d)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length and <span class="math inline">\(d\)</span> is the dimension of the input embeddings. The <span class="math inline">\(n^2\)</span> term arises from the dot product between the query and key matrices, which requires comparing each position in the sequence with every other position. While this complexity is higher than CNNs and RNNs for short sequences, the ability to parallelize the attention computation makes Transformers much faster for long sequences. Additionally, the direct computation of relationships between all positions allows Transformers to capture long-range dependencies more effectively than RNNs, which are limited by their sequential processing. The computational bottleneck is typically the <span class="math inline">\(n^2\)</span> factor, but techniques like sparse attention attempt to reduce this.</p></li>
</ul>
<p><strong>Summary Table:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 26%">
<col style="width: 30%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>CNN</th>
<th>RNN</th>
<th>Transformer (Self-Attention)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Operation</td>
<td>Convolution with local filters</td>
<td>Recurrent processing of sequential data</td>
<td>Parallel computation of attention weights</td>
</tr>
<tr class="even">
<td>Key Math</td>
<td>Convolution integral/sum</td>
<td>Hidden state update equations</td>
<td>Scaled dot-product attention</td>
</tr>
<tr class="odd">
<td>Complexity</td>
<td><span class="math inline">\(O(N \cdot F_H \cdot F_W \cdot C_{in} \cdot C_{out})\)</span></td>
<td><span class="math inline">\(O(T \cdot (d^2 + d \cdot i + d \cdot o))\)</span></td>
<td><span class="math inline">\(O(n^2 \cdot d)\)</span></td>
</tr>
<tr class="even">
<td>Dependency</td>
<td>Local</td>
<td>Sequential</td>
<td>Global (all positions)</td>
</tr>
<tr class="odd">
<td>Parallelization</td>
<td>High</td>
<td>Limited</td>
<td>High</td>
</tr>
<tr class="even">
<td>Use Cases</td>
<td>Image/Video Processing, Local Patterns</td>
<td>Sequential data, Time Series</td>
<td>NLP, Long-range Dependencies</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to explain these concepts in an interview:</p>
<ol type="1">
<li><strong>Start with the Big Picture:</strong>
<ul>
<li>Begin by highlighting that CNNs, RNNs, and Transformers are fundamental architectures in deep learning, each designed to handle different types of data and dependencies.</li>
<li>Mention that their key differences lie in how they process information and capture spatial/temporal relationships.</li>
</ul></li>
<li><strong>Explain CNNs:</strong>
<ul>
<li>“CNNs are designed for processing grid-like data, like images. They use convolution operations, where a filter slides across the input, performing element-wise multiplications and summations.”</li>
<li>“Mathematically, we’re essentially computing a discrete convolution, as shown by this equation: <span class="math display">\[(I * K)(i, j) = \sum_{m} \sum_{n} I(i-m, j-n) K(m, n)\]</span></li>
<li>“The complexity is related to the filter size and the number of channels, but because filters are small and operations are local, CNNs are computationally efficient and highly parallelizable.”</li>
</ul></li>
<li><strong>Transition to RNNs:</strong>
<ul>
<li>“RNNs, on the other hand, are designed for sequential data. They maintain a hidden state that’s updated at each time step, incorporating both the current input and the previous state.”</li>
<li>“The core equations involve updating the hidden state <span class="math inline">\(h_t\)</span> based on the input <span class="math inline">\(x_t\)</span> and the previous hidden state <span class="math inline">\(h_{t-1}\)</span>: <span class="math display">\[h_t = \sigma(W_{ih}x_t + b_{ih} + W_{hh}h_{t-1} + b_{hh})\]</span> <span class="math display">\[y_t = W_{hy}h_t + b_{hy}\]</span>”</li>
<li>“The key challenge with RNNs is their sequential nature, which limits parallelization and can lead to vanishing or exploding gradients. While LSTMs and GRUs help mitigate these issues, they increase complexity.”</li>
</ul></li>
<li><strong>Introduce Transformers:</strong>
<ul>
<li>“Transformers revolutionized sequence modeling by introducing the self-attention mechanism, which captures relationships between all positions in the input sequence in parallel.”</li>
<li>“The core idea is to compute queries, keys, and values from the input embeddings, and then use these to compute attention weights. The attention weights determines how much attention to pay to other parts of the sequence.”</li>
<li>“The heart of the transformer is the self-attention mechanism, calculated by: <span class="math display">\[Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span>.” Explain each part of the equation.</li>
<li>“The complexity of self-attention is <span class="math inline">\(O(n^2 \cdot d)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length, due to computing pairwise interactions between all positions. However, this can be highly parallelized, making transformers efficient for long sequences.”</li>
</ul></li>
<li><strong>Summarize and Highlight Trade-offs:</strong>
<ul>
<li>“In summary, CNNs excel at local feature extraction with high parallelization, RNNs handle sequential data but face challenges with long-range dependencies and parallelization, and Transformers leverage self-attention for capturing long-range dependencies with high parallelization, at the cost of higher computational complexity for shorter sequences.”</li>
<li>“The choice of architecture depends on the specific task and the nature of the data.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanations. Allow time for the interviewer to process the information.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you’re in a virtual interview, consider sharing your screen to display equations or diagrams.</li>
<li><strong>Check for Understanding:</strong> Pause periodically and ask if the interviewer has any questions.</li>
<li><strong>Focus on Key Concepts:</strong> Rather than getting bogged down in every detail, emphasize the core ideas and trade-offs.</li>
<li><strong>Be Ready to Elaborate:</strong> Be prepared to provide more details on any aspect of the explanation if the interviewer asks for it.</li>
<li><strong>Relate to Real-World Examples:</strong> If appropriate, mention how these architectures are used in specific applications (e.g., CNNs for image recognition, RNNs for speech recognition, Transformers for machine translation).</li>
<li><strong>Be Confident:</strong> Present your knowledge with confidence, but also be humble and acknowledge the limitations of each architecture.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>