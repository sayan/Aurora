<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>key_differences_between_rnn__cnn_based_models_and_transformers_8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-9.-how-do-these-architectures-differ-in-terms-of-scalability-and-deployment-considerations-particularly-in-real-time-systems" class="level2">
<h2 class="anchored" data-anchor-id="question-9.-how-do-these-architectures-differ-in-terms-of-scalability-and-deployment-considerations-particularly-in-real-time-systems">Question: 9. How do these architectures differ in terms of scalability and deployment considerations, particularly in real-time systems?</h2>
<p><strong>Best Answer</strong></p>
<p>The core differences between Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformers in terms of scalability and deployment, especially within real-time systems, stem from their architectural designs and computational properties. Each has distinct advantages and disadvantages.</p>
<p><strong>1. Scalability:</strong></p>
<ul>
<li><p><strong>RNNs:</strong> RNNs, particularly LSTMs and GRUs, process sequential data iteratively, making them inherently sequential. This sequential dependency significantly limits parallelization. If <span class="math inline">\(T\)</span> is the sequence length, each time step <span class="math inline">\(t\)</span> depends on the hidden state from the previous time step <span class="math inline">\(t-1\)</span>. The computational graph unfolds over time, which means the computation for <span class="math inline">\(h_t\)</span> (the hidden state at time <span class="math inline">\(t\)</span>) can only begin after <span class="math inline">\(h_{t-1}\)</span> is calculated.</p>
<p><span class="math display">\[h_t = f(W_{hh}h_{t-1} + W_{xh}x_t)\]</span></p>
<p>This makes RNNs less scalable for long sequences because the computational time increases linearly with the sequence length.</p></li>
<li><p><strong>CNNs:</strong> CNNs, particularly 1D CNNs used in sequence modeling, offer some degree of parallelization. While the convolution operation itself can be parallelized across different parts of the input sequence, the receptive field dictates the context size. To capture long-range dependencies, you need to either stack many convolutional layers or use dilated convolutions. Stacking layers increases the depth of the network, potentially making it harder to train and deeper networks also increase latency. Dilated convolutions increase the receptive field without adding layers, by introducing gaps between the kernel elements. However, very large dilation rates can cause the “dilution” of local dependencies.</p>
<p>A convolutional layer’s output at position <span class="math inline">\(i\)</span> can be written as:</p>
<p><span class="math display">\[y_i = \sum_{k=0}^{K-1} x_{i+k} * w_k + b\]</span></p>
<p>Where <span class="math inline">\(x\)</span> is the input sequence, <span class="math inline">\(w\)</span> is the kernel, <span class="math inline">\(K\)</span> is the kernel size, and <span class="math inline">\(b\)</span> is the bias. The key point is the ability to compute <span class="math inline">\(y_i\)</span> for different <span class="math inline">\(i\)</span> values in parallel.</p></li>
<li><p><strong>Transformers:</strong> Transformers are highly parallelizable. The self-attention mechanism allows each element in the input sequence to attend to all other elements simultaneously. The attention weights are calculated as follows:</p>
<p><span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
<p>where <span class="math inline">\(Q\)</span> (queries), <span class="math inline">\(K\)</span> (keys), and <span class="math inline">\(V\)</span> (values) are matrices derived from the input sequence, and <span class="math inline">\(d_k\)</span> is the dimension of the keys. The matrix multiplication <span class="math inline">\(QK^T\)</span> can be efficiently parallelized on GPUs or TPUs. This parallelization is a huge advantage for long sequences. Transformers are significantly more scalable than RNNs. The computational complexity of the attention mechanism is <span class="math inline">\(O(N^2)\)</span>, where N is the sequence length. While this seems quadratic, the parallelizability allows it to be much faster in practice, especially on GPUs.</p></li>
</ul>
<p><strong>2. Deployment Considerations (Real-Time Systems):</strong></p>
<ul>
<li><p><strong>RNNs:</strong> The sequential nature of RNNs poses a significant challenge for real-time deployment. The latency for processing each time step accumulates, making them unsuitable for applications requiring low-latency responses with long input sequences, such as real-time speech recognition or machine translation. The memory footprint can be relatively small, especially for simple RNN architectures, but this often comes at the cost of performance.</p></li>
<li><p><strong>CNNs:</strong> CNNs can be more hardware-efficient compared to RNNs due to their localized operations and weight sharing. The localized nature of convolution can be implemented efficiently on specialized hardware like FPGAs or ASICs. 1D CNNs are often preferred over RNNs for real-time systems requiring higher throughput.</p></li>
<li><p><strong>Transformers:</strong> While Transformers offer superior accuracy and scalability, they typically have larger model sizes and higher computational requirements than RNNs or CNNs. The large model size can be a challenge for deployment on resource-constrained devices. However, the high throughput due to parallelization can make them suitable for real-time systems if sufficient computational resources are available.</p></li>
</ul>
<p><strong>3. Trade-offs:</strong></p>
<ul>
<li><p><strong>Model Size:</strong> Transformers generally have larger model sizes compared to RNNs and CNNs. This is primarily due to the attention mechanism and the need for multiple layers to capture complex dependencies.</p></li>
<li><p><strong>Throughput vs.&nbsp;Latency:</strong> RNNs have low throughput but potentially lower latency for <em>very</em> short sequences. CNNs offer a trade-off between throughput and latency. Transformers offer high throughput due to parallelization but can have higher latency if not optimized properly, or if memory access becomes the bottleneck.</p></li>
<li><p><strong>Memory Constraints:</strong> Larger model sizes require more memory, which can be a limiting factor for deployment on edge devices or embedded systems. Model compression techniques such as quantization, pruning, and knowledge distillation are often employed to reduce the model size and memory footprint of Transformers.</p></li>
</ul>
<p><strong>4. Real-World Considerations:</strong></p>
<ul>
<li><p><strong>Hardware Acceleration:</strong> Specialized hardware accelerators like GPUs, TPUs, and FPGAs can significantly improve the performance of all three architectures. However, Transformers benefit the most from hardware acceleration due to their parallelizable nature.</p></li>
<li><p><strong>Optimization Techniques:</strong> Model compression techniques like quantization, pruning, and knowledge distillation are crucial for deploying large models like Transformers on resource-constrained devices.</p></li>
<li><p><strong>Streaming Inference:</strong> For real-time systems, streaming inference is often required. This involves processing the input sequence in chunks or segments. RNNs can be naturally adapted to streaming inference, while CNNs and Transformers require careful design to ensure low latency.</p></li>
</ul>
<p>In summary, the choice between RNNs, CNNs, and Transformers for real-time systems depends on the specific application requirements, available computational resources, and the trade-offs between model size, throughput, and latency. Transformers are generally preferred for applications requiring high accuracy and scalability, while CNNs are often a good choice for resource-constrained devices or applications where hardware efficiency is critical. RNNs are becoming less prevalent except in niche applications with memory or computational constraints.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to present this information in an interview:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“The key differences between RNNs, CNNs, and Transformers regarding scalability and deployment, especially in real-time, boil down to their architectural designs and computational properties. Each has its strengths and weaknesses.”</li>
</ul></li>
<li><strong>Address Scalability First:</strong>
<ul>
<li>“Let’s start with scalability. RNNs are inherently sequential due to their recurrent connections. Each time step depends on the previous one, limiting parallelization. This becomes a bottleneck for long sequences.”</li>
<li>“For example, mathematically, the hidden state at time <span class="math inline">\(t\)</span>, <span class="math inline">\(h_t\)</span>, depends on <span class="math inline">\(h_{t-1}\)</span> as shown in the equation: <span class="math inline">\(h_t = f(W_{hh}h_{t-1} + W_{xh}x_t)\)</span>. This sequential dependency hinders parallel computation.”</li>
</ul></li>
<li><strong>Transition to CNNs and Highlight Trade-offs:</strong>
<ul>
<li>“CNNs offer some parallelism through convolution operations but capturing long-range dependencies requires either deep networks or dilated convolutions. This creates trade-offs, as deeper networks can be harder to train and lead to latency, and large dilation rates can dilute local dependencies.”</li>
<li>“Each output <span class="math inline">\(y_i\)</span> can be computed in parallel with others using <span class="math inline">\(y_i = \sum_{k=0}^{K-1} x_{i+k} * w_k + b\)</span>.”</li>
</ul></li>
<li><strong>Emphasize Transformer’s Parallelism:</strong>
<ul>
<li>“Transformers, on the other hand, are highly parallelizable, especially with the self-attention mechanism. Each element can attend to all others simultaneously, which can be parallelized on GPUs.”</li>
<li>“The attention mechanism computes attention weights using <span class="math inline">\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span>. The matrix multiplication here is highly parallelizable.”</li>
<li>“So while the complexity is <span class="math inline">\(O(N^2)\)</span>, the parallelization gives it great speed.”</li>
</ul></li>
<li><strong>Move to Deployment Considerations for Real-Time Systems:</strong>
<ul>
<li>“Now, regarding deployment in real-time systems: RNNs suffer from accumulated latency, making them less suitable for low-latency applications with long sequences.”</li>
<li>“CNNs are more hardware-efficient due to localized operations, which can be efficiently implemented on specialized hardware.”</li>
<li>“Transformers, while highly accurate and scalable, typically have larger model sizes and computational demands. Model compression techniques become essential.”</li>
</ul></li>
<li><strong>Discuss Trade-offs (Model Size, Throughput, Latency):</strong>
<ul>
<li>“There are key trade-offs to consider. Transformers generally have larger model sizes, affecting memory requirements. RNNs have low throughput, while CNNs offer a balance. Transformers provide high throughput but can suffer from higher latency if not carefully optimized.”</li>
</ul></li>
<li><strong>Highlight Real-World Considerations:</strong>
<ul>
<li>“In practice, hardware acceleration is crucial. GPUs, TPUs, and FPGAs greatly improve performance, especially for Transformers. Also, optimization techniques like quantization, pruning, and knowledge distillation are vital for deploying large models on resource-constrained devices.”</li>
<li>“Streaming inference is important for real-time systems. Adapting CNNs and Transformers to streaming requires careful design.”</li>
</ul></li>
<li><strong>Conclude with a Summary:</strong>
<ul>
<li>“In summary, the best choice depends on the specific requirements of the application. Transformers excel in accuracy and scalability, CNNs offer hardware efficiency, and RNNs are becoming less common except for niche areas with large memory constraints.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you are in a virtual interview, consider sharing your screen to display equations or diagrams.</li>
<li><strong>Encourage Interaction:</strong> Ask the interviewer if they have any questions or would like you to elaborate on any specific point.</li>
<li><strong>Simplify Complex Concepts:</strong> When discussing mathematical formulas, provide intuitive explanations and real-world examples to help the interviewer understand the concepts.</li>
<li><strong>Be Confident:</strong> Project confidence in your knowledge and abilities.</li>
<li><strong>Show Practical Awareness:</strong> Highlight real-world considerations and optimization techniques to demonstrate your understanding of practical deployment challenges.</li>
</ul>
<p>By following these steps, you can effectively communicate your expertise and demonstrate your understanding of the key differences between RNNs, CNNs, and Transformers in terms of scalability and deployment.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>