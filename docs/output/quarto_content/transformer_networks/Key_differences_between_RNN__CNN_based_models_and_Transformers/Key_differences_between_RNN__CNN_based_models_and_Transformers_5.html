<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>key_differences_between_rnn__cnn_based_models_and_transformers_5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-6.-how-do-positional-encodings-in-transformers-compare-with-the-inherent-sequential-nature-of-rnns-and-the-local-structure-exploited-by-cnns" class="level2">
<h2 class="anchored" data-anchor-id="question-6.-how-do-positional-encodings-in-transformers-compare-with-the-inherent-sequential-nature-of-rnns-and-the-local-structure-exploited-by-cnns">Question: 6. How do positional encodings in Transformers compare with the inherent sequential nature of RNNs and the local structure exploited by CNNs?</h2>
<p><strong>Best Answer</strong></p>
<p>The Transformer architecture marked a significant departure from Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), especially in how it handles sequential data. This difference stems from the fundamental architectural designs and how each network captures temporal or spatial dependencies within the data.</p>
<ul>
<li><p><strong>RNNs: Inherent Sequential Processing</strong></p>
<p>RNNs, by design, are inherently sequential. They process input data one element at a time, maintaining a hidden state that is updated at each step. This hidden state acts as a “memory” of the sequence, allowing the network to capture dependencies between elements that are far apart. The update rule for the hidden state <span class="math inline">\(h_t\)</span> at time <span class="math inline">\(t\)</span> is typically defined as:</p>
<p><span class="math display">\[
h_t = f(h_{t-1}, x_t)
\]</span></p>
<p>where <span class="math inline">\(x_t\)</span> is the input at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(f\)</span> is a non-linear function (e.g., a sigmoid, tanh, or ReLU activation applied to a linear combination of <span class="math inline">\(h_{t-1}\)</span> and <span class="math inline">\(x_t\)</span>). The output <span class="math inline">\(y_t\)</span> is then often a function of the hidden state:</p>
<p><span class="math display">\[
y_t = g(h_t)
\]</span></p>
<p>Due to this sequential nature, RNNs implicitly encode positional information. The order in which the data is fed into the network directly influences the learned representations. The earlier elements in the sequence affect the hidden states that are used for processing later elements. However, this sequential processing limits parallelization, making RNNs slower to train on long sequences. Additionally, RNNs can suffer from vanishing/exploding gradient problems, which make it difficult to learn long-range dependencies. Variants like LSTMs and GRUs were designed to mitigate these issues, but the inherent sequential bottleneck remains.</p></li>
<li><p><strong>CNNs: Exploiting Local Structure</strong></p>
<p>CNNs, traditionally used for image processing, capture local patterns through convolutional filters. These filters slide across the input, detecting features within a local receptive field. For 1D sequences (like text), CNNs learn patterns of <span class="math inline">\(n\)</span>-grams, where <span class="math inline">\(n\)</span> is the filter size. The output feature map <span class="math inline">\(F\)</span> for a given filter <span class="math inline">\(W\)</span> applied to an input <span class="math inline">\(X\)</span> can be described as:</p>
<p><span class="math display">\[
F[i] = \sum_{k=1}^{n} W[k] \cdot X[i+k-1] + b
\]</span></p>
<p>where <span class="math inline">\(b\)</span> is a bias term.</p>
<p>CNNs can process the entire input sequence in parallel, offering significant speed advantages over RNNs. However, CNNs do not inherently encode positional information in the same way as RNNs or Transformers. To capture longer-range dependencies, CNNs rely on stacking multiple layers, each layer increasing the receptive field. Dilated convolutions offer another approach, increasing the receptive field without adding more layers by introducing gaps between the filter elements. For example, a dilated convolution with dilation rate <span class="math inline">\(d\)</span> would compute:</p>
<p><span class="math display">\[
F[i] = \sum_{k=1}^{n} W[k] \cdot X[i + (k-1) \cdot d] + b
\]</span></p>
<p>While CNNs are efficient at capturing local features, they may require deeper architectures or dilated convolutions to model long-range dependencies effectively. The positional information is implicitly encoded through the hierarchy of convolutional layers and their receptive fields.</p></li>
<li><p><strong>Transformers: Parallel Processing with Positional Encodings</strong></p>
<p>Transformers eschew recurrence and convolutions entirely, relying instead on self-attention mechanisms to capture relationships between all elements in the input sequence simultaneously. This allows for parallel processing, greatly accelerating training. However, because the self-attention mechanism is permutation-invariant (i.e., it doesn’t inherently consider the order of the input), Transformers require explicit <em>positional encodings</em> to inform the model about the position of each element in the sequence.</p>
<p>Positional encodings are added to the input embeddings <em>before</em> they are fed into the self-attention layers. Common positional encodings include sinusoidal functions:</p>
<p><span class="math display">\[
PE(pos, 2i) = sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span></p>
<p><span class="math display">\[
PE(pos, 2i+1) = cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]</span></p>
<p>where <span class="math inline">\(pos\)</span> is the position of the element in the sequence, <span class="math inline">\(i\)</span> is the dimension index, and <span class="math inline">\(d_{model}\)</span> is the dimensionality of the embeddings. These sinusoidal functions provide a unique positional signature for each element, allowing the model to distinguish between elements at different positions. The addition of positional encodings ensures that the model is aware of the order of the sequence elements, despite the parallel processing nature of the self-attention mechanism.</p></li>
<li><p><strong>Comparison Summary</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 23%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>RNNs</th>
<th>CNNs</th>
<th>Transformers (with Positional Encodings)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sequential</td>
<td>Yes</td>
<td>No (parallel processing of local windows)</td>
<td>No (parallel processing with positional encodings)</td>
</tr>
<tr class="even">
<td>Positional Encoding</td>
<td>Implicit (through hidden state)</td>
<td>Implicit (through stacking and receptive field)</td>
<td>Explicit (added to input embeddings)</td>
</tr>
<tr class="odd">
<td>Long-Range Dependencies</td>
<td>Difficult (vanishing gradients)</td>
<td>Requires deeper architectures/dilated convolutions</td>
<td>Excellent (through self-attention)</td>
</tr>
<tr class="even">
<td>Parallelization</td>
<td>Limited</td>
<td>High</td>
<td>High</td>
</tr>
</tbody>
</table></li>
</ul>
<p>In summary, RNNs inherently capture sequential information but suffer from parallelization limitations and vanishing gradients. CNNs efficiently process local structures in parallel, but require deeper architectures or dilated convolutions for long-range dependencies. Transformers, by using positional encodings and self-attention, achieve parallel processing while effectively modeling both local and long-range dependencies, but require explicit mechanisms to inject information about position. Each architecture has its strengths and weaknesses, making the choice dependent on the specific task and data characteristics.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested narration strategy for this question, keeping in mind clarity and depth:</p>
<ol type="1">
<li><strong>Start with the High-Level Difference:</strong>
<ul>
<li>“The key difference lies in how each architecture handles sequential information and dependencies. RNNs process data sequentially, CNNs exploit local structures, and Transformers use attention mechanisms with positional encodings to enable parallel processing.”</li>
</ul></li>
<li><strong>Explain RNNs (Emphasize Sequential Nature):</strong>
<ul>
<li>“RNNs are inherently sequential. They process one element at a time, updating a hidden state that acts as a memory. This makes them naturally sensitive to order.”</li>
<li>“The hidden state at time <em>t</em>, <span class="math inline">\(h_t\)</span>, is a function of the previous hidden state and the current input: <span class="math inline">\(h_t = f(h_{t-1}, x_t)\)</span>. This sequential update is how the network captures temporal dependencies.”</li>
<li>“However, this sequential nature limits parallelization, and they can struggle with long-range dependencies due to vanishing/exploding gradients.”</li>
</ul></li>
<li><strong>Explain CNNs (Emphasize Local Structure and Parallelism):</strong>
<ul>
<li>“CNNs, on the other hand, excel at capturing local patterns in parallel. They use convolutional filters to detect features within a local receptive field. Think of it as identifying n-grams in text.”</li>
<li>“A filter W slides across the input X to produce a feature map, calculated as: <span class="math inline">\(F[i] = \sum_{k=1}^{n} W[k] \cdot X[i+k-1] + b\)</span>.”</li>
<li>“While they process in parallel, capturing long-range dependencies requires stacking layers or using dilated convolutions to increase the receptive field.”</li>
</ul></li>
<li><strong>Introduce Transformers (Highlight Positional Encoding):</strong>
<ul>
<li>“Transformers take a completely different approach. They process the entire sequence in parallel using self-attention, allowing them to capture relationships between all elements simultaneously.”</li>
<li>“However, since self-attention is permutation-invariant, Transformers <em>require</em> explicit positional encodings. These encodings are added to the input embeddings to inform the model about the position of each element.”</li>
<li>“Common positional encodings are sinusoidal functions, like: <span class="math inline">\(PE(pos, 2i) = sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\)</span> and <span class="math inline">\(PE(pos, 2i+1) = cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)\)</span>.”</li>
<li>“This ensures that the model is aware of the order, despite the parallel processing.”</li>
</ul></li>
<li><strong>Summarize and Compare (Table format if possible in person):</strong>
<ul>
<li>“In summary, RNNs are sequential, CNNs are local and parallel, and Transformers are parallel with positional encodings. RNNs inherently encode position, CNNs implicitly do so through stacking layers, while Transformers explicitly add positional information.”</li>
</ul></li>
<li><strong>Adapt to Interviewer:</strong>
<ul>
<li>Gauge the interviewer’s background. If they seem mathematically inclined, delve deeper into the equations. Otherwise, focus on the conceptual understanding.</li>
<li>Pause after explaining each architecture to allow for questions and steer the conversation based on their interests.</li>
<li>If they ask about the drawbacks of positional encodings, you can discuss limitations of fixed encodings versus learned encodings, or the challenge of extrapolating to sequence lengths not seen during training.</li>
<li>End by highlighting that the best architecture depends on the task.</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to absorb the information.</li>
<li><strong>Use Visual Aids (if possible):</strong> Drawing simple diagrams of the architectures can be helpful.</li>
<li><strong>Encourage Interaction:</strong> Ask the interviewer if they have any questions or if they’d like you to elaborate on a particular aspect.</li>
<li><strong>Be Confident:</strong> Demonstrate a strong understanding of the concepts, but also acknowledge the limitations of each approach.</li>
<li><strong>Check for understanding after presenting any equations.</strong> “Does that equation make sense?” “Are you familiar with this specific positional encoding method?”</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>