<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>key_differences_between_rnn__cnn_based_models_and_transformers_1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-2.-how-do-rnns-cnns-and-transformers-handle-long-range-dependencies-and-what-are-the-potential-pitfalls-of-each-approach" class="level2">
<h2 class="anchored" data-anchor-id="question-2.-how-do-rnns-cnns-and-transformers-handle-long-range-dependencies-and-what-are-the-potential-pitfalls-of-each-approach">Question: 2. How do RNNs, CNNs, and Transformers handle long-range dependencies, and what are the potential pitfalls of each approach?</h2>
<p><strong>Best Answer</strong></p>
<p>Handling long-range dependencies is a crucial aspect of sequence modeling. Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformers each tackle this challenge with different mechanisms, and each approach has its own limitations.</p>
<p><strong>1. Recurrent Neural Networks (RNNs)</strong></p>
<ul>
<li><strong>Mechanism:</strong> RNNs process sequential data one step at a time, maintaining a hidden state that summarizes the information from previous steps. This hidden state is updated at each time step and, in principle, allows the network to carry information across long distances. <span class="math display">\[h_t = f(h_{t-1}, x_t)\]</span> where <span class="math inline">\(h_t\)</span> is the hidden state at time <span class="math inline">\(t\)</span>, <span class="math inline">\(x_t\)</span> is the input at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(f\)</span> is an activation function (e.g., tanh or ReLU).</li>
<li><strong>Long-Range Dependency Handling:</strong> RNNs <em>attempt</em> to handle long-range dependencies by propagating information through the hidden state. The updated hidden state is based on a combination of the previous hidden state and the current input, ideally capturing the context needed for future predictions.</li>
<li><strong>Potential Pitfalls:</strong>
<ul>
<li><strong>Vanishing/Exploding Gradients:</strong> During backpropagation through time (BPTT), the gradients can either vanish (decay exponentially) or explode (grow exponentially) as they are propagated through many layers. Vanishing gradients prevent the network from learning long-range dependencies because the earlier layers receive little or no gradient signal. Exploding gradients, on the other hand, can cause unstable training and lead to divergence.</li>
<li><strong>Mathematical Explanation of Vanishing/Exploding Gradients:</strong> The gradient of the loss function <span class="math inline">\(L\)</span> with respect to the hidden state at time <span class="math inline">\(k\)</span>, <span class="math inline">\(\frac{\partial L}{\partial h_k}\)</span>, depends on the product of Jacobians: <span class="math display">\[\frac{\partial L}{\partial h_k} = \frac{\partial L}{\partial h_T} \prod_{t=k+1}^{T} \frac{\partial h_t}{\partial h_{t-1}}\]</span> where <span class="math inline">\(T\)</span> is the final time step. The Jacobian <span class="math inline">\(\frac{\partial h_t}{\partial h_{t-1}}\)</span> reflects how sensitive the hidden state at time <span class="math inline">\(t\)</span> is to changes in the hidden state at time <span class="math inline">\(t-1\)</span>. If the largest eigenvalue of this Jacobian is less than 1, the gradient will vanish exponentially as we backpropagate further back in time. Conversely, if the largest eigenvalue is greater than 1, the gradient will explode.</li>
<li><strong>Mitigation Techniques:</strong>
<ul>
<li><strong>LSTM and GRU:</strong> Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks introduce gating mechanisms that regulate the flow of information through the hidden state. These gates help to alleviate the vanishing gradient problem by allowing the network to selectively remember or forget information over long sequences. LSTM uses a cell state <span class="math inline">\(C_t\)</span> along with gates to control the flow of information:
<ul>
<li>Forget gate: <span class="math inline">\(f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)\)</span></li>
<li>Input gate: <span class="math inline">\(i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)\)</span></li>
<li>Cell state update: <span class="math inline">\(\tilde{C_t} = \tanh(W_C [h_{t-1}, x_t] + b_C)\)</span></li>
<li>New cell state: <span class="math inline">\(C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C_t}\)</span></li>
<li>Output gate: <span class="math inline">\(o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)\)</span></li>
<li>Hidden state: <span class="math inline">\(h_t = o_t \odot \tanh(C_t)\)</span></li>
</ul></li>
<li><strong>Gradient Clipping:</strong> Clipping the gradients to a certain range can prevent them from exploding. This involves rescaling the gradient vector if its norm exceeds a predefined threshold.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>2. Convolutional Neural Networks (CNNs)</strong></p>
<ul>
<li><strong>Mechanism:</strong> CNNs apply convolutional filters to local regions of the input sequence. Each filter learns to detect specific patterns, and the network progressively builds higher-level representations by stacking convolutional layers. <span class="math display">\[y[i] = \sum_{k=1}^{K} w[k] * x[i+k-1] + b\]</span> where <span class="math inline">\(y[i]\)</span> is the output at position <span class="math inline">\(i\)</span>, <span class="math inline">\(x\)</span> is the input sequence, <span class="math inline">\(w\)</span> is the filter kernel of size <span class="math inline">\(K\)</span>, and <span class="math inline">\(b\)</span> is the bias.</li>
<li><strong>Long-Range Dependency Handling:</strong> Standard CNNs have a limited receptive field, meaning that each neuron can only see a small portion of the input sequence. To capture long-range dependencies, deep CNNs with many layers are needed, where the receptive field increases with each layer.</li>
<li><strong>Potential Pitfalls:</strong>
<ul>
<li><strong>Limited Receptive Field:</strong> Capturing very long-range dependencies requires very deep networks, which can be computationally expensive and difficult to train. Even with deep networks, it can be challenging for information from distant parts of the sequence to effectively influence the representations at a given location.</li>
<li><strong>Mitigation Techniques:</strong>
<ul>
<li><strong>Dilated Convolutions:</strong> Dilated convolutions introduce gaps between the filter weights, effectively increasing the receptive field without increasing the number of parameters. The dilation factor determines the size of the gaps. For a dilation factor <span class="math inline">\(d\)</span>, the convolution operation becomes: <span class="math display">\[y[i] = \sum_{k=1}^{K} w[k] * x[i + d(k-1)] + b\]</span></li>
<li><strong>Stacked Convolutional Layers:</strong> Stacking multiple convolutional layers increases the receptive field.</li>
<li><strong>Attention Mechanisms (Hybrid Approach):</strong> Combining CNNs with attention mechanisms can allow the network to selectively attend to relevant parts of the input sequence, regardless of their distance.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>3. Transformers</strong></p>
<ul>
<li><strong>Mechanism:</strong> Transformers rely entirely on attention mechanisms, specifically self-attention, to capture relationships between different positions in the input sequence. Self-attention allows each position to attend to all other positions, directly modeling long-range dependencies.</li>
<li><strong>Long-Range Dependency Handling:</strong> Self-attention enables the model to directly capture dependencies between any two positions in the sequence, regardless of their distance. The attention weights indicate the importance of each position in the sequence for computing the representation at a given position.
<ul>
<li><strong>Attention Calculation:</strong> <span class="math display">\[Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span> where <span class="math inline">\(Q\)</span> is the query matrix, <span class="math inline">\(K\)</span> is the key matrix, <span class="math inline">\(V\)</span> is the value matrix, and <span class="math inline">\(d_k\)</span> is the dimension of the keys.</li>
</ul></li>
<li><strong>Potential Pitfalls:</strong>
<ul>
<li><strong>Computational Complexity:</strong> The self-attention mechanism has a quadratic computational complexity with respect to the sequence length, <span class="math inline">\(O(n^2)\)</span>, because each position needs to attend to every other position. This can be a bottleneck for very long sequences.</li>
<li><strong>Mitigation Techniques:</strong>
<ul>
<li><strong>Sparse Attention:</strong> Sparse attention mechanisms reduce the computational complexity by only allowing each position to attend to a subset of other positions.</li>
<li><strong>Linear Attention:</strong> Linear attention mechanisms reduce the computational complexity to linear, <span class="math inline">\(O(n)\)</span>.</li>
<li><strong>Positional Encodings:</strong> Transformers do not inherently capture the order of the sequence. Positional encodings are added to the input embeddings to provide information about the position of each token in the sequence. Common positional encodings include sinusoidal functions: <span class="math display">\[PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)\]</span> <span class="math display">\[PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)\]</span></li>
<li><strong>Layer Stacking and Parallelization:</strong> Deep Transformer networks require careful attention to training stability. Layer normalization and residual connections are crucial. The parallel nature of the attention mechanism makes Transformers highly amenable to parallelization on GPUs/TPUs.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Summary Table:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 27%">
<col style="width: 24%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>RNN</th>
<th>CNN</th>
<th>Transformer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Long-Range Dependency</td>
<td>Limited by vanishing/exploding gradients</td>
<td>Limited by receptive field</td>
<td>Direct attention to all positions</td>
</tr>
<tr class="even">
<td>Computational Complexity</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(n)\)</span> (but depth matters)</td>
<td><span class="math inline">\(O(n^2)\)</span> (can be reduced with sparse attention)</td>
</tr>
<tr class="odd">
<td>Mitigation Techniques</td>
<td>LSTM, GRU, gradient clipping</td>
<td>Dilated convolutions, deep stacking</td>
<td>Sparse attention, positional encodings</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to deliver this answer effectively in an interview:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“Handling long-range dependencies is a key challenge in sequence modeling. RNNs, CNNs, and Transformers address this issue in fundamentally different ways, each with its own strengths and weaknesses.”</li>
</ul></li>
<li><strong>RNN Explanation:</strong>
<ul>
<li>“RNNs process data sequentially, maintaining a hidden state that’s updated at each step. Ideally, this allows them to capture dependencies across long sequences. However, they suffer from the vanishing/exploding gradient problem, making it difficult to learn long-range relationships.”</li>
<li>(If asked for more detail) “The vanishing gradient problem occurs because the gradients are multiplied during backpropagation. If these gradients are small, they diminish exponentially as they are propagated backward. Similarly, if they are too large, the gradients can explode, leading to unstable training.”</li>
<li>“To mitigate this, we often use LSTMs or GRUs, which employ gating mechanisms to regulate the flow of information. Gradient clipping is another technique.”</li>
</ul></li>
<li><strong>CNN Explanation:</strong>
<ul>
<li>“CNNs, on the other hand, use convolutional filters to extract local features. To capture long-range dependencies, you need deep networks with large receptive fields. However, very deep CNNs can be computationally expensive.”</li>
<li>“Dilated convolutions are a technique to increase the receptive field without adding parameters. They introduce gaps between the filter weights.”</li>
<li>“We can also stack multiple CNN layers to capture longer range dependencies.”</li>
</ul></li>
<li><strong>Transformer Explanation:</strong>
<ul>
<li>“Transformers take a completely different approach, using self-attention to directly model relationships between all positions in the sequence. This allows them to capture long-range dependencies very effectively.”</li>
<li>(If asked for more detail on self-attention) “Self-attention calculates attention weights that determine how much each position in the sequence should attend to every other position. The attention score is calculated as the dot product of the queries and keys, scaled by the square root of the dimension of the keys, and then passed through a softmax function.” Mention <span class="math inline">\(Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span>.</li>
<li>“The main drawback of Transformers is the quadratic computational complexity, <span class="math inline">\(O(n^2)\)</span>, which can be a problem for very long sequences.”</li>
<li>“Mitigation strategies include sparse attention, linear attention, and efficient implementations that leverage parallel processing.” Don’t forget to mention “positional embeddings are crucial to provide information about the position of each token in the sequence”</li>
</ul></li>
<li><strong>Summarize and Compare:</strong>
<ul>
<li>“In summary, RNNs struggle with long-range dependencies due to vanishing/exploding gradients, CNNs are limited by receptive fields, and Transformers excel at capturing long-range dependencies but can be computationally expensive. The choice of model depends on the specific application and the length of the sequences being processed.” Mention the table to compare and contrast these models.</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanations, especially when discussing mathematical details.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you are interviewing remotely and have the ability to share a whiteboard, use it to draw diagrams illustrating the different architectures and mechanisms.</li>
<li><strong>Check for Understanding:</strong> Pause periodically and ask the interviewer if they have any questions.</li>
<li><strong>Be Flexible:</strong> Be prepared to adjust the level of detail based on the interviewer’s questions and their level of understanding.</li>
<li><strong>Be Confident:</strong> Speak clearly and confidently, demonstrating your expertise in the area.</li>
</ul>
<p><strong>Walking Through Mathematical Sections:</strong></p>
<ul>
<li><strong>Don’t Just Recite:</strong> Avoid simply reciting equations without explaining their meaning.</li>
<li><strong>Provide Intuition:</strong> Explain the intuition behind the equations in plain English. For example, when discussing the self-attention equation, explain that it calculates the attention weights based on the similarity between the query and key vectors.</li>
<li><strong>Focus on Key Concepts:</strong> Highlight the key variables and operations in the equations, and explain their role in the overall process.</li>
<li><strong>Offer Examples:</strong> If appropriate, provide concrete examples to illustrate how the equations work in practice.</li>
<li><strong>Gauge the Interviewer’s Interest:</strong> Pay attention to the interviewer’s body language and questions to gauge their level of interest in the mathematical details. Adjust your explanation accordingly. If they seem less interested, focus more on the high-level concepts. If they are very interested, you can dive into more detail.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>