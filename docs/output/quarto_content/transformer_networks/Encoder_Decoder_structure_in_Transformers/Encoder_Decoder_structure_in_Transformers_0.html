<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>encoder_decoder_structure_in_transformers_0</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-1.-can-you-describe-the-overall-architecture-of-the-encoder-decoder-transformer-what-are-the-primary-responsibilities-of-the-encoder-and-the-decoder-in-this-setup" class="level2">
<h2 class="anchored" data-anchor-id="question-1.-can-you-describe-the-overall-architecture-of-the-encoder-decoder-transformer-what-are-the-primary-responsibilities-of-the-encoder-and-the-decoder-in-this-setup">Question: 1. Can you describe the overall architecture of the Encoder-Decoder Transformer? What are the primary responsibilities of the encoder and the decoder in this setup?</h2>
<p><strong>Best Answer</strong></p>
<p>The Transformer architecture, introduced in the paper “Attention is All You Need,” revolutionized sequence-to-sequence modeling by eschewing recurrent and convolutional layers in favor of attention mechanisms. The core of the Transformer is its encoder-decoder structure, each component playing a distinct role in processing and generating sequences.</p>
<p><strong>Overall Architecture</strong></p>
<p>The Transformer model consists of two main parts: the encoder and the decoder. Both the encoder and the decoder are composed of multiple identical layers stacked on top of each other. The input sequence is first processed by the encoder, and then its output is used by the decoder to generate the output sequence. Let’s break down the key components:</p>
<ul>
<li><strong>Encoder:</strong> The encoder’s primary responsibility is to transform the input sequence into a rich, contextualized representation. This representation captures the nuances and relationships between the elements of the input.</li>
<li><strong>Decoder:</strong> The decoder takes the encoder’s output and generates the output sequence one element at a time. It conditions its generation on the encoder’s representation and the previously generated elements.</li>
</ul>
<p><strong>Encoder Details</strong></p>
<p>The encoder consists of a stack of <span class="math inline">\(N\)</span> identical layers. Each layer has two sub-layers:</p>
<ol type="1">
<li><strong>Multi-Head Self-Attention:</strong> This layer allows the encoder to weigh the importance of different parts of the input sequence when processing each element. It computes attention scores between all pairs of tokens in the input sequence.</li>
<li><strong>Feed-Forward Network:</strong> A fully connected feed-forward network is applied to each position independently and identically.</li>
</ol>
<p>These two sub-layers are followed by residual connections and layer normalization. That is, the output of each sub-layer is LayerNorm(<span class="math inline">\(x\)</span> + Sublayer(<span class="math inline">\(x\)</span>)), where Sublayer(<span class="math inline">\(x\)</span>) is the function implemented by the sub-layer itself.</p>
<p><em>Mathematical Representation:</em> Let <span class="math inline">\(X = (x_1, x_2, ..., x_n)\)</span> be the input sequence to the encoder.</p>
<ol type="1">
<li><p>Positional Encoding: First, positional encodings <span class="math inline">\(P = (p_1, p_2, ..., p_n)\)</span> are added to the input embeddings <span class="math inline">\(X\)</span> to provide information about the position of each token in the sequence. These encodings are typically sine and cosine functions of different frequencies: <span class="math display">\[
PE(pos, 2i) = sin(\frac{pos}{10000^{2i/d_{model}}})
\]</span> <span class="math display">\[
PE(pos, 2i+1) = cos(\frac{pos}{10000^{2i/d_{model}}})
\]</span> Where <span class="math inline">\(pos\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension. <span class="math inline">\(d_{model}\)</span> is the dimension of the embedding space.</p></li>
<li><p>Multi-Head Attention: The input to the multi-head attention layer is <span class="math inline">\(X + P\)</span>. The self-attention mechanism can be mathematically described as:</p></li>
</ol>
<p><span class="math display">\[
  Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
  \]</span></p>
<p>Where <span class="math inline">\(Q\)</span> is the query matrix, <span class="math inline">\(K\)</span> is the key matrix, <span class="math inline">\(V\)</span> is the value matrix and <span class="math inline">\(d_k\)</span> is the dimension of the key vectors. Multi-head attention runs the attention mechanism <span class="math inline">\(h\)</span> times with different learned linear projections of the queries, keys, and values. These are then concatenated and linearly transformed into the final output:</p>
<p><span class="math display">\[
  MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
  \]</span></p>
<p>where <span class="math inline">\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)</span>. <span class="math inline">\(W_i^Q\)</span>, <span class="math inline">\(W_i^K\)</span>, <span class="math inline">\(W_i^V\)</span> and <span class="math inline">\(W^O\)</span> are parameter matrices.</p>
<ol start="3" type="1">
<li>Feed-Forward Network: The output of the multi-head attention layer is then passed through a position-wise feed-forward network (FFN):</li>
</ol>
<p><span class="math display">\[
  FFN(x) = ReLU(xW_1)W_2
  \]</span></p>
<p>Where <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> are weight matrices.</p>
<p>Each of these operations is followed by an Add &amp; Norm operation, which adds the input to the layer and normalizes the result: <span class="math display">\[
LayerNorm(x + Sublayer(x))
\]</span> where Sublayer(x) is the function implemented by the sub-layer itself.</p>
<ul>
<li><em>Key aspects:</em>
<ul>
<li><em>Self-attention allows the encoder to consider the context of the entire input sequence when processing each word.</em></li>
<li><em>Stacking multiple layers allows the encoder to learn hierarchical representations of the input.</em></li>
<li><em>Residual connections help to mitigate the vanishing gradient problem, enabling the training of deeper networks.</em></li>
</ul></li>
</ul>
<p><strong>Decoder Details</strong></p>
<p>The decoder also consists of a stack of <span class="math inline">\(N\)</span> identical layers. Each layer has three sub-layers:</p>
<ol type="1">
<li><strong>Masked Multi-Head Self-Attention:</strong> Similar to the encoder’s self-attention, but with a mask to prevent the decoder from “cheating” by looking at future tokens in the output sequence during training. This ensures that the prediction for position <span class="math inline">\(i\)</span> only depends on the known outputs at positions less than <span class="math inline">\(i\)</span>.</li>
<li><strong>Encoder-Decoder Attention:</strong> This layer allows the decoder to attend to the output of the encoder. It helps the decoder focus on the relevant parts of the input sequence when generating each element of the output sequence. The queries come from the previous decoder layer, and the keys and values come from the output of the encoder.</li>
<li><strong>Feed-Forward Network:</strong> Same as in the encoder.</li>
</ol>
<p>Again, each sub-layer is followed by residual connections and layer normalization.</p>
<p><em>Mathematical Representation:</em> Let <span class="math inline">\(Y = (y_1, y_2, ..., y_m)\)</span> be the output sequence generated by the decoder. The decoder uses the output of the encoder and the previously generated tokens to predict the next token in the sequence.</p>
<ol type="1">
<li><p>Masked Multi-Head Self-Attention: The masked self-attention is the same as the encoder’s self-attention, but with a mask applied to the attention weights to prevent the decoder from attending to future tokens. This ensures that the prediction for position <span class="math inline">\(i\)</span> only depends on the known outputs at positions less than <span class="math inline">\(i\)</span>. The mask can be represented as a matrix <span class="math inline">\(M\)</span>, where <span class="math inline">\(M_{ij} = 0\)</span> if <span class="math inline">\(j \leq i\)</span> and <span class="math inline">\(-\infty\)</span> otherwise. The attention mechanism becomes: <span class="math display">\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}} + M)V
\]</span></p></li>
<li><p>Encoder-Decoder Attention: This attention layer is crucial for connecting the encoder and decoder. The queries come from the previous decoder layer, and the keys and values come from the output of the encoder. This allows the decoder to focus on the relevant parts of the input sequence when generating the output sequence. <span class="math display">\[
Attention(Q_{decoder}, K_{encoder}, V_{encoder}) = softmax(\frac{Q_{decoder}K_{encoder}^T}{\sqrt{d_k}})V_{encoder}
\]</span></p></li>
<li><p>Feed-Forward Network: Same as in the encoder.</p></li>
</ol>
<p>Like the encoder, each of these operations is followed by an Add &amp; Norm operation: <span class="math display">\[
LayerNorm(x + Sublayer(x))
\]</span></p>
<ul>
<li><em>Key aspects:</em>
<ul>
<li><em>Masked self-attention ensures that the decoder only uses information from previous tokens when generating the current token.</em></li>
<li><em>Encoder-decoder attention allows the decoder to focus on relevant parts of the input sequence.</em></li>
<li><em>The decoder generates the output sequence one element at a time, conditioned on the encoder’s output and the previously generated elements.</em></li>
</ul></li>
</ul>
<p><strong>Responsibilities Summarized</strong></p>
<ul>
<li><strong>Encoder:</strong> Creates a context-rich representation of the input sequence.</li>
<li><strong>Decoder:</strong> Generates the output sequence, conditioned on the encoder’s representation and its own previous outputs.</li>
</ul>
<p><strong>Importance of Key Modules</strong></p>
<ul>
<li><strong>Multi-Head Attention:</strong> Captures relationships between words in a sentence, allowing the model to understand context and meaning.</li>
<li><strong>Positional Encodings:</strong> Provide information about the order of words, which is crucial for understanding syntax and semantics.</li>
<li><strong>Feed-Forward Networks:</strong> Introduce non-linearity and allow the model to learn complex patterns in the data.</li>
<li><strong>Residual Connections &amp; Layer Normalization:</strong> Facilitate training of deep networks by addressing vanishing gradients and improving convergence.</li>
</ul>
<p><strong>Real-World Considerations</strong></p>
<ul>
<li><strong>Computational Cost:</strong> Transformers are computationally intensive, especially for long sequences. Techniques like attention pruning or sparse attention can mitigate this.</li>
<li><strong>Memory Requirements:</strong> The attention mechanism requires significant memory. Gradient checkpointing can reduce memory usage at the cost of increased computation.</li>
<li><strong>Sequence Length Limitations:</strong> Standard Transformers have quadratic complexity with respect to sequence length due to the attention mechanism (<span class="math inline">\(O(n^2)\)</span>). Variants like Longformer and Reformer address this limitation.</li>
<li><strong>Training Data:</strong> Transformers require large amounts of training data to perform well. Transfer learning from pre-trained models (e.g., BERT, GPT) is often used to fine-tune them for specific tasks when data is limited.</li>
</ul>
<hr>
<p><strong>How to Narrate</strong></p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong> “The Transformer model, introduced in ‘Attention is All You Need,’ uses an encoder-decoder architecture to perform sequence-to-sequence tasks. Unlike RNNs or CNNs, it relies entirely on attention mechanisms.”</p></li>
<li><p><strong>Explain the Encoder’s Role:</strong> “The encoder takes the input sequence and transforms it into a contextualized representation. This representation captures the relationships between different elements of the input.”</p></li>
<li><p><strong>Break Down the Encoder Layer:</strong> “Each encoder layer consists of two main sub-layers: multi-head self-attention and a feed-forward network. The self-attention mechanism allows the encoder to weigh the importance of different words in the input sequence. Then, a feed-forward network is applied to each position independently.”</p></li>
<li><p><strong>Optionally, Introduce Math Sparingly:</strong> “Mathematically, the attention mechanism can be represented as softmax(<span class="math inline">\(\frac{QK^T}{\sqrt{d_k}}\)</span>)V, where Q, K, and V are query, key, and value matrices. Multi-head attention runs this in parallel with different linear projections.” (Only include this if the interviewer seems receptive to mathematical detail; otherwise, focus on the conceptual explanation.)</p></li>
<li><p><strong>Explain the Decoder’s Role:</strong> “The decoder generates the output sequence, one token at a time, conditioned on the encoder’s output and the previously generated tokens.”</p></li>
<li><p><strong>Break Down the Decoder Layer:</strong> “Each decoder layer has three sub-layers: masked multi-head self-attention, encoder-decoder attention, and a feed-forward network. The masked self-attention prevents the decoder from looking ahead during training. The encoder-decoder attention allows the decoder to focus on the relevant parts of the input sequence.”</p></li>
<li><p><strong>Emphasize Encoder-Decoder Interaction:</strong> “The encoder-decoder attention mechanism is key. The queries come from the previous decoder layer, while the keys and values come from the encoder output. This allows the decoder to selectively attend to the most relevant parts of the input.”</p></li>
<li><p><strong>Summarize Responsibilities Clearly:</strong> “So, to summarize, the encoder <em>encodes</em> the input into a rich representation, and the decoder <em>decodes</em> this representation to generate the output.”</p></li>
<li><p><strong>Discuss Real-World Considerations (If Asked or to Show Depth):</strong> “In practice, Transformers can be computationally expensive, especially for long sequences. Techniques like sparse attention are used to address this. Also, they require large amounts of training data, so transfer learning is often employed.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush. Allow time for the interviewer to process the information.</li>
<li><strong>Use Visual Aids (If Possible):</strong> If you’re in a virtual interview, consider sharing your screen and showing a diagram of the Transformer architecture.</li>
<li><strong>Gauge the Interviewer’s Level:</strong> Adapt the level of detail to the interviewer’s background. If they seem less familiar with the topic, focus on the high-level concepts. If they are more knowledgeable, delve into the mathematical details.</li>
<li><strong>Use Analogies:</strong> Relate the concepts to things the interviewer might already know. For example, you could compare self-attention to how a reader focuses on different parts of a sentence to understand its meaning.</li>
<li><strong>Be Ready to Answer Follow-Up Questions:</strong> The interviewer will likely ask questions to probe your understanding. Be prepared to elaborate on specific aspects of the architecture or discuss related topics.</li>
<li><strong>Pause and Ask for Clarification:</strong> If you are not sure you understand the question, don’t hesitate to ask for clarification. It’s better to clarify before answering than to provide an irrelevant answer.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>