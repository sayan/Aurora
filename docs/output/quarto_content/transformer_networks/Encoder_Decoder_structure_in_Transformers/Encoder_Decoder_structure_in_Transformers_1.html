<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>encoder_decoder_structure_in_transformers_1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-2.-what-role-does-multi-head-self-attention-play-in-both-the-encoder-and-decoder-how-does-the-masked-self-attention-in-the-decoder-differ-from-that-in-the-encoder" class="level2">
<h2 class="anchored" data-anchor-id="question-2.-what-role-does-multi-head-self-attention-play-in-both-the-encoder-and-decoder-how-does-the-masked-self-attention-in-the-decoder-differ-from-that-in-the-encoder">Question: 2. What role does multi-head self-attention play in both the encoder and decoder? How does the masked self-attention in the decoder differ from that in the encoder?</h2>
<p><strong>Best Answer</strong></p>
<p>Multi-head self-attention is a crucial component of the Transformer architecture, playing a vital role in both the encoder and decoder. It allows the model to attend to different parts of the input sequence and capture various relationships and dependencies. The key idea is to project the input into multiple subspaces (heads) and perform self-attention independently in each subspace. This enables the model to learn diverse representations and attend to different aspects of the input sequence simultaneously.</p>
<p><strong>Role of Multi-Head Self-Attention:</strong></p>
<ol type="1">
<li><p><strong>Parallel Attention:</strong> Multi-head attention provides a mechanism for the model to attend to different parts of the input sequence in parallel. This is achieved by splitting the input into multiple heads and computing attention independently for each head.</p></li>
<li><p><strong>Diverse Representations:</strong> Each attention head learns a different set of weights, allowing the model to capture diverse relationships and dependencies between input elements. The concatenation of these diverse representations creates a richer, more expressive representation of the input sequence.</p></li>
<li><p><strong>Capturing Long-Range Dependencies:</strong> Self-attention, in general, allows the model to directly attend to any part of the input sequence, regardless of the distance. Multi-head attention amplifies this capability by providing multiple perspectives on these dependencies.</p></li>
</ol>
<p><strong>Mathematical Formulation:</strong></p>
<p>Given an input sequence, we first transform it into three matrices: <span class="math inline">\(Q\)</span> (query), <span class="math inline">\(K\)</span> (key), and <span class="math inline">\(V\)</span> (value). For a single head, the attention is computed as:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(d_k\)</span> is the dimension of the key vectors. The scaling factor <span class="math inline">\(\sqrt{d_k}\)</span> is used to prevent the dot products from becoming too large, which can lead to small gradients after the softmax.</p>
<p>In multi-head attention, the input is projected into <span class="math inline">\(h\)</span> different heads:</p>
<p><span class="math display">\[
Q_i = XW_i^Q, \quad K_i = XW_i^K, \quad V_i = XW_i^V
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the input sequence and <span class="math inline">\(W_i^Q\)</span>, <span class="math inline">\(W_i^K\)</span>, and <span class="math inline">\(W_i^V\)</span> are the projection matrices for the <span class="math inline">\(i\)</span>-th head.</p>
<p>The attention is then computed for each head:</p>
<p><span class="math display">\[
\text{Attention}_i = \text{Attention}(Q_i, K_i, V_i)
\]</span></p>
<p>The outputs of all heads are concatenated and linearly transformed to produce the final output:</p>
<p><span class="math display">\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{Attention}_1, \dots, \text{Attention}_h)W^O
\]</span></p>
<p>where <span class="math inline">\(W^O\)</span> is the output projection matrix.</p>
<p><strong>Difference between Encoder and Decoder Self-Attention:</strong></p>
<p>The key difference lies in the masking applied in the decoder’s self-attention mechanism.</p>
<ul>
<li><p><strong>Encoder Self-Attention:</strong> In the encoder, each position can attend to all positions in the input sequence. There is no masking applied. The encoder is responsible for understanding the full context of the input.</p></li>
<li><p><strong>Decoder Masked Self-Attention:</strong> In the decoder, a mask is applied to prevent each position from attending to future positions. This is crucial for autoregressive generation, where the model generates the output sequence one token at a time, conditioned on the previously generated tokens. The mask ensures that the prediction for a given position only depends on the known outputs at previous positions. Without this mask, the decoder could “cheat” by looking at future tokens, rendering the training process useless for autoregressive generation.</p></li>
</ul>
<p><strong>Mathematical Representation of Masking:</strong></p>
<p>The mask is a matrix <span class="math inline">\(M\)</span> of the same size as <span class="math inline">\(QK^T\)</span>. The elements of <span class="math inline">\(M\)</span> are set to <span class="math inline">\(-\infty\)</span> for the positions that should be masked and <span class="math inline">\(0\)</span> otherwise. The attention calculation in the decoder becomes:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V
\]</span></p>
<p>The <span class="math inline">\(-\infty\)</span> values ensure that the softmax output is 0 for the masked positions, effectively preventing attention to those positions.</p>
<p><strong>Importance for Autoregressive Generation:</strong></p>
<p>Masked self-attention in the decoder is fundamental for autoregressive generation. It ensures that the model learns to predict the next token based only on the tokens generated so far, mimicking the real-world scenario where the future is unknown. This enables the model to generate coherent and contextually relevant sequences.</p>
<p>In summary, multi-head self-attention allows the model to focus on different parts of the input sequence simultaneously, while the masking in the decoder prevents information leakage and ensures proper autoregressive generation. These features are key to the Transformer’s success in various NLP tasks.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to present this information in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Big Picture:</strong> “Multi-head self-attention is a core component of Transformers, enabling the model to capture complex relationships within sequences. It’s used in both the encoder and decoder, but with a critical difference in the decoder: masking.”</p></li>
<li><p><strong>Explain Multi-Head Attention (without overwhelming):</strong> “Instead of performing a single attention calculation, we project the input into multiple ‘heads’. Each head learns different relationships, allowing the model to attend to the input from various perspectives simultaneously. Think of it as having multiple experts looking at the data, each focusing on something different.”</p></li>
<li><p><strong>Briefly touch on the math (only if asked to dive deeper):</strong> “Mathematically, we’re projecting the input into Query, Key, and Value matrices for each head, calculating attention using the softmax function on <span class="math inline">\(QK^T\)</span>, scaling by <span class="math inline">\(\sqrt{d_k}\)</span> to stabilize training, and then concatenating the results from each head.” (Write the attention equation <span class="math display">\[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span> on whiteboard, if available)</p></li>
<li><p><strong>Highlight the Encoder vs.&nbsp;Decoder Difference:</strong> “In the encoder, each position can attend to all other positions. This allows the encoder to fully understand the context of the input.”</p></li>
<li><p><strong>Emphasize the Masking in the Decoder (the crucial point):</strong> “The decoder is where masking comes in. Because the decoder generates the output sequence one token at a time <em>auto-regressively</em>, we <em>must</em> prevent it from ‘peeking’ at future tokens during training. We do this by applying a mask to the attention scores.”</p></li>
<li><p><strong>Explain the Mask’s Role:</strong> “The mask sets attention scores for future positions to negative infinity (or a very large negative number). This forces the softmax to output zero probability for those positions, effectively ignoring them. This ensures the decoder learns to predict each token conditioned only on the tokens it has already generated.” (If asked, write down the attention equation <span class="math display">\[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V\]</span> to show the effect of mask <span class="math inline">\(M\)</span>.)</p></li>
<li><p><strong>Connect to Autoregressive Generation:</strong> “This masking is absolutely essential for autoregressive generation because it mimics how we generate sequences in the real world – one step at a time, without knowing the future. Without masking, the model would learn to ‘cheat’ and wouldn’t be able to generate sequences correctly.”</p></li>
<li><p><strong>Summarize:</strong> “So, multi-head attention provides diverse perspectives, while masking in the decoder ensures the model learns to generate sequences in a proper autoregressive fashion.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pause and Breathe:</strong> Don’t rush through the explanation, especially when explaining the math.</li>
<li><strong>Use Analogies:</strong> Explain the concepts using analogies (e.g., multiple experts looking at data) to make them easier to understand.</li>
<li><strong>Check for Understanding:</strong> Ask the interviewer if they have any questions or if they would like you to elaborate on any specific point.</li>
<li><strong>Write on the Board (if available):</strong> Use the whiteboard to draw diagrams or write down key equations. This can help the interviewer visualize the concepts and follow your explanation.</li>
<li><strong>Tailor to the Audience:</strong> If the interviewer seems less technical, focus on the high-level concepts and avoid diving too deep into the math. If they seem more technical, be prepared to provide more detailed explanations.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>