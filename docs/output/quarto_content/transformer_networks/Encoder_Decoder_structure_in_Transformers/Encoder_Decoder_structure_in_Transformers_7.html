<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>encoder_decoder_structure_in_transformers_7</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-8.-consider-a-real-world-deployment-scenario-such-as-translating-documents-in-a-low-resource-language.-what-strategies-might-you-adopt-to-handle-noisy-or-messy-data-and-how-would-you-ensure-scalability-and-low-latency" class="level2">
<h2 class="anchored" data-anchor-id="question-8.-consider-a-real-world-deployment-scenario-such-as-translating-documents-in-a-low-resource-language.-what-strategies-might-you-adopt-to-handle-noisy-or-messy-data-and-how-would-you-ensure-scalability-and-low-latency">Question: 8. Consider a real-world deployment scenario, such as translating documents in a low-resource language. What strategies might you adopt to handle noisy or messy data, and how would you ensure scalability and low latency?</h2>
<p><strong>Best Answer</strong></p>
<p>Handling noisy data, ensuring scalability, and maintaining low latency in a real-world deployment scenario like translating documents in a low-resource language presents several challenges. Here’s a breakdown of strategies addressing each aspect:</p>
<section id="handling-noisy-and-messy-data" class="level3">
<h3 class="anchored" data-anchor-id="handling-noisy-and-messy-data">1. Handling Noisy and Messy Data</h3>
<p>Noisy data in the context of low-resource language translation can stem from various sources: OCR errors, grammatical inconsistencies, informal language usage, or even inaccuracies in the parallel corpora used for training. We need a multi-faceted approach.</p>
<ul>
<li><p><strong>Data Preprocessing and Cleaning:</strong></p>
<ul>
<li><strong>Normalization:</strong> Converting text to a uniform case (lower or upper) to reduce variance.</li>
<li><strong>Tokenization:</strong> Careful tokenization is crucial. SentencePiece or Byte-Pair Encoding (BPE) are preferred over simple word-based tokenization, as they handle out-of-vocabulary (OOV) words gracefully.</li>
<li><strong>Noise Reduction:</strong> Applying regular expressions or custom scripts to remove or correct common OCR errors or inconsistencies. For instance, removing extraneous characters or standardizing date formats.</li>
<li><strong>Spell Checking and Correction:</strong> Using spell-checking algorithms, potentially fine-tuned for the specific low-resource language if resources are available. Consider incorporating contextual information to choose the correct suggestion.</li>
<li><strong>Data Augmentation:</strong> Synthetically increasing the training data by introducing variations (e.g., back-translation, random word swaps, synonym replacement). This can improve the model’s robustness to noise. Back-translation involves translating the source language to another language and then back to the source, generating new variations.</li>
</ul></li>
<li><p><strong>Robust Model Architectures and Training Techniques:</strong></p>
<ul>
<li><strong>Transfer Learning:</strong> Leverage pre-trained multilingual models like mBART, XLM-R, or mT5. These models have been trained on a vast amount of data across many languages, capturing general linguistic knowledge that can be fine-tuned for the low-resource language.</li>
<li><strong>Fine-tuning with Noisy Data:</strong> When fine-tuning, consider using a curriculum learning approach. Start with cleaner subsets of the data and gradually introduce more noisy examples. This allows the model to first learn the basic patterns before being exposed to noise.</li>
<li><strong>Noise-Aware Training:</strong> Design loss functions that are less sensitive to noisy labels or inputs. For example, using robust loss functions like Huber loss instead of squared error loss. Or using techniques like label smoothing.</li>
<li><strong>Adversarial Training:</strong> Introduce adversarial examples during training to make the model more robust to perturbations in the input. This helps the model generalize better to noisy real-world data. The aim is to minimize the model’s performance on adversarially perturbed data, i.e.,</li>
</ul>
<p><span class="math display">\[
\min_{\theta} \mathbb{E}_{(x, y) \sim D} \max_{\delta \in S} L(f_{\theta}(x + \delta), y)
\]</span></p>
<p>where <span class="math inline">\(x\)</span> is the input, <span class="math inline">\(y\)</span> is the true label, <span class="math inline">\(\theta\)</span> is the model’s parameters, <span class="math inline">\(\delta\)</span> is a small perturbation within a set <span class="math inline">\(S\)</span>, <span class="math inline">\(f_{\theta}\)</span> is the model, <span class="math inline">\(L\)</span> is the loss function, and <span class="math inline">\(D\)</span> is the data distribution.</p>
<ul>
<li><strong>Ensemble Methods:</strong> Train multiple models and combine their predictions. This can help reduce the impact of errors made by individual models, leading to more robust overall performance.</li>
</ul></li>
</ul>
</section>
<section id="ensuring-scalability-and-low-latency" class="level3">
<h3 class="anchored" data-anchor-id="ensuring-scalability-and-low-latency">2. Ensuring Scalability and Low Latency</h3>
<p>Scalability and low latency are crucial for real-world deployment. These considerations need to be addressed from model architecture, optimization, to deployment infrastructure:</p>
<ul>
<li><strong>Model Optimization:</strong>
<ul>
<li><p><strong>Quantization:</strong> Reduce the model size and inference time by quantizing the weights and activations. Techniques like post-training quantization or quantization-aware training can be used. Convert the weights from FP32 (32-bit floating point) to INT8 (8-bit integer).</p>
<p>The basic idea is:</p>
<p><span class="math display">\[
Q(x) = scale * round(x / scale)
\]</span></p>
<p>where <span class="math inline">\(x\)</span> is the original floating-point value, <span class="math inline">\(Q(x)\)</span> is the quantized value, and <span class="math inline">\(scale\)</span> is a scaling factor.</p></li>
<li><p><strong>Pruning:</strong> Remove less important connections in the neural network to reduce its size and computational cost. Structured pruning removes entire neurons or channels, while unstructured pruning removes individual weights.</p></li>
<li><p><strong>Knowledge Distillation:</strong> Train a smaller, faster “student” model to mimic the behavior of a larger, more accurate “teacher” model. This allows the student model to achieve performance close to the teacher while being more efficient.</p></li>
<li><p><strong>Layer Fusion:</strong> Combine multiple layers into a single layer to reduce memory access and improve throughput. For example, fusing batch normalization layers into convolutional layers.</p></li>
<li><p><strong>Efficient Attention Mechanisms:</strong> Explore alternative attention mechanisms that are more computationally efficient than standard self-attention, such as linear attention or sparse attention.</p></li>
</ul></li>
<li><strong>Efficient Inference Infrastructure:</strong>
<ul>
<li><strong>Batching:</strong> Process multiple translation requests in a single batch to improve throughput.</li>
<li><strong>Caching:</strong> Cache frequently requested translations to reduce latency.</li>
<li><strong>Hardware Acceleration:</strong> Utilize GPUs, TPUs, or specialized accelerators for faster inference.</li>
<li><strong>Model Serving Frameworks:</strong> Deploy the model using frameworks like TensorFlow Serving, TorchServe, or Triton Inference Server, which are designed for high-performance inference.</li>
<li><strong>Distributed Inference:</strong> Distribute the inference workload across multiple machines or devices to handle high traffic volumes. Use techniques like model parallelism or data parallelism.</li>
<li><strong>Asynchronous Processing:</strong> Use asynchronous processing to handle translation requests without blocking the main thread, improving responsiveness.</li>
</ul></li>
<li><strong>Deployment strategies:</strong>
<ul>
<li><strong>Microservices Architecture:</strong> Breaking down the translation service into smaller, independent microservices allows for scaling specific components based on demand. For example, separating the preprocessing, translation, and postprocessing steps into different services.</li>
<li><strong>Load Balancing:</strong> Distribute incoming translation requests across multiple servers or instances to prevent overload and ensure high availability.</li>
<li><strong>Auto-scaling:</strong> Automatically adjust the number of servers or instances based on the current traffic load to maintain low latency and handle peak demand.</li>
<li><strong>Content Delivery Network (CDN):</strong> Caching translated documents at geographically distributed locations to reduce latency for users accessing the content from different regions.</li>
</ul></li>
</ul>
</section>
<section id="monitoring-and-adaptive-learning" class="level3">
<h3 class="anchored" data-anchor-id="monitoring-and-adaptive-learning">3. Monitoring and Adaptive Learning</h3>
<ul>
<li><strong>Real-time Monitoring:</strong> Implement monitoring systems to track key metrics like latency, throughput, and error rates.</li>
<li><strong>Active Learning:</strong> Continuously improve the model by actively selecting the most informative examples for labeling and retraining. This is particularly useful for low-resource languages where labeled data is scarce.</li>
<li><strong>Feedback Loops:</strong> Incorporate user feedback to identify areas where the model is performing poorly and use this feedback to improve the model.</li>
</ul>
<p>In summary, handling noisy data, ensuring scalability, and maintaining low latency in a real-world deployment scenario for low-resource language translation requires a holistic approach that combines data preprocessing, robust model architectures, model optimization, and efficient inference infrastructure. Continuous monitoring and adaptive learning are crucial for maintaining and improving the system’s performance over time.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to present this information during an interview:</p>
<ol type="1">
<li><strong>Start with the Problem Statement:</strong>
<ul>
<li>“The task of translating documents in a low-resource language presents unique challenges regarding noisy data, scalability, and latency. To address these, I’d adopt a comprehensive strategy spanning data preprocessing, model architecture, optimization, and deployment infrastructure.”</li>
</ul></li>
<li><strong>Address Noisy Data:</strong>
<ul>
<li>“First, handling noisy data: I would implement several preprocessing techniques. Normalization, cleaning using regex, more robust tokenization algorithms and spell correction, and maybe even using data augmentation like back-translation, random word swaps to increase the robustness to noise.”</li>
<li>“Then, the model architecture itself has to be trained with noisy data in mind. I’d start with transfer learning from a pre-trained multilingual model like mBART or XLM-R. Then fine-tune with noisy data using curriculum learning to first learn the basic patterns before being exposed to noise. I could even use adversarial training to make the model more robust to perturbations in the input. I could use ensemble methods too.”</li>
</ul></li>
<li><strong>Address Scalability and Latency:</strong>
<ul>
<li>“Next, for scalability and low latency: The goal is making the inference as fast as possible while keeping a good quality. Start with post-training quantization or quantization-aware training which convert the weights from FP32 to INT8 to reduce the model size and inference time.”</li>
<li>“I would use pruning to remove less important connections, and knowledge distillation to train a smaller, faster student model to mimic the behavior of a larger teacher model. Layer fusion can be used to combine multiple layers into a single layer to reduce memory access and improve throughput. Another interesting option is using Efficient Attention Mechanisms.”</li>
<li>“For the serving framework, TensorFlow Serving, TorchServe, or Triton Inference Server. They are designed for high-performance inference. We can then use Distributed Inference to split the workload across multiple machines. Batching is important to improve throughput.”</li>
<li>“I would use microservices architecture, load balancing, auto-scaling, and content delivery network (CDN) to ensure a high availability and low latency.”</li>
</ul></li>
<li><strong>Mention Monitoring and Adaptive Learning:</strong>
<ul>
<li>“Finally, ongoing monitoring is key. I would track latency, throughput, and error rates in real-time. Active learning can be used to continuously improve the model, especially since the data are scarce. User feedback is important to close the loop.”</li>
</ul></li>
<li><strong>Handling Mathematical Sections:</strong>
<ul>
<li>When you mention adversarial training: “Adversarial training involves introducing small perturbations to the input during training to make the model more robust. The goal is to minimize the model’s performance on these perturbed examples.” Don’t dive too deep into the equation unless asked.</li>
<li>When you mention quantization: “Quantization can be expressed mathematically as scaling and rounding the floating-point values to integers. Effectively, reduce memory usage and increase speed.”</li>
</ul></li>
<li><strong>Communication Tips:</strong>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the answer.</li>
<li><strong>Use clear and concise language:</strong> Avoid jargon unless necessary.</li>
<li><strong>Check for understanding:</strong> Pause periodically to ask if the interviewer has any questions.</li>
<li><strong>Be prepared to elaborate:</strong> Be ready to go into more detail on any specific area if asked.</li>
<li><strong>Show Enthusiasm:</strong> Convey your genuine interest in solving these challenges.</li>
</ul></li>
</ol>
<p>By following this structure, you can clearly articulate your understanding of the problem, your proposed solutions, and the reasoning behind them. Remember to adapt your response based on the specific requirements of the role and the interviewer’s background.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>