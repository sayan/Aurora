<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>encoder_decoder_structure_in_transformers_8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-9.-how-can-the-standard-encoder-decoder-transformer-architecture-be-adapted-for-tasks-beyond-sequence-to-sequence-such-as-summarization-or-question-answering" class="level2">
<h2 class="anchored" data-anchor-id="question-9.-how-can-the-standard-encoder-decoder-transformer-architecture-be-adapted-for-tasks-beyond-sequence-to-sequence-such-as-summarization-or-question-answering">Question: 9. How can the standard Encoder-Decoder Transformer architecture be adapted for tasks beyond sequence-to-sequence, such as summarization or question answering?</h2>
<p><strong>Best Answer</strong></p>
<p>The Transformer architecture, with its encoder-decoder structure, was initially conceived for sequence-to-sequence tasks like machine translation. However, its ability to model long-range dependencies through self-attention makes it highly adaptable to other tasks, including summarization and question answering, which inherently require understanding relationships between distant parts of the input. The key is to tailor the input representation, output decoding process, and training regime to suit the specifics of the target task.</p>
<p>Here’s a breakdown of how the Transformer can be adapted for tasks beyond simple sequence transduction:</p>
<ul>
<li><p><strong>Task-Specific Pre-training:</strong></p>
<ul>
<li>The most common technique is to leverage transfer learning. This typically involves pre-training the Transformer on a large corpus of text using objectives relevant to language understanding.</li>
<li>Examples include:
<ul>
<li><strong>Masked Language Modeling (MLM):</strong> Introduced in BERT, MLM involves randomly masking tokens in the input sequence and training the model to predict the masked tokens. This forces the model to learn contextual representations. <span class="math display">\[ P(x_i | x_1, ..., x_{i-1}, x_{i+1}, ..., x_n) \]</span> where <span class="math inline">\(x_i\)</span> is the masked token and <span class="math inline">\(x_1, ..., x_n\)</span> is the input sequence.</li>
<li><strong>Next Sentence Prediction (NSP):</strong> Also introduced in BERT, NSP involves training the model to predict whether two given sentences are consecutive in the original document. This helps the model understand inter-sentence relationships.</li>
<li><strong>Causal Language Modeling (CLM):</strong> Used in GPT, CLM trains the model to predict the next token in a sequence given the preceding tokens. <span class="math display">\[ P(x_t | x_1, ..., x_{t-1}) \]</span> where <span class="math inline">\(x_t\)</span> is the token to predict and <span class="math inline">\(x_1, ..., x_{t-1}\)</span> are the preceding tokens.</li>
</ul></li>
<li>Pre-training provides a solid foundation of language understanding, which can be fine-tuned for the specific downstream task. Models like BERT, RoBERTa, BART, and T5 are frequently used as starting points.</li>
</ul></li>
<li><p><strong>Input Representation Modification:</strong></p>
<ul>
<li>The input to the Transformer needs to be formatted appropriately for the task. For example:
<ul>
<li><strong>Summarization:</strong> The input is the source document, and the output is the summarized text. The input can be tokenized and fed into the encoder. BART is a good example of a model designed for this, using a denoising autoencoder approach combined with a standard sequence-to-sequence Transformer.</li>
<li><strong>Question Answering (QA):</strong> The input often consists of the question and the context document (the passage where the answer is likely to be found). These can be concatenated, separated by a special token (e.g., <code>[SEP]</code>), and fed into the encoder.</li>
<li>Example: <code>[CLS] Question: What is the capital of France? [SEP] Context: France is a country in Europe. The capital of France is Paris. [SEP]</code></li>
</ul></li>
<li>For QA tasks, the output might be a span within the context document that represents the answer.</li>
</ul></li>
<li><p><strong>Output Decoding Strategies:</strong></p>
<ul>
<li>The decoding process also needs to be adapted. For sequence generation tasks like summarization, common decoding strategies include:
<ul>
<li><strong>Greedy Decoding:</strong> Selects the most probable token at each step. Simple but can lead to suboptimal results. <span class="math display">\[ \hat{y}_t = \text{argmax}_{y_t} P(y_t | y_1, ..., y_{t-1}, x) \]</span> where <span class="math inline">\(\hat{y}_t\)</span> is the predicted token at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(x\)</span> is the input sequence.</li>
<li><strong>Beam Search:</strong> Maintains a beam of <em>k</em> most probable sequences at each step, expanding each sequence with the possible next tokens. This helps find higher-quality outputs than greedy decoding, but is computationally more expensive.</li>
<li><strong>Sampling-based methods:</strong> Temperature sampling, Top-k sampling, and nucleus sampling introduce randomness into the decoding process, promoting diversity in the generated text.</li>
</ul></li>
<li>For QA, the output is often a span of text. This can be modeled as predicting the start and end indices within the input context. The probability of a span (i, j) being the correct answer can be calculated as: <span class="math display">\[P(\text{span} = (i, j)) = P(\text{start} = i) \cdot P(\text{end} = j)\]</span> where <span class="math inline">\(P(\text{start} = i)\)</span> and <span class="math inline">\(P(\text{end} = j)\)</span> are the probabilities of the start and end positions being <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, respectively, as predicted by the model.</li>
</ul></li>
<li><p><strong>Attention Mechanism Modifications:</strong></p>
<ul>
<li>While the standard self-attention mechanism is powerful, modifications can sometimes improve performance:
<ul>
<li><strong>Pointer Networks:</strong> For summarization, Pointer Networks can be used to copy words directly from the source document into the summary, which is helpful for handling named entities and rare words. This can be implemented as an additional attention mechanism that attends to the input sequence.</li>
<li><strong>Coverage Mechanism:</strong> To avoid repetition in summarization, a coverage mechanism can track which parts of the source document have already been attended to during decoding, penalizing attention to those areas again.</li>
</ul></li>
</ul></li>
<li><p><strong>Fine-tuning:</strong></p>
<ul>
<li>After pre-training, the Transformer is fine-tuned on the specific target task using labeled data.</li>
<li>Fine-tuning involves updating the model’s weights to optimize performance on the task-specific objective function. This often requires careful tuning of hyperparameters like learning rate and batch size.</li>
</ul></li>
<li><p><strong>Handling Domain-Specific Context:</strong></p>
<ul>
<li>For tasks involving specific domains (e.g., legal documents, scientific papers), incorporating domain-specific knowledge can be beneficial. This can be done through:
<ul>
<li>Fine-tuning on domain-specific data.</li>
<li>Incorporating domain-specific embeddings.</li>
<li>Using knowledge graphs to provide additional context.</li>
</ul></li>
</ul></li>
<li><p><strong>Architectural Variations:</strong></p>
<ul>
<li>While the standard encoder-decoder architecture is widely used, other variations exist that can be beneficial for specific tasks.
<ul>
<li><strong>Encoder-only models (e.g., BERT):</strong> Well-suited for tasks that require understanding the input but don’t involve generating new text, such as classification and question answering.</li>
<li><strong>Decoder-only models (e.g., GPT):</strong> Excellent for text generation tasks, such as language modeling and creative writing.</li>
</ul></li>
</ul></li>
</ul>
<p>In summary, adapting the Transformer architecture for tasks beyond sequence-to-sequence involves a combination of task-specific pre-training, input representation engineering, output decoding strategy selection, and fine-tuning. These adaptations allow the Transformer to leverage its powerful attention mechanism to excel in a wide range of natural language processing tasks.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to deliver this answer in an interview:</p>
<ol type="1">
<li><strong>Start with the Core Idea:</strong>
<ul>
<li>Begin by stating that the Transformer architecture, while designed for sequence-to-sequence tasks like translation, is highly adaptable due to its self-attention mechanism. This highlights the key strength that enables its versatility.</li>
<li><em>Example:</em> “The Transformer’s strength lies in its self-attention, which allows it to model long-range dependencies effectively. This makes it adaptable to tasks beyond just sequence-to-sequence problems.”</li>
</ul></li>
<li><strong>Explain Task-Specific Pre-training (Highlight Key Examples):</strong>
<ul>
<li>Discuss the importance of pre-training and provide concrete examples like MLM, NSP, and CLM. Briefly explain what these objectives accomplish.</li>
<li><em>Example:</em> “A crucial step is pre-training the Transformer on a large corpus. Techniques like Masked Language Modeling, where we predict masked words, or Next Sentence Prediction, where we predict if two sentences follow each other, allow the model to learn rich contextual representations.”</li>
<li><em>If the interviewer seems engaged, you can briefly mention models like BERT, RoBERTa, BART, and T5.</em></li>
</ul></li>
<li><strong>Describe Input/Output Adaptation:</strong>
<ul>
<li>Explain that the input and output formats need to be tailored to the specific task. Use summarization and question answering as examples.</li>
<li><em>Example:</em> “For question answering, we might concatenate the question and context passage. For summarization, the input would be the document, and the output is the summarized text.”</li>
</ul></li>
<li><strong>Discuss Decoding Strategies (Focus on Key Methods):</strong>
<ul>
<li>Mention common decoding strategies like greedy decoding and beam search. If you discussed sampling methods, make sure you talk about them.</li>
<li><em>Example:</em> “When generating text, we use decoding strategies. Beam search helps find better outputs by considering multiple possibilities, while sampling methods can introduce more diversity.”</li>
</ul></li>
<li><strong>Optional: Briefly Mention Attention Mechanism Modifications:</strong>
<ul>
<li>Only if the interviewer seems very interested, briefly touch on modifications to the attention mechanism, such as pointer networks or coverage mechanisms.</li>
<li><em>Example:</em> “For certain tasks, we can even modify the attention mechanism itself. Pointer Networks are helpful in summarization for copying words directly from the source text.”</li>
</ul></li>
<li><strong>Emphasize Fine-tuning:</strong>
<ul>
<li>Stress the importance of fine-tuning the pre-trained Transformer on the specific task with labeled data.</li>
<li><em>Example:</em> “The final step is to fine-tune the pre-trained model on the specific task using labeled data. This is where we optimize the model for the task-specific objective.”</li>
</ul></li>
<li><strong>Consider Domain-Specific Knowledge:</strong>
<ul>
<li>If relevant to the role, mention the importance of incorporating domain-specific knowledge for tasks that involve specialized domains.</li>
<li><em>Example:</em> “For tasks in specialized fields like law or science, we can further enhance performance by incorporating domain-specific data or knowledge graphs.”</li>
</ul></li>
<li><strong>Summarize:</strong>
<ul>
<li>Conclude by reiterating that adapting the Transformer involves a combination of pre-training, input/output engineering, and fine-tuning, allowing it to be applied to a wide range of NLP tasks.</li>
<li><em>Example:</em> “In summary, adapting the Transformer for different tasks requires a combination of task-specific pre-training, input representation engineering, output decoding strategy, and fine-tuning. This allows us to unlock its potential for various NLP applications.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use visual cues:</strong> If possible, use hand gestures or draw simple diagrams to illustrate key concepts.</li>
<li><strong>Check for understanding:</strong> After explaining a complex concept, ask the interviewer if they have any questions.</li>
<li><strong>Avoid jargon:</strong> Use technical terms when necessary, but explain them clearly.</li>
<li><strong>Show enthusiasm:</strong> Demonstrate your passion for the topic.</li>
</ul>
<p><strong>Handling Mathematical Sections:</strong></p>
<ul>
<li><strong>Introduce equations:</strong> Before presenting an equation, briefly explain what it represents.</li>
<li><strong>Walk through the equation:</strong> Explain the meaning of each term and how they relate to the overall concept.</li>
<li><strong>Provide intuition:</strong> Explain the intuition behind the equation in plain English.</li>
<li><strong>Don’t get bogged down in details:</strong> Focus on the key takeaways rather than getting lost in the mathematical minutiae.</li>
<li><em>Example:</em> “Masked Language Modeling uses the equation <span class="math display">\[P(x_i | x_1, ..., x_{i-1}, x_{i+1}, ..., x_n)\]</span>. Essentially, we’re trying to predict the probability of a masked word (<span class="math inline">\(x_i\)</span>) given its surrounding context. This forces the model to learn relationships between words.”</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>