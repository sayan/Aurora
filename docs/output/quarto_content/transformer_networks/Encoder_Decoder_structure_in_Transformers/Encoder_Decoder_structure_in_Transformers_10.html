<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>encoder_decoder_structure_in_transformers_10</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-11.-what-are-some-potential-pitfalls-or-edge-cases-that-might-arise-during-the-training-of-an-encoder-decoder-transformer-on-multilingual-datasets-and-how-might-you-address-them" class="level2">
<h2 class="anchored" data-anchor-id="question-11.-what-are-some-potential-pitfalls-or-edge-cases-that-might-arise-during-the-training-of-an-encoder-decoder-transformer-on-multilingual-datasets-and-how-might-you-address-them">Question: 11. What are some potential pitfalls or edge cases that might arise during the training of an Encoder-Decoder Transformer on multilingual datasets, and how might you address them?</h2>
<p><strong>Best Answer</strong></p>
<p>Training an Encoder-Decoder Transformer on multilingual datasets presents a unique set of challenges compared to monolingual training. These pitfalls and edge cases stem from the inherent complexities of dealing with multiple languages simultaneously, including differences in vocabulary size, linguistic structure, data availability, and cultural nuances. Here’s a breakdown of the potential issues and corresponding mitigation strategies:</p>
<p><strong>1. Vocabulary Mismatches and Handling Rare Words:</strong></p>
<ul>
<li><strong>Pitfall:</strong> Each language has its own unique vocabulary. A naive approach of using separate vocabularies for each language can lead to a massive vocabulary size, increasing computational cost and memory requirements. Furthermore, some words might be rare or unseen in certain languages, leading to poor performance.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Shared Sub-word Tokenization:</strong> Techniques like Byte Pair Encoding (BPE), WordPiece, or SentencePiece learn a shared vocabulary across all languages by breaking down words into smaller sub-word units. This reduces vocabulary size and helps the model generalize to unseen words by composing them from known sub-words. For instance, the word “unbelievable” can be broken down into “un”, “believe”, and “able”, which might be present in other languages or training examples.</li>
<li>Mathematically, BPE merges the most frequent pair of symbols in the corpus iteratively until the desired vocabulary size is reached. If we have a corpus <span class="math inline">\(C\)</span> and a vocabulary <span class="math inline">\(V\)</span>, BPE aims to find a vocabulary <span class="math inline">\(V'\)</span> such that <span class="math inline">\(|V'| &lt; |V|\)</span> and the encoding of <span class="math inline">\(C\)</span> using <span class="math inline">\(V'\)</span> is efficient.</li>
<li><strong>Vocabulary Pruning:</strong> Remove infrequent tokens after sub-word tokenization to further reduce vocabulary size without significantly affecting performance.</li>
<li><strong>Special Tokens:</strong> Introduce special tokens like <code>&lt;UNK&gt;</code> (unknown), <code>&lt;BOS&gt;</code> (beginning of sequence), <code>&lt;EOS&gt;</code> (end of sequence), and language-specific tokens (e.g., <code>&lt;ENG&gt;</code>, <code>&lt;FRA&gt;</code>) to handle out-of-vocabulary words and signal language identity.</li>
</ul></li>
</ul>
<p><strong>2. Data Imbalance and Language Dominance:</strong></p>
<ul>
<li><strong>Pitfall:</strong> Multilingual datasets often exhibit significant imbalances in the amount of training data available for each language. The model might overfit to languages with abundant data (dominant languages) and perform poorly on languages with scarce data (low-resource languages).</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Data Augmentation:</strong> Artificially increase the size of low-resource language datasets by applying techniques like back-translation, synonym replacement, or random insertion/deletion.</li>
<li><strong>Back-translation</strong> involves translating a sentence from a low-resource language to a high-resource language and then back to the low-resource language. This generates new training examples while preserving the meaning.</li>
<li><strong>Sampling Strategies:</strong> Employ sampling techniques to balance the contribution of each language during training.
<ul>
<li><strong>Temperature Scaling</strong> of probabilities of sampling. Higher temperature gives more weight to under-represented languages.</li>
<li><strong>Weighted Sampling:</strong> Assign higher weights to examples from low-resource languages and lower weights to examples from high-resource languages. We can define a weight <span class="math inline">\(w_i\)</span> for each language <span class="math inline">\(i\)</span> based on its proportion in the dataset <span class="math inline">\(p_i\)</span>:</li>
</ul>
<span class="math display">\[w_i = \frac{1/p_i}{\sum_j (1/p_j)}\]</span>
<ul>
<li><strong>Oversampling:</strong> Duplicate examples from low-resource languages to match the size of high-resource language datasets. Be cautious of overfitting when oversampling significantly.</li>
<li><strong>Undersampling:</strong> Randomly remove examples from high-resource languages to match the size of low-resource language datasets. This can lead to information loss if not done carefully.</li>
</ul></li>
<li><strong>Transfer Learning and Fine-tuning:</strong> Pre-train the model on a large monolingual corpus (in a dominant language) and then fine-tune it on the multilingual dataset. This allows the model to leverage knowledge learned from the dominant language to improve performance on low-resource languages.</li>
<li><strong>Meta-Learning:</strong> Use meta-learning techniques to learn how to quickly adapt to new languages with limited data. For example, MAML (Model-Agnostic Meta-Learning) aims to find a good initial parameter set for fast fine-tuning on new tasks (languages in this case).</li>
</ul></li>
</ul>
<p><strong>3. Linguistic Differences and Cross-lingual Interference:</strong></p>
<ul>
<li><strong>Pitfall:</strong> Languages differ significantly in terms of syntax, morphology, and semantics. The model might struggle to learn representations that generalize across languages, leading to cross-lingual interference where learning one language negatively impacts performance on another.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Language-Specific Layers:</strong> Introduce language-specific layers (e.g., embeddings, attention mechanisms, or feed-forward networks) to capture language-specific features. This allows the model to learn distinct representations for each language while still sharing common parameters.</li>
<li><strong>Adversarial Training:</strong> Use adversarial training to encourage the model to learn language-invariant features. This involves training a discriminator to distinguish between languages and then training the encoder to fool the discriminator.</li>
<li><strong>Multi-task Learning:</strong> Jointly train the model on multiple tasks (e.g., machine translation, language modeling, part-of-speech tagging) to encourage the learning of more general and robust representations.</li>
<li><strong>Explicit Language Embeddings:</strong> Incorporate language embeddings as input to the model to explicitly inform the model about the language of each input sequence.</li>
</ul></li>
</ul>
<p><strong>4. Overfitting and Generalization:</strong></p>
<ul>
<li><strong>Pitfall:</strong> Training a complex Transformer model on a limited multilingual dataset can easily lead to overfitting, especially for low-resource languages. The model might memorize the training data and fail to generalize to unseen examples.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Regularization:</strong> Apply regularization techniques like L1 or L2 regularization, dropout, or weight decay to prevent overfitting.</li>
<li><strong>Early Stopping:</strong> Monitor the performance of the model on a validation set and stop training when the performance starts to degrade.</li>
<li><strong>Cross-validation:</strong> Use cross-validation to evaluate the model’s performance and ensure that it generalizes well to unseen data.</li>
<li><strong>Parameter Sharing:</strong> Strategically share parameters between languages to reduce the number of trainable parameters and improve generalization.</li>
<li><strong>Smaller Model Sizes:</strong> Experiment with smaller transformer architectures for low-resource settings where data scarcity prevents effective training of larger models.</li>
</ul></li>
</ul>
<p><strong>5. Evaluation and Benchmarking:</strong></p>
<ul>
<li><strong>Pitfall:</strong> Evaluating multilingual models can be challenging due to the lack of standardized benchmarks and evaluation metrics that account for the diverse characteristics of different languages.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Multilingual Benchmarks:</strong> Use established multilingual benchmarks like XGLUE, Flores, or MLQA to evaluate the model’s performance.</li>
<li><strong>Language-Specific Metrics:</strong> Use language-specific evaluation metrics to assess the model’s performance on each language individually. For machine translation, consider metrics like BLEU, METEOR, and CHRF.</li>
<li><strong>Human Evaluation:</strong> Conduct human evaluation to assess the quality of the model’s output, especially for tasks where automatic metrics might not be reliable.</li>
</ul></li>
</ul>
<p><strong>6. Computational Resources:</strong></p>
<ul>
<li><strong>Pitfall:</strong> Training large Transformer models on multilingual datasets requires significant computational resources, including memory, processing power, and time.</li>
<li><strong>Mitigation:</strong>
<ul>
<li><strong>Mixed Precision Training:</strong> Use mixed precision training (e.g., FP16) to reduce memory consumption and speed up training.</li>
<li><strong>Gradient Accumulation:</strong> Accumulate gradients over multiple mini-batches to simulate larger batch sizes without exceeding memory limits.</li>
<li><strong>Distributed Training:</strong> Distribute the training workload across multiple GPUs or machines to accelerate training.</li>
<li><strong>Model Parallelism:</strong> Partition the model across multiple devices to handle models that are too large to fit on a single device.</li>
</ul></li>
</ul>
<p><strong>7. Domain Mismatch:</strong> * <strong>Pitfall:</strong> If the training data for each language comes from different domains, the model might struggle to learn a unified representation that works well across all languages. * <strong>Mitigation:</strong> * <strong>Domain Adaptation:</strong> Use domain adaptation techniques to transfer knowledge from one domain to another. * <strong>Curate Domain-Aligned Datasets:</strong> Attempt to balance the domain representation across languages in the training data.</p>
<p>By carefully considering these potential pitfalls and implementing appropriate mitigation strategies, it is possible to train high-performing Encoder-Decoder Transformer models on multilingual datasets. The key is to address the challenges of vocabulary mismatches, data imbalance, linguistic differences, overfitting, evaluation difficulties, and computational limitations in a principled and systematic manner.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to present this information in an interview, focusing on clarity and depth:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“Training multilingual Transformers presents unique challenges due to differences in languages. I can discuss several pitfalls and how to address them.”</li>
<li>This sets the stage and assures the interviewer you understand the breadth of the topic.</li>
</ul></li>
<li><strong>Vocabulary Mismatches and Rare Words:</strong>
<ul>
<li>“One key issue is vocabulary. Each language has a distinct vocabulary. Using individual vocabularies leads to large model sizes. The solution is shared sub-word tokenization using BPE, WordPiece, or SentencePiece. These techniques break words into smaller units, allowing the model to generalize. BPE, for example, iteratively merges frequent symbol pairs.”</li>
<li>If the interviewer asks for more detail on BPE, explain: “BPE aims to create a smaller vocabulary <span class="math inline">\(V'\)</span> from a larger one <span class="math inline">\(V\)</span> by merging the most frequent pairs until a target size is reached. We aim to efficiently encode the corpus <span class="math inline">\(C\)</span> using <span class="math inline">\(V'\)</span>.”</li>
</ul></li>
<li><strong>Data Imbalance:</strong>
<ul>
<li>“Another major issue is data imbalance. Some languages have significantly less data. This leads to overfitting on dominant languages. To mitigate this, we can use data augmentation techniques like back-translation, where we translate to a high-resource language and back. We can also employ sampling strategies.”</li>
<li>Then offer the math: “We can use weighted sampling, assigning a weight <span class="math inline">\(w_i\)</span> to language <span class="math inline">\(i\)</span> based on its proportion <span class="math inline">\(p_i\)</span> in the dataset, like so: <span class="math inline">\(w_i = \frac{1/p_i}{\sum_j (1/p_j)}\)</span>”</li>
</ul></li>
<li><strong>Linguistic Differences and Cross-Lingual Interference:</strong>
<ul>
<li>“Linguistic differences can cause cross-lingual interference. We can address this by using language-specific layers to capture unique language features, and using adversarial training to make feature extractions less language specific”</li>
</ul></li>
<li><strong>Overfitting:</strong>
<ul>
<li>“Overfitting is a common problem, especially for low-resource languages. We address this using standard regularization techniques like L1/L2 regularization, dropout, and early stopping. Parameter sharing between languages helps too.”</li>
</ul></li>
<li><strong>Evaluation:</strong>
<ul>
<li>“Evaluating multilingual models requires using multilingual benchmarks like XGLUE or Flores, and employing language-specific evaluation metrics alongside human evaluation.”</li>
</ul></li>
<li><strong>Computational Resources:</strong>
<ul>
<li>“Training these models is computationally intensive. We can use mixed precision training, gradient accumulation, distributed training, and model parallelism to handle large models efficiently.”</li>
</ul></li>
<li><strong>Domain Mismatch:</strong>
<ul>
<li>“Another potential pitfall arises if the training data for each language comes from different domains, which can hinder the model’s ability to learn a unified representation. In these cases, domain adaptation techniques or curating domain-aligned datasets may be necessary.”</li>
</ul></li>
<li><strong>Summarize and Invite Questions:</strong>
<ul>
<li>“In summary, training multilingual Transformers requires careful consideration of vocabulary, data balance, linguistic differences, overfitting, evaluation, and computational costs. By addressing these challenges systematically, we can build effective multilingual models. Do you have any questions about these points?”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to absorb the information.</li>
<li><strong>Check for Understanding:</strong> Pause occasionally and ask if the interviewer has any questions.</li>
<li><strong>Be Flexible:</strong> Be prepared to dive deeper into any specific area that the interviewer shows interest in.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you are in a virtual interview, consider using a whiteboard or screen sharing to illustrate concepts or equations. If in person, draw on the whiteboard to show the equations.</li>
<li><strong>Focus on Clarity:</strong> Avoid jargon unless you are certain the interviewer is familiar with it. Define any technical terms you use.</li>
<li><strong>Connect Theory to Practice:</strong> Whenever possible, relate the concepts to real-world applications or examples.</li>
<li><strong>Maintain Eye Contact:</strong> If you are in a virtual interview, look directly at the camera. If you are in person, make eye contact with the interviewer.</li>
<li><strong>Be Confident:</strong> Project confidence in your knowledge and abilities.</li>
</ul>
<p><strong>Walking Through Math:</strong></p>
<ul>
<li><strong>Provide Context:</strong> Before presenting an equation, explain what it represents and why it’s important.</li>
<li><strong>Break It Down:</strong> Explain each term in the equation and its role.</li>
<li><strong>Use Simple Language:</strong> Avoid overly technical language.</li>
<li><strong>Offer Examples:</strong> Provide concrete examples to illustrate the equation.</li>
<li><strong>Don’t Assume Prior Knowledge:</strong> Assume the interviewer may not be familiar with the equation.</li>
<li><strong>Check for Understanding:</strong> Ask if the interviewer has any questions about the equation.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>