<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>encoder_decoder_structure_in_transformers_2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-3.-how-does-the-encoder-decoder-transformer-manage-variable-length-input-and-output-sequences-what-is-the-importance-of-positional-encoding-in-this-context" class="level2">
<h2 class="anchored" data-anchor-id="question-3.-how-does-the-encoder-decoder-transformer-manage-variable-length-input-and-output-sequences-what-is-the-importance-of-positional-encoding-in-this-context">Question: 3. How does the Encoder-Decoder Transformer manage variable-length input and output sequences? What is the importance of positional encoding in this context?</h2>
<p><strong>Best Answer</strong></p>
<p>The Encoder-Decoder Transformer architecture elegantly handles variable-length input and output sequences through a combination of its self-attention mechanism and the crucial addition of positional encodings. Let’s break down each aspect:</p>
<p><strong>1. Handling Variable-Length Sequences:</strong></p>
<p>Unlike recurrent neural networks (RNNs) that process sequences sequentially, Transformers operate on the entire input sequence in parallel. This parallelism is enabled by the self-attention mechanism.</p>
<ul>
<li><p><strong>Self-Attention:</strong> The self-attention mechanism allows each word in the input sequence to attend to all other words, computing a weighted average of their representations. These weights reflect the relevance of each word to the current word. This is done independently of the word’s position in the sequence (initially). The attention mechanism’s equations are as follows:</p>
<ul>
<li><p><strong>Query, Key, and Value:</strong> Each input embedding <span class="math inline">\(x_i\)</span> is linearly transformed into three vectors: Query (<span class="math inline">\(Q_i\)</span>), Key (<span class="math inline">\(K_i\)</span>), and Value (<span class="math inline">\(V_i\)</span>). These transformations are learned. The matrices <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, and <span class="math inline">\(W_V\)</span> are weight matrices. <span class="math display">\[Q = XW_Q, K = XW_K, V = XW_V\]</span></p></li>
<li><p><strong>Attention Weights:</strong> The attention weight between words <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is computed as the scaled dot product of their Query and Key vectors, followed by a softmax:</p>
<p><span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
<p>where <span class="math inline">\(d_k\)</span> is the dimensionality of the Key vectors. The scaling by <span class="math inline">\(\sqrt{d_k}\)</span> prevents the dot products from becoming too large, which can push the softmax function into regions where gradients are very small.</p></li>
<li><p><strong>Variable-Length Inputs:</strong> Since the attention mechanism operates on sets of vectors, it naturally adapts to different input lengths. The input is simply a matrix <span class="math inline">\(X\)</span> of shape (sequence length, embedding dimension), and the attention mechanism processes it without being constrained by a fixed sequence length. The output of each layer will also be a matrix of the same shape (sequence length, embedding dimension).</p></li>
</ul></li>
<li><p><strong>Encoder-Decoder Structure:</strong></p>
<ul>
<li><strong>Encoder:</strong> The encoder takes a variable-length input sequence and transforms it into a sequence of continuous representations. This encoding captures the contextual information of the input.</li>
<li><strong>Decoder:</strong> The decoder takes the encoder’s output and generates a variable-length output sequence, one element at a time. It uses an “autoregressive” approach, meaning that the prediction at each step depends on the previously generated tokens and the encoder’s output. The attention mechanism in the decoder also allows it to attend to the encoder’s output, effectively aligning the input and output sequences.</li>
<li><strong>Masking:</strong> In the decoder, a masking mechanism is used during training to prevent the decoder from “cheating” by looking at future tokens in the target sequence. This ensures that the decoder only uses information from previously generated tokens to predict the next token.</li>
</ul></li>
</ul>
<p><strong>2. Importance of Positional Encoding:</strong></p>
<p>The self-attention mechanism, while powerful, is permutation-invariant. This means that if you shuffle the order of the input words, the self-attention mechanism will produce the same output. This is because the attention mechanism computes relationships between words but doesn’t inherently understand their position in the sequence. This is a major problem because word order is critical to meaning.</p>
<p>Positional encoding addresses this limitation by injecting information about the position of each word into the input embeddings.</p>
<ul>
<li><strong>Sine/Cosine Positional Encodings:</strong> The original Transformer paper introduced sine and cosine functions with different frequencies to encode position.
<ul>
<li><strong>Formulas:</strong> <span class="math display">\[PE(pos, 2i) = sin(\frac{pos}{10000^{2i/d_{model}}})\]</span> <span class="math display">\[PE(pos, 2i+1) = cos(\frac{pos}{10000^{2i/d_{model}}})\]</span> where: * <span class="math inline">\(pos\)</span> is the position of the word in the sequence. * <span class="math inline">\(i\)</span> is the dimension index of the positional encoding vector. * <span class="math inline">\(d_{model}\)</span> is the dimensionality of the word embeddings.</li>
<li><strong>Why Sine/Cosine?</strong> These functions were chosen because they allow the model to easily learn to attend to relative positions. For any fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear transformation of <span class="math inline">\(PE_{pos}\)</span>. This makes it easier for the model to generalize to sequences longer than those seen during training. This can be proven using trigonometric identities.</li>
<li><strong>Adding to Embeddings:</strong> The positional encodings are added to the word embeddings: <span class="math display">\[x'_i = x_i + PE(i)\]</span> where <span class="math inline">\(x_i\)</span> is the original word embedding and <span class="math inline">\(x'_i\)</span> is the modified embedding that includes positional information. The <span class="math inline">\(x'_i\)</span> becomes the input <span class="math inline">\(X\)</span> to the self-attention layers described above.</li>
</ul></li>
<li><strong>Learned Positional Embeddings:</strong> An alternative to sine/cosine encodings is to learn positional embeddings directly. In this approach, each position is assigned a unique vector, which is learned during training, similarly to word embeddings. Both learned and fixed positional encodings have been shown to perform well, and the choice between them often depends on the specific task and dataset.</li>
</ul>
<p><strong>In summary:</strong> The Transformer’s ability to handle variable-length sequences stems from its parallel processing of the input via self-attention. Positional encoding is vital because it augments the word embeddings with information about the word’s location in the sequence, thereby reinstating the importance of order that would otherwise be lost due to the permutation-invariance of self-attention. Without positional encoding, the Transformer would be unable to distinguish between different word orders, which is crucial for understanding language.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with the High-Level Picture:</strong></p>
<ul>
<li>“The Transformer architecture handles variable-length input and output sequences through a combination of its self-attention mechanism and positional encodings. Unlike RNNs, which process sequences sequentially, Transformers process the entire input in parallel.”</li>
</ul></li>
<li><p><strong>Explain Self-Attention (Main Focus):</strong></p>
<ul>
<li>“The key is the self-attention mechanism. It allows each word to attend to all other words, computing a weighted average. These weights indicate the relevance of each word to the current word.”</li>
<li><em>If the interviewer seems receptive, briefly mention the Query/Key/Value concepts and the scaled dot-product attention formula:</em>
<ul>
<li>“More specifically, each word is transformed into a Query, Key, and Value vector. The attention weights are calculated using the scaled dot product of the Queries and Keys, followed by a softmax, like this: <span class="math inline">\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span>.”</li>
<li><em>Emphasize the scaling factor prevents gradients from vanishing.</em></li>
</ul></li>
<li>“Because the attention mechanism operates on sets of vectors, it can naturally adapt to different input lengths. The input just becomes a matrix of (sequence length, embedding dimension).”</li>
</ul></li>
<li><p><strong>Discuss Encoder-Decoder Structure:</strong></p>
<ul>
<li>“The encoder transforms the input sequence into a sequence of continuous representations. The decoder generates the output sequence one element at a time, attending to both the encoder’s output and the previously generated tokens.”</li>
<li><em>Mention masking in the decoder:</em> “In the decoder, a masking mechanism prevents it from ‘cheating’ by looking at future tokens during training.”</li>
</ul></li>
<li><p><strong>Highlight the Importance of Positional Encoding:</strong></p>
<ul>
<li>“Now, the crucial part is positional encoding. Self-attention is permutation-invariant, which means it doesn’t inherently understand word order. Word order, of course, is critical to the meaning of language.”</li>
<li>“Positional encoding injects information about the position of each word into the input embeddings, thus reinstating the importance of order.”</li>
</ul></li>
<li><p><strong>Explain Positional Encoding Techniques:</strong></p>
<ul>
<li>“The original paper used sine and cosine functions with different frequencies.”</li>
<li><em>If the interviewer wants more detail, give the formulas:</em>
<ul>
<li>“The formulas are: <span class="math inline">\(PE(pos, 2i) = sin(\frac{pos}{10000^{2i/d_{model}}})\)</span> and <span class="math inline">\(PE(pos, 2i+1) = cos(\frac{pos}{10000^{2i/d_{model}}})\)</span>.”</li>
<li><em>Explain the rationale:</em> “These functions were chosen because they allow the model to easily learn relative positions. There is a proof available based on trigonometric identities showing that for a fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear transformation of <span class="math inline">\(PE_{pos}\)</span>”.</li>
</ul></li>
<li>“Alternatively, we can use <em>learned</em> positional embeddings, where each position is assigned a unique vector learned during training. Both approaches work well.”</li>
</ul></li>
<li><p><strong>Conclude and Summarize:</strong></p>
<ul>
<li>“In summary, the Transformer handles variable-length sequences with self-attention, and positional encoding ensures that word order is properly taken into account. Without positional encoding, the model would be unable to distinguish between different word orders.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush. Give the interviewer time to process the information.</li>
<li><strong>Use visual cues:</strong> If you were in person, you could use hand gestures to illustrate the flow of information. In a virtual interview, consider briefly sketching a simplified Transformer diagram if allowed (check with the interviewer first).</li>
<li><strong>Pause for questions:</strong> Periodically pause and ask if the interviewer has any questions. This ensures they are following along and allows you to address any areas of confusion.</li>
<li><strong>Avoid jargon:</strong> While it’s okay to use technical terms, avoid excessive jargon. Explain concepts clearly and concisely.</li>
<li><strong>Be prepared to go deeper:</strong> The interviewer may ask follow-up questions about specific aspects of the Transformer architecture or positional encoding. Be prepared to elaborate on your explanations.</li>
<li><strong>Stay enthusiastic:</strong> Your enthusiasm for the topic will make a positive impression.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>