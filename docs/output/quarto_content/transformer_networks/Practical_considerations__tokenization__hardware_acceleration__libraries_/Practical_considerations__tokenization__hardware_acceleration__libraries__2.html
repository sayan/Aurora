<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>practical_considerations__tokenization__hardware_acceleration__libraries__2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-what-challenges-do-you-face-when-deploying-models-that-rely-on-tokenization-in-production-environments-and-what-strategies-do-you-employ-to-ensure-consistency-between-training-and-inference" class="level2">
<h2 class="anchored" data-anchor-id="question-what-challenges-do-you-face-when-deploying-models-that-rely-on-tokenization-in-production-environments-and-what-strategies-do-you-employ-to-ensure-consistency-between-training-and-inference">Question: What challenges do you face when deploying models that rely on tokenization in production environments, and what strategies do you employ to ensure consistency between training and inference?</h2>
<p><strong>Best Answer</strong></p>
<p>Deploying models that rely on tokenization in production environments presents several significant challenges. These challenges arise from the need to maintain consistency between the tokenization process used during training and the one used during inference, while also optimizing for performance and handling unexpected input. Here’s a detailed breakdown of the challenges and strategies:</p>
<p><strong>1. Tokenizer Versioning and Consistency:</strong></p>
<ul>
<li><strong>Challenge:</strong> Tokenizers can be complex, with numerous parameters and rules that define how text is split into tokens. If the tokenizer used during inference differs even slightly from the one used during training, it can lead to significant discrepancies in the input representation, resulting in degraded model performance. Imagine a scenario where, during training, a specific URL is tokenized as a single token, but during inference, a slight update to the tokenization library splits the URL into multiple tokens. This could drastically alter the model’s interpretation of the input.</li>
<li><strong>Strategy:</strong>
<ul>
<li><strong>Versioning:</strong> Implement strict version control for the tokenizer. This includes not only the tokenizer library itself (e.g., SentencePiece, Hugging Face’s Transformers tokenizers) but also the specific configuration used to initialize it. Use a dependency management system to ensure that the exact same version of the tokenizer and its dependencies are used in both training and production environments.</li>
<li><strong>Serialization:</strong> Serialize the trained tokenizer along with the model artifacts. This ensures that the exact tokenizer used during training is loaded in the production environment. Most tokenizer libraries provide methods for saving and loading the tokenizer’s configuration and vocabulary (e.g., <code>tokenizer.save_pretrained()</code> in Hugging Face’s Transformers). Store the version of the tokenizer in the model metadata for traceability.</li>
<li><strong>Testing:</strong> Develop comprehensive integration tests that specifically check the output of the tokenizer for various input strings. These tests should be run in both the training and production environments to verify that the tokenization process is identical.</li>
</ul></li>
</ul>
<p><strong>2. Handling Out-of-Vocabulary (OOV) Tokens:</strong></p>
<ul>
<li><strong>Challenge:</strong> During inference, the model may encounter words or tokens that were not present in the training vocabulary (OOV tokens). How these tokens are handled can significantly impact model performance. A naive approach of simply ignoring OOV tokens can lead to information loss, while treating all OOV tokens the same can mask important distinctions between them.</li>
<li><strong>Strategy:</strong>
<ul>
<li><strong><code>&lt;UNK&gt;</code> Token:</strong> Replace OOV tokens with a special <code>&lt;UNK&gt;</code> token during both training and inference. This teaches the model to handle unknown words gracefully.</li>
<li><strong>Subword Tokenization:</strong> Use subword tokenization algorithms (e.g., Byte Pair Encoding (BPE), WordPiece, SentencePiece) that break down words into smaller, more frequent subword units. This reduces the number of OOV tokens, as many unseen words can be represented by combinations of known subwords. For example, the word “unseen” might be broken down into “un” + “seen,” both of which are likely to be in the vocabulary.</li>
<li><strong>Character-Level Fallback:</strong> As a last resort, consider falling back to character-level tokenization for OOV tokens. This can capture some information about the unknown word based on its individual characters.</li>
<li><strong>Dynamic Vocabulary Updates:</strong> In some scenarios, it might be feasible to periodically update the vocabulary with new words encountered during inference. However, this requires careful monitoring and retraining to avoid introducing inconsistencies.</li>
</ul></li>
</ul>
<p><strong>3. Synchronization Between Training and Production Pipelines:</strong></p>
<ul>
<li><strong>Challenge:</strong> The tokenization process is often part of a larger data preprocessing pipeline. Ensuring that all steps in this pipeline are consistent between training and production can be complex, especially when dealing with distributed systems or different programming languages.</li>
<li><strong>Strategy:</strong>
<ul>
<li><strong>Infrastructure as Code (IaC):</strong> Use IaC tools to define and provision the infrastructure for both training and production environments. This ensures that the environments are identical, reducing the risk of inconsistencies.</li>
<li><strong>Containerization:</strong> Package the tokenization pipeline (including the tokenizer, its dependencies, and any preprocessing code) into a container image (e.g., Docker). This ensures that the same code and environment are used in both training and production.</li>
<li><strong>Feature Store:</strong> Use a feature store to manage and serve the preprocessed data. This provides a centralized repository for features, ensuring that the same features are used in both training and inference.</li>
<li><strong>Monitoring:</strong> Implement monitoring to detect discrepancies between the data distributions in training and production. If significant differences are detected, it may be necessary to retrain the model or adjust the tokenization pipeline.</li>
</ul></li>
</ul>
<p><strong>4. Performance Optimization:</strong></p>
<ul>
<li><strong>Challenge:</strong> Tokenization can be a computationally expensive process, especially for large volumes of text. Optimizing the tokenization pipeline is crucial for achieving acceptable latency in production environments.</li>
<li><strong>Strategy:</strong>
<ul>
<li><strong>Batch Processing:</strong> Tokenize input text in batches to leverage parallelism and reduce overhead.</li>
<li><strong>Hardware Acceleration:</strong> Utilize hardware acceleration (e.g., GPUs) to speed up the tokenization process. Some tokenizer libraries provide GPU-optimized implementations.</li>
<li><strong>Caching:</strong> Cache the results of tokenization for frequently occurring input strings. This can significantly reduce latency for common queries. However, be careful to invalidate the cache when the tokenizer or its configuration is updated.</li>
<li><strong>Tokenizer Selection:</strong> Carefully choose a tokenizer that balances accuracy and performance. Some tokenizers are faster than others, depending on the algorithm and implementation.</li>
</ul></li>
</ul>
<p><strong>5. Error Propagation and Debugging:</strong></p>
<ul>
<li><strong>Challenge:</strong> Errors in the tokenization process can propagate through the entire model, leading to unpredictable results. Debugging these errors can be difficult, especially in complex systems.</li>
<li><strong>Strategy:</strong>
<ul>
<li><strong>Logging:</strong> Implement detailed logging throughout the tokenization pipeline. This should include the input text, the tokenized output, and any error messages.</li>
<li><strong>Unit Testing:</strong> Write thorough unit tests for each component of the tokenization pipeline.</li>
<li><strong>Visualization:</strong> Visualize the tokenization process to identify potential errors. This can be done by displaying the input text alongside the corresponding tokens.</li>
</ul></li>
</ul>
<p><strong>6. Handling Special Characters and Encoding Issues:</strong></p>
<ul>
<li><strong>Challenge:</strong> Real-world text data often contains special characters, emojis, and encoding issues that can cause problems for tokenizers.</li>
<li><strong>Strategy:</strong>
<ul>
<li><strong>Normalization:</strong> Normalize the input text by converting it to a consistent encoding (e.g., UTF-8), removing or replacing special characters, and handling encoding errors.</li>
<li><strong>Tokenizer Configuration:</strong> Configure the tokenizer to handle special characters appropriately. Some tokenizers provide options for specifying the set of characters to include in the vocabulary.</li>
<li><strong>Preprocessing:</strong> Implement preprocessing steps to remove or replace emojis, handle URLs, and perform other text cleaning tasks.</li>
</ul></li>
</ul>
<p><strong>Mathematical Considerations (Illustrative Examples):</strong></p>
<p>While tokenization itself doesn’t typically involve complex mathematical formulas, the subsequent embedding and modeling steps do. Let’s consider how tokenization influences these:</p>
<ul>
<li><strong>Word Embeddings:</strong> Tokenization transforms text into a sequence of tokens, which are then typically converted into numerical representations using word embeddings. A simple example is one-hot encoding. If we have a vocabulary of size <span class="math inline">\(V\)</span>, each token is represented as a vector of length <span class="math inline">\(V\)</span> with a 1 at the index corresponding to the token and 0s everywhere else.</li>
</ul>
<p><span class="math display">\[
\text{One-hot Encoding of Token } t_i = [0, 0, ..., 1, ..., 0] \in \mathbb{R}^V
\]</span></p>
<p>Where the ‘1’ is at the index corresponding to the token <span class="math inline">\(t_i\)</span>. A mismatch in tokenization leads to entirely different one-hot vectors, hence a total disruption of the model.</p>
<ul>
<li><strong>Subword Embeddings (BPE Example):</strong> Byte Pair Encoding (BPE) merges frequently occurring character sequences into new tokens. Let <span class="math inline">\(C\)</span> be the set of characters in the training data. BPE iteratively merges the most frequent pair of symbols until a desired vocabulary size <span class="math inline">\(V\)</span> is reached. The probability of a sequence of subwords being merged is proportional to its frequency in the corpus.</li>
</ul>
<p><span class="math display">\[
\text{merge}(x, y) = \text{argmax}_{x, y \in V} \text{count}(xy)
\]</span></p>
<p>Where <span class="math inline">\(count(xy)\)</span> is the number of times the sequence <span class="math inline">\(xy\)</span> appears in the corpus. Again, consistency in applying this merge rule is key between training and deployment.</p>
<p>By carefully addressing these challenges and implementing the strategies outlined above, you can ensure that your models perform reliably and consistently in production environments.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this to an interviewer:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong></p>
<ul>
<li>“Deploying models with tokenization in production presents several key challenges related to consistency between training and inference. These challenges revolve around tokenizer versioning, handling out-of-vocabulary tokens, pipeline synchronization, performance optimization, and error management.”</li>
</ul></li>
<li><p><strong>Address Tokenizer Versioning and Consistency:</strong></p>
<ul>
<li>“One of the most critical aspects is ensuring that the <em>exact same</em> tokenizer is used in both training and production. Even subtle differences in the tokenizer’s rules can lead to significant performance degradation.”</li>
<li>“To address this, we implement strict version control for the tokenizer and its configuration. We serialize the trained tokenizer alongside the model artifacts to guarantee that the correct version is loaded in production. Rigorous integration tests also verify the tokenizer’s output in both environments.”</li>
</ul></li>
<li><p><strong>Discuss Handling Out-of-Vocabulary (OOV) Tokens:</strong></p>
<ul>
<li>“Handling OOV tokens is another major concern. We typically use a combination of techniques, including replacing OOV tokens with a special <code>&lt;UNK&gt;</code> token, employing subword tokenization algorithms like BPE or WordPiece, and potentially falling back to character-level tokenization.”</li>
<li>“Subword tokenization is particularly effective because it breaks down words into smaller, more frequent units, reducing the number of OOV tokens and allowing the model to generalize better to unseen words. For example, ‘unseen’ could become ‘un’ + ‘seen.’”</li>
</ul></li>
<li><p><strong>Explain Synchronization Between Training and Production Pipelines:</strong></p>
<ul>
<li>“Ensuring consistency across the entire data preprocessing pipeline is also crucial. We use Infrastructure as Code (IaC) to provision identical environments, containerization with Docker to package the tokenization pipeline, and feature stores to manage and serve preprocessed data.”</li>
</ul></li>
<li><p><strong>Address Performance Optimization:</strong></p>
<ul>
<li>“Tokenization can be computationally expensive, so we focus on optimization techniques such as batch processing, hardware acceleration with GPUs, and caching frequently occurring input strings.”</li>
</ul></li>
<li><p><strong>Discuss Error Propagation and Debugging:</strong></p>
<ul>
<li>“Finally, we emphasize logging, unit testing, and visualization to proactively identify and address potential errors in the tokenization process. Detailed logging of the input text, tokenized output, and error messages helps us quickly diagnose and resolve issues.”</li>
</ul></li>
<li><p><strong>Mention Encoding and Special Characters</strong></p>
<ul>
<li>“Real-world text data can contain special characters or be encoded in a variety of formats. Thus, the first step is to normalize the input text to consistent encoding before the tokenization”</li>
</ul></li>
<li><p><strong>Illustrate with an Example (Optional, depending on interviewer’s interest):</strong></p>
<ul>
<li>“For example, imagine that during training, a specific URL is tokenized as a single token, but during inference, a library update causes it to be split into multiple tokens. This difference in input representation could significantly impact the model’s interpretation and performance.”</li>
</ul></li>
<li><p><strong>Mathematical Touch (Optional, gauge the audience):</strong></p>
<ul>
<li>“While the tokenization process itself might not have complex equations, the resulting token representation directly impacts subsequent steps like embedding. Consider one-hot encoding: a token mismatch means an entirely different one-hot vector, disrupting the model’s input. Or with BPE, the consistency of merging subwords is crucial to matching the training data representation.” (Present the equations from the “Best Answer” section if the interviewer probes deeper).</li>
</ul></li>
<li><p><strong>Conclude with a Summary:</strong></p>
<ul>
<li>“In summary, successful deployment of models with tokenization requires careful attention to detail, robust version control, sophisticated OOV handling, pipeline synchronization, performance optimization, and thorough error management. By addressing these challenges proactively, we can ensure that our models perform reliably and consistently in production.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Take your time to articulate each point clearly.</li>
<li><strong>Use clear and concise language:</strong> Avoid jargon unless you are certain that the interviewer is familiar with it.</li>
<li><strong>Provide examples:</strong> Use concrete examples to illustrate your points and make them more relatable.</li>
<li><strong>Pause for questions:</strong> Give the interviewer opportunities to ask questions and clarify any points that they may not understand.</li>
<li><strong>Adapt to the interviewer:</strong> Adjust your level of detail and technical depth based on the interviewer’s background and interest. If they seem interested in a particular aspect, delve deeper into it. If they seem less interested, move on to the next point.</li>
<li><strong>Confidence:</strong> Show confidence in your knowledge and experience. Speak clearly and assertively.</li>
</ul>
<p>By following these guidelines, you can effectively communicate your understanding of the challenges and strategies involved in deploying models with tokenization in production environments, demonstrating your expertise and suitability for a senior-level role.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>