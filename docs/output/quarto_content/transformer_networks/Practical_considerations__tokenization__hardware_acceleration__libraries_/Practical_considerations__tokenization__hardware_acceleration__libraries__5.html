<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>practical_considerations__tokenization__hardware_acceleration__libraries__5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-how-do-libraries-such-as-tensorflow-pytorch-or-hugging-face-facilitate-practical-considerations-like-tokenization-and-hardware-acceleration-can-you-compare-their-strengths-and-weaknesses" class="level2">
<h2 class="anchored" data-anchor-id="question-how-do-libraries-such-as-tensorflow-pytorch-or-hugging-face-facilitate-practical-considerations-like-tokenization-and-hardware-acceleration-can-you-compare-their-strengths-and-weaknesses">Question: How do libraries such as TensorFlow, PyTorch, or Hugging Face facilitate practical considerations like tokenization and hardware acceleration? Can you compare their strengths and weaknesses?</h2>
<p><strong>Best Answer</strong></p>
<p>TensorFlow, PyTorch, and Hugging Face provide abstractions and tools that greatly simplify complex tasks like tokenization and hardware acceleration, which are crucial for deep learning workflows. Each library, however, approaches these tasks with its own distinct philosophy and implementation, leading to various strengths and weaknesses.</p>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<p>Tokenization is the process of breaking down text into smaller units (tokens) which can be processed by a machine learning model. Different libraries offer varying degrees of pre-built tokenizers and extensibility:</p>
<ul>
<li><strong>TensorFlow:</strong>
<ul>
<li>Provides <code>tf.keras.preprocessing.text.Tokenizer</code> for basic tokenization tasks. This covers splitting text into words and creating a vocabulary index.</li>
<li>TensorFlow Text offers more advanced tokenization options, including subword tokenization (e.g., WordPiece, SentencePiece) and Unicode normalization.</li>
<li>TensorFlow Text makes efficient use of TensorFlow graphs, which can be optimized for both CPU and GPU. It also supports streaming for large datasets.</li>
<li><strong>Strength:</strong> Tight integration with the TensorFlow ecosystem, allowing for seamless inclusion of tokenization within TensorFlow graphs. Good performance and support for multiple languages with TensorFlow Text.</li>
<li><strong>Weakness:</strong> The <code>tf.keras.preprocessing.text.Tokenizer</code> is relatively basic compared to the tokenizers offered by Hugging Face. Requires more manual effort for complex tokenization schemes if not using TensorFlow Text.</li>
</ul></li>
<li><strong>PyTorch:</strong>
<ul>
<li>PyTorch itself doesn’t offer built-in tokenization tools as comprehensive as TensorFlow or Hugging Face.</li>
<li>Relies on external libraries such as <code>torchtext</code> and <code>transformers</code> (from Hugging Face) for tokenization. <code>torchtext</code> provides utilities for data processing, including tokenization, vocabulary building, and batching.</li>
<li><strong>Strength:</strong> Highly flexible; allows users to integrate any custom tokenization pipeline. Integration with Hugging Face <code>transformers</code> gives access to a wide range of pre-trained tokenizers.</li>
<li><strong>Weakness:</strong> Requires more manual setup and integration of external libraries. <code>torchtext</code> has been historically criticized for its API complexity.</li>
</ul></li>
<li><strong>Hugging Face Transformers:</strong>
<ul>
<li>Offers a dedicated <code>tokenizers</code> library, providing fast and efficient tokenizers implemented in Rust with Python bindings. This library includes implementations of WordPiece, BPE, SentencePiece, and other popular tokenization algorithms.</li>
<li>Provides pre-trained tokenizers corresponding to many pre-trained models, making it easy to use the same tokenization scheme used during pre-training.</li>
<li>Supports both fast (Rust-based) and slow (Python-based) tokenizers. The fast tokenizers offer significant performance improvements.</li>
<li><strong>Strength:</strong> State-of-the-art tokenization capabilities, wide range of pre-trained tokenizers, and excellent performance. Easy to use and integrate with pre-trained models.</li>
<li><strong>Weakness:</strong> Tightly coupled with the Transformers ecosystem. Might require more effort to integrate into non-Transformers-based workflows. Adds a dependency on Rust, which can increase build complexity.</li>
</ul></li>
</ul>
<p><em>Mathematical Formulation of Tokenization</em> Consider tokenizing a sentence <span class="math inline">\(S\)</span> of length <span class="math inline">\(n\)</span> into a sequence of tokens <span class="math inline">\(T = \{t_1, t_2, ..., t_m\}\)</span> where <span class="math inline">\(m\)</span> is the number of tokens and <span class="math inline">\(m \leq n\)</span>. A tokenizer function <span class="math inline">\(f\)</span> maps the sentence <span class="math inline">\(S\)</span> to the token sequence <span class="math inline">\(T\)</span>: <span class="math display">\[
f(S) \rightarrow T
\]</span> Subword tokenization algorithms like WordPiece and BPE iteratively merge frequent character sequences into single tokens, reducing the vocabulary size. The goal is to minimize the description length of the data. In BPE, given a corpus <span class="math inline">\(C\)</span>, we merge the most frequent pair of tokens <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> into a new token <span class="math inline">\(ab\)</span> until the desired vocabulary size is reached. The merging operation can be expressed as: <span class="math display">\[
(a, b) = \text{argmax}_{(x, y)} \text{count}(xy)
\]</span> where <span class="math inline">\(\text{count}(xy)\)</span> is the frequency of the token pair <span class="math inline">\(xy\)</span> in the corpus <span class="math inline">\(C\)</span>.</p>
</section>
<section id="hardware-acceleration" class="level3">
<h3 class="anchored" data-anchor-id="hardware-acceleration">Hardware Acceleration</h3>
<p>Hardware acceleration, primarily using GPUs (Graphics Processing Units) and TPUs (Tensor Processing Units), is essential for training and inference of deep learning models.</p>
<ul>
<li><strong>TensorFlow:</strong>
<ul>
<li>Provides excellent support for GPU acceleration using NVIDIA’s CUDA and cuDNN libraries.</li>
<li>Supports distributed training across multiple GPUs and TPUs.</li>
<li>TensorFlow’s XLA (Accelerated Linear Algebra) compiler can further optimize computations for specific hardware. XLA performs graph-level optimizations, such as operator fusion and memory allocation, to improve performance.</li>
<li>TPU support is a major strength, allowing for extremely fast training on Google’s custom hardware. TPUs require code to be written using TensorFlow’s graph execution model and optimized for the TPU architecture.</li>
<li><strong>Strength:</strong> Strong GPU support, excellent TPU support, and XLA compiler for optimization. Mature and well-tested distributed training capabilities.</li>
<li><strong>Weakness:</strong> Can sometimes be more complex to debug GPU-related issues compared to PyTorch. XLA compilation can add overhead to the initial training stages.</li>
</ul></li>
<li><strong>PyTorch:</strong>
<ul>
<li>Also provides excellent support for GPU acceleration using CUDA.</li>
<li>Offers a more Pythonic and dynamic programming style, which can make debugging easier.</li>
<li>Supports distributed training using <code>torch.distributed</code> package, which provides various communication backends (e.g., NCCL, Gloo, MPI).</li>
<li>PyTorch has better tooling and ecosystem for GPU-accelerated research and prototyping.</li>
<li><strong>Strength:</strong> Easy to use and debug with a dynamic computation graph. Strong GPU support and a growing ecosystem of GPU-accelerated libraries.</li>
<li><strong>Weakness:</strong> TPU support is not as mature as TensorFlow’s. Requires more manual effort for distributed training setup compared to some TensorFlow configurations.</li>
</ul></li>
<li><strong>Hugging Face Transformers:</strong>
<ul>
<li>Leverages the hardware acceleration capabilities of the underlying TensorFlow or PyTorch framework.</li>
<li>Provides abstractions for running models on GPUs and TPUs.</li>
<li>Offers utilities for distributed training, simplifying the process of training large models across multiple devices.</li>
<li>The <code>accelerate</code> library abstracts away the differences between various hardware setups and frameworks, allowing to run the same code on CPU, GPU or TPU.</li>
<li><strong>Strength:</strong> Simplifies hardware acceleration through abstractions and utilities. <code>accelerate</code> allows code to remain agnostic to the specific hardware used.</li>
<li><strong>Weakness:</strong> Relies on the underlying framework for hardware acceleration. Does not provide its own low-level hardware acceleration implementations.</li>
</ul></li>
</ul>
<p><em>Mathematical Description of Hardware Acceleration</em> Hardware acceleration speeds up matrix operations, which are fundamental to neural networks. Consider a matrix multiplication <span class="math inline">\(C = AB\)</span>, where <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times k\)</span> matrix, <span class="math inline">\(B\)</span> is a <span class="math inline">\(k \times n\)</span> matrix, and <span class="math inline">\(C\)</span> is an <span class="math inline">\(m \times n\)</span> matrix. The standard algorithm requires <span class="math inline">\(m \cdot n \cdot k\)</span> operations. <span class="math display">\[
C_{ij} = \sum_{l=1}^{k} A_{il} B_{lj}
\]</span> GPUs and TPUs parallelize this operation across multiple cores, significantly reducing the computation time. The speedup can be approximated by: <span class="math display">\[
\text{Speedup} = \frac{\text{Time on CPU}}{\text{Time on GPU}} \approx \frac{\text{Number of CPU Cores}}{\text{Number of GPU Cores}}
\]</span> This is a simplified view; actual speedup depends on factors like memory bandwidth, communication overhead, and kernel optimization.</p>
</section>
<section id="comparison-table" class="level3">
<h3 class="anchored" data-anchor-id="comparison-table">Comparison Table</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 28%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>TensorFlow</th>
<th>PyTorch</th>
<th>Hugging Face Transformers</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tokenization</td>
<td><code>tf.keras.preprocessing.text.Tokenizer</code>, TensorFlow Text</td>
<td><code>torchtext</code>, Hugging Face <code>transformers</code></td>
<td><code>tokenizers</code> library</td>
</tr>
<tr class="even">
<td>Hardware Acceleration</td>
<td>Strong GPU and TPU support, XLA compiler</td>
<td>Strong GPU support, growing ecosystem</td>
<td>Leverages underlying framework’s acceleration</td>
</tr>
<tr class="odd">
<td>Ease of Use</td>
<td>Can be complex for debugging, good tooling</td>
<td>More Pythonic, easier debugging</td>
<td>High-level API, simplifies many tasks</td>
</tr>
<tr class="even">
<td>Ecosystem</td>
<td>Mature and large</td>
<td>Growing rapidly</td>
<td>Focused on NLP, strong model hub</td>
</tr>
<tr class="odd">
<td>Deployment</td>
<td>TensorFlow Serving, TensorFlow Lite</td>
<td>TorchServe, PyTorch Mobile</td>
<td>Integrated with TensorFlow and PyTorch deployment solutions</td>
</tr>
</tbody>
</table>
<p>In summary, TensorFlow excels in production environments with its robust deployment options and TPU support. PyTorch is favored for research and rapid prototyping due to its flexibility and ease of debugging. Hugging Face Transformers provides state-of-the-art NLP tools and simplifies many common tasks but relies on the underlying framework for core functionalities. The choice of library depends on the specific requirements of the project.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on delivering this answer in an interview, focusing on clarity and demonstrating expertise without overwhelming the interviewer:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong></p>
<ul>
<li>“Tokenization and hardware acceleration are critical for modern deep learning. TensorFlow, PyTorch, and Hugging Face offer different ways to handle these, each with its own strengths.” This sets the stage and avoids immediately diving into details.</li>
</ul></li>
<li><p><strong>Discuss Tokenization:</strong></p>
<ul>
<li>“Let’s start with tokenization. This is how we turn text into something our models can understand. TensorFlow provides <code>tf.keras.preprocessing.text.Tokenizer</code> for basic tasks. TensorFlow Text for advanced. PyTorch relies more on external libraries like <code>torchtext</code> and the Hugging Face <code>transformers</code> library.”</li>
<li>“Hugging Face really shines here. Their <code>tokenizers</code> library is incredibly efficient and provides pre-trained tokenizers for almost any model you can think of.”</li>
<li><em>(If asked for details on tokenization algorithms like BPE):</em> “Algorithms like BPE iteratively merge frequent character pairs into single tokens to reduce the vocabulary size. The goal is to find the optimal balance between vocabulary size and sequence length.” <em>Do not dive into the equations unless prompted. Be prepared to provide the BPE equations.</em></li>
</ul></li>
<li><p><strong>Move to Hardware Acceleration:</strong></p>
<ul>
<li>“Next, hardware acceleration is essential for performance. TensorFlow and PyTorch both have excellent support for GPUs using CUDA.”</li>
<li>“TensorFlow has a strong edge with TPUs, Google’s specialized hardware. PyTorch, being more Pythonic, sometimes makes GPU debugging easier. The <code>accelerate</code> library allows code to be run agnostic to the hardware being used.”</li>
<li><em>(If asked about XLA):</em> “TensorFlow’s XLA compiler performs graph-level optimizations which can boost performance on CPUs, GPUs, and TPUs, but this does come with added compilation time.”</li>
<li><em>(If asked about the mathematics)</em> “Fundamentally, hardware acceleration speeds up matrix operations, and the speedup is roughly proportional to the ratio of cores on the GPU vs.&nbsp;CPU. Of course, other factors like memory bandwidth play a crucial role.”</li>
</ul></li>
<li><p><strong>Provide a Summary Comparison (Refer to the Table):</strong></p>
<ul>
<li>“To summarize, TensorFlow is great for production and TPUs. PyTorch excels in research and ease of use. Hugging Face simplifies NLP tasks and provides state-of-the-art tokenization. Choosing the right tool depends on the specific project.”</li>
</ul></li>
<li><p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Speak clearly and avoid rushing. Pause after key points to allow the interviewer to digest the information.</li>
<li><strong>Use “Signposts”:</strong> Use phrases like “Now, let’s move on to…” or “In summary…” to guide the interviewer through your answer.</li>
<li><strong>Check for Understanding:</strong> Periodically ask, “Does that make sense?” or “Would you like me to elaborate on any of those points?”</li>
<li><strong>Be Ready to Dive Deeper:</strong> Have the mathematical details and inner workings ready in case the interviewer asks for more depth. However, avoid dumping all the technical details at once.</li>
<li><strong>Highlight Practical Experience:</strong> If you have experience using these libraries for real-world projects, mention them briefly to demonstrate practical application of your knowledge.</li>
<li><strong>Acknowledge Trade-offs:</strong> Emphasize that there is no one-size-fits-all answer and that the choice depends on the specific context and requirements.</li>
</ul></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>