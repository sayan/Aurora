<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>practical_considerations__tokenization__hardware_acceleration__libraries__9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-what-best-practices-do-you-follow-when-developing-and-deploying-libraries-for-tokenization-and-hardware-accelerated-model-inference-to-ensure-scalability-and-maintainability" class="level2">
<h2 class="anchored" data-anchor-id="question-what-best-practices-do-you-follow-when-developing-and-deploying-libraries-for-tokenization-and-hardware-accelerated-model-inference-to-ensure-scalability-and-maintainability">Question: What best practices do you follow when developing and deploying libraries for tokenization and hardware-accelerated model inference to ensure scalability and maintainability?</h2>
<p><strong>Best Answer</strong></p>
<p>Developing and deploying libraries for tokenization and hardware-accelerated model inference requires careful attention to several key areas to ensure scalability, maintainability, and performance. Here’s a breakdown of best practices I follow, covering design, implementation, testing, deployment, and monitoring:</p>
<p><strong>1. Modular Design and Abstraction</strong></p>
<ul>
<li><strong>Clear Separation of Concerns:</strong> Divide the library into distinct modules, each responsible for a specific task. This includes modules for:
<ul>
<li>Tokenization (e.g., subword tokenization, byte-pair encoding).</li>
<li>Hardware acceleration (e.g., using CUDA, TensorRT, ONNX Runtime).</li>
<li>Model loading and management.</li>
<li>Input/output processing.</li>
</ul></li>
<li><strong>Abstraction Layers:</strong> Introduce abstraction layers to hide implementation details and provide a stable API for users. This allows us to swap out underlying hardware or tokenization algorithms without breaking existing code. For instance, define an abstract <code>Tokenizer</code> class with methods like <code>tokenize()</code> and <code>detokenize()</code>, and then implement concrete subclasses for different tokenization methods.</li>
<li><strong>Interfaces and Protocols:</strong> Use well-defined interfaces for communication between modules. This enhances modularity and testability. For example, input and output data structures can be defined as protocols or schemas (e.g., using Protobuf or FlatBuffers) to ensure compatibility and efficient serialization.</li>
</ul>
<p><strong>2. Code Quality and Standards</strong></p>
<ul>
<li><strong>Coding Style and Conventions:</strong> Adhere to a consistent coding style guide (e.g., PEP 8 for Python, Google C++ Style Guide for C++) and enforce it using linters and formatters (e.g., <code>flake8</code>, <code>black</code>, <code>clang-format</code>).</li>
<li><strong>Code Reviews:</strong> Implement a rigorous code review process to catch errors, enforce coding standards, and share knowledge among team members.</li>
<li><strong>Documentation:</strong> Write comprehensive documentation for all modules, classes, and functions. Use tools like Sphinx (for Python) or Doxygen (for C++) to generate API documentation. Provide clear examples of how to use the library.</li>
</ul>
<p><strong>3. Testing</strong></p>
<ul>
<li><strong>Unit Tests:</strong> Write unit tests for each module to verify its functionality. Use a testing framework like <code>pytest</code> (Python) or Google Test (C++). Aim for high test coverage (e.g., &gt;80%). Focus on testing edge cases and boundary conditions.</li>
<li><strong>Integration Tests:</strong> Write integration tests to verify that different modules work together correctly. Simulate real-world scenarios and test end-to-end workflows.</li>
<li><strong>Performance Benchmarks:</strong> Create performance benchmarks to measure the speed and memory usage of the library. Use profiling tools (e.g., <code>perf</code>, <code>nvprof</code>) to identify bottlenecks. Track performance metrics over time to detect regressions.</li>
<li><strong>Hardware-Specific Tests:</strong> Test the library on different hardware platforms (e.g., different GPUs, CPUs) to ensure compatibility and performance.</li>
<li><strong>Fuzz Testing:</strong> Employ fuzzing techniques to uncover vulnerabilities and unexpected behavior by feeding the library with randomly generated inputs.</li>
</ul>
<p><strong>4. Hardware Acceleration</strong></p>
<ul>
<li><strong>Targeted Optimization:</strong> Profile the model and identify the most computationally intensive parts. Focus hardware acceleration efforts on those parts.</li>
<li><strong>Framework Selection:</strong> Choose a hardware acceleration framework that is appropriate for the task. Options include:
<ul>
<li><strong>CUDA/cuDNN:</strong> For NVIDIA GPUs, provides low-level control and maximum performance.</li>
<li><strong>TensorRT:</strong> An NVIDIA SDK for high-performance deep learning inference. Optimizes models for specific GPUs.</li>
<li><strong>ONNX Runtime:</strong> A cross-platform inference engine that supports a wide range of hardware. Good for portability.</li>
<li><strong>Intel oneAPI:</strong> For Intel CPUs and GPUs, provides a unified programming model.</li>
</ul></li>
<li><strong>Quantization and Pruning:</strong> Reduce model size and improve inference speed by using quantization (e.g., converting weights from FP32 to INT8) and pruning (removing unnecessary connections in the network).</li>
<li><strong>Kernel Fusion:</strong> Combine multiple operations into a single kernel to reduce kernel launch overhead. This can significantly improve performance, especially for small operations.</li>
<li><strong>Asynchronous Execution:</strong> Overlap data transfers and kernel execution to hide latency. Use CUDA streams or asynchronous API calls.</li>
<li><strong>Memory Management:</strong> Optimize memory usage to minimize data transfers between CPU and GPU. Use pinned memory to improve transfer speeds. Consider using memory pools to reduce allocation overhead.</li>
</ul>
<p><strong>5. Tokenization</strong></p>
<ul>
<li><strong>Algorithm Selection:</strong> Choose a tokenization algorithm that is appropriate for the language and task. Options include:
<ul>
<li><strong>WordPiece:</strong> Used in BERT and other models. Splits words into subwords based on frequency.</li>
<li><strong>Byte-Pair Encoding (BPE):</strong> A data compression algorithm that can be used for subword tokenization.</li>
<li><strong>SentencePiece:</strong> A language-agnostic tokenization library that supports BPE, WordPiece, and unigram language models.</li>
</ul></li>
<li><strong>Vocabulary Management:</strong> Manage the vocabulary carefully. Consider using a fixed vocabulary size to control memory usage. Handle out-of-vocabulary (OOV) tokens gracefully (e.g., using a special <code>&lt;unk&gt;</code> token).</li>
<li><strong>Normalization:</strong> Normalize the input text before tokenization (e.g., lowercasing, removing punctuation, handling Unicode).</li>
<li><strong>Pre- and Post-processing:</strong> Implement pre- and post-processing steps as needed (e.g., adding special tokens, padding sequences).</li>
</ul>
<p><strong>6. Deployment</strong></p>
<ul>
<li><strong>Versioning:</strong> Use a version control system (e.g., Git) to track changes to the library. Use semantic versioning (e.g., <code>major.minor.patch</code>) to indicate compatibility.</li>
<li><strong>Packaging:</strong> Package the library in a way that is easy to install and use. Use a package manager like <code>pip</code> (Python) or <code>conda</code>. Create platform-specific packages (e.g., wheels for Python).</li>
<li><strong>Containerization:</strong> Use containerization technologies like Docker to create consistent and reproducible environments. This simplifies deployment and reduces the risk of compatibility issues.</li>
<li><strong>Continuous Integration/Continuous Deployment (CI/CD):</strong> Set up a CI/CD pipeline to automate the build, test, and deployment process. Use tools like Jenkins, GitLab CI, or GitHub Actions.</li>
<li><strong>Infrastructure as Code (IaC):</strong> Use IaC tools like Terraform or CloudFormation to manage the infrastructure that the library runs on. This allows you to automate the creation and configuration of servers, networks, and other resources.</li>
</ul>
<p><strong>7. Monitoring and Logging</strong></p>
<ul>
<li><strong>Logging:</strong> Implement comprehensive logging to track the behavior of the library. Log important events, errors, and warnings. Use a logging framework like <code>logging</code> (Python) or <code>spdlog</code> (C++).</li>
<li><strong>Monitoring:</strong> Monitor the performance of the library in production. Track metrics like inference latency, throughput, and error rate. Use monitoring tools like Prometheus or Grafana.</li>
<li><strong>Alerting:</strong> Set up alerts to notify you of problems. Alert on high error rates, slow inference times, or resource exhaustion.</li>
<li><strong>Feedback Loops:</strong> Establish feedback loops to continuously improve the library. Collect user feedback, analyze logs and metrics, and identify areas for optimization.</li>
<li><strong>A/B Testing:</strong> Use A/B testing to compare different versions of the library. Measure the impact of changes on key metrics.</li>
</ul>
<p><strong>8. Scalability Considerations</strong></p>
<ul>
<li><strong>Stateless Design:</strong> Design the inference service to be stateless, so that requests can be routed to any available instance.</li>
<li><strong>Horizontal Scaling:</strong> Scale the inference service horizontally by adding more instances. Use a load balancer to distribute traffic across instances.</li>
<li><strong>Caching:</strong> Use caching to reduce the load on the model. Cache frequently accessed data, such as tokenized input sequences or model outputs.</li>
<li><strong>Batching:</strong> Batch multiple requests together to improve throughput. This reduces the overhead of kernel launches and data transfers.</li>
</ul>
<p>By following these best practices, you can develop and deploy libraries for tokenization and hardware-accelerated model inference that are scalable, maintainable, and performant.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to present this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong></p>
<ul>
<li>“When developing and deploying libraries for tokenization and hardware acceleration, my focus is on creating solutions that are scalable, maintainable, and performant. I achieve this through a combination of good software engineering practices and careful attention to the specifics of hardware and NLP.”</li>
</ul></li>
<li><p><strong>Explain Modular Design:</strong></p>
<ul>
<li>“A key aspect is modular design. I break down the library into distinct modules responsible for tokenization, hardware acceleration, model loading, and I/O, ensuring a clear separation of concerns.” Mention the abstract <code>Tokenizer</code> class as an example.</li>
<li>“Abstraction layers are also crucial. They allow us to swap out underlying hardware or tokenization algorithms without disrupting the user-facing API. Using interfaces ensures clear communication between these modules.”</li>
</ul></li>
<li><p><strong>Discuss Code Quality and Testing:</strong></p>
<ul>
<li>“Code quality is paramount. I adhere to strict coding style guidelines and enforce them using linters and formatters. Code reviews are a standard part of the process.”</li>
<li>“Testing is extensive, covering unit tests, integration tests, and performance benchmarks. I pay special attention to hardware-specific tests and utilize fuzzing to uncover edge cases. Performance tracking prevents regressions.”</li>
</ul></li>
<li><p><strong>Dive into Hardware Acceleration:</strong></p>
<ul>
<li>“For hardware acceleration, the approach depends on the specific hardware and performance goals. I start by profiling the model to identify bottlenecks. Then, I’d choose the appropriate framework, like CUDA/cuDNN, TensorRT, or ONNX Runtime.”</li>
<li>“Techniques like quantization, pruning, and kernel fusion are employed to optimize performance. Asynchronous execution and careful memory management further improve efficiency.”</li>
</ul></li>
<li><p><strong>Explain Tokenization Strategies:</strong></p>
<ul>
<li>“Tokenization involves selecting the appropriate algorithm based on the language and task. I consider options like WordPiece, BPE, and SentencePiece.”</li>
<li>“Vocabulary management and normalization are also important, along with pre- and post-processing steps to prepare the data for the model.”</li>
</ul></li>
<li><p><strong>Cover Deployment and Versioning:</strong></p>
<ul>
<li>“Deployment is handled through version control with semantic versioning, proper packaging using tools like pip, and containerization with Docker for reproducible environments.”</li>
<li>“CI/CD pipelines automate the build, test, and deployment process. Infrastructure as Code allows for automated infrastructure management.”</li>
</ul></li>
<li><p><strong>Discuss Monitoring and Feedback:</strong></p>
<ul>
<li>“Monitoring is essential. I implement comprehensive logging and track key performance metrics like latency, throughput, and error rate. Alerting is set up to notify of issues.”</li>
<li>“I establish feedback loops to continuously improve the library, incorporating user feedback and analyzing logs. A/B testing is used to compare different versions.”</li>
</ul></li>
<li><p><strong>Highlight Scalability:</strong></p>
<ul>
<li>“Scalability is achieved through stateless design, horizontal scaling, caching, and batching.”</li>
</ul></li>
<li><p><strong>Concluding Remarks</strong></p>
<ul>
<li>“By following these practices, I aim to deliver robust, scalable, and maintainable libraries that meet the demanding requirements of modern machine learning applications.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Speak clearly and deliberately.</li>
<li><strong>Use Examples:</strong> Provide concrete examples to illustrate your points. For instance, mention specific tools or libraries you have used.</li>
<li><strong>Check for Understanding:</strong> Pause occasionally and ask the interviewer if they have any questions.</li>
<li><strong>Adjust to the Interviewer’s Level:</strong> If the interviewer is less technical, focus on the high-level concepts. If they are more technical, go into more detail.</li>
<li><strong>Be Honest About Limitations:</strong> If you don’t know the answer to a question, admit it and offer to follow up later.</li>
<li><strong>Enthusiasm:</strong> Showing enthusiasm for the topic can make a big difference.</li>
</ul>
<p>By following these guidelines, you can effectively communicate your expertise and demonstrate your ability to develop and deploy high-quality libraries for tokenization and hardware-accelerated model inference.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>