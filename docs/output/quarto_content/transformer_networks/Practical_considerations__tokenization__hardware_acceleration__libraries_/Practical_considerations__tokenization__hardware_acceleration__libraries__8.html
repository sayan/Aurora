<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>practical_considerations__tokenization__hardware_acceleration__libraries__8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-describe-the-considerations-involved-in-choosing-between-cpu-and-gputpu-acceleration-for-a-given-ml-application.-what-are-the-key-factors-that-influence-your-decision" class="level2">
<h2 class="anchored" data-anchor-id="question-describe-the-considerations-involved-in-choosing-between-cpu-and-gputpu-acceleration-for-a-given-ml-application.-what-are-the-key-factors-that-influence-your-decision">Question: Describe the considerations involved in choosing between CPU and GPU/TPU acceleration for a given ML application. What are the key factors that influence your decision?</h2>
<p><strong>Best Answer</strong></p>
<p>Choosing between CPU, GPU, and TPU acceleration for a machine learning application is a crucial decision that significantly impacts performance, cost, and deployment. The optimal choice depends on a complex interplay of factors including model architecture, workload characteristics, budget, availability, and specific application requirements. Here’s a breakdown of the key considerations:</p>
<p><strong>1. Computational Characteristics of the Model and Workload:</strong></p>
<ul>
<li><p><strong>Model Size and Complexity:</strong></p>
<ul>
<li><em>Small to Medium-Sized Models (e.g., simple linear models, shallow neural networks):</em> CPUs can often handle these efficiently, especially for smaller datasets. The overhead of transferring data to and from the GPU can outweigh the benefits of GPU acceleration for smaller models.</li>
<li><em>Large and Complex Models (e.g., Deep Neural Networks (DNNs), Transformers, Large Language Models (LLMs)):</em> GPUs and TPUs excel here. The massive parallelism offered by these accelerators is essential for training and inference. The more parameters a model has and the more complex the operations, the more significant the performance gain from using GPUs/TPUs.</li>
</ul></li>
<li><p><strong>Type of Operations:</strong></p>
<ul>
<li><em>Matrix Multiplications and Linear Algebra:</em> GPUs and TPUs are specifically designed and highly optimized for these operations, which are fundamental to many machine learning algorithms. They achieve much higher throughput than CPUs for these tasks.</li>
<li><em>Element-wise Operations and Control Flow:</em> CPUs may be more efficient for tasks that involve a lot of complex control flow, conditional statements, or element-wise operations where parallelization is less straightforward. However, GPUs have been improving their handling of these tasks.</li>
</ul></li>
<li><p><strong>Batch Size:</strong></p>
<ul>
<li>GPUs and TPUs generally perform best with large batch sizes, which allows them to fully utilize their parallel processing capabilities. However, excessively large batch sizes can negatively impact model generalization and training stability. The relationship between batch size (<span class="math inline">\(B\)</span>), memory usage (<span class="math inline">\(M\)</span>), and computational workload (<span class="math inline">\(W\)</span>) per batch is:</li>
</ul>
<p><span class="math display">\[M \propto B\]</span> <span class="math display">\[W \propto B\]</span></p>
<p>Therefore, increasing batch size linearly increases both memory usage and computational workload per step.</p></li>
<li><p><strong>Data Parallelism vs.&nbsp;Model Parallelism:</strong></p>
<ul>
<li><em>Data Parallelism:</em> GPUs and TPUs are well-suited for data parallelism, where the model is replicated across multiple devices, and each device processes a different subset of the data.</li>
<li><em>Model Parallelism:</em> For extremely large models that cannot fit on a single device, model parallelism is necessary. This involves partitioning the model across multiple devices. While GPUs can support model parallelism, TPUs are often designed with interconnects optimized for this type of parallelism (e.g., TPU pods).</li>
</ul></li>
</ul>
<p><strong>2. Hardware Availability and Cost:</strong></p>
<ul>
<li><p><strong>CPUs:</strong> CPUs are generally readily available and more cost-effective for basic machine learning tasks and development. Most machines already have capable CPUs.</p></li>
<li><p><strong>GPUs:</strong> GPUs offer a significant performance boost over CPUs for many machine learning workloads, but they come at a higher cost. Cloud-based GPU instances (e.g., AWS, GCP, Azure) provide a flexible and scalable option, but costs can add up quickly, especially for long training runs.</p></li>
<li><p><strong>TPUs:</strong> TPUs are specialized accelerators designed by Google specifically for deep learning workloads. They are typically only available through Google Cloud Platform (GCP). While TPUs can offer substantial performance gains over GPUs for certain models (especially large ones), they come with a steeper learning curve and potentially higher costs depending on usage.</p></li>
</ul>
<p><strong>3. Memory Considerations:</strong></p>
<ul>
<li><strong>CPU Memory (RAM):</strong> CPUs typically have access to larger amounts of system RAM than GPUs. This can be advantageous for handling large datasets that don’t fit into GPU memory.</li>
<li><strong>GPU Memory (VRAM):</strong> GPUs have limited VRAM. The model, data, and intermediate activations must fit within the VRAM. This is a key constraint, especially for large models. Memory transfer between CPU and GPU is often a bottleneck.</li>
<li><strong>TPU Memory:</strong> TPUs have their own on-chip memory architecture optimized for matrix operations.</li>
</ul>
<p><strong>4. Software Ecosystem and Framework Support:</strong></p>
<ul>
<li><strong>CPUs:</strong> CPUs have mature and comprehensive software support. Most machine learning frameworks (e.g., TensorFlow, PyTorch, scikit-learn) are well-optimized for CPUs.</li>
<li><strong>GPUs:</strong> GPUs also have excellent framework support, with optimized libraries (e.g., CUDA, cuDNN) for deep learning. PyTorch and Tensorflow are well established on GPUs.</li>
<li><strong>TPUs:</strong> TPUs are primarily supported by TensorFlow and JAX, with growing support in PyTorch. Using TPUs may require adapting code to the TPU programming model.</li>
</ul>
<p><strong>5. Power Consumption and Thermal Management:</strong></p>
<ul>
<li><strong>CPUs:</strong> CPUs typically consume less power than GPUs, making them a more energy-efficient choice for smaller workloads or deployments where power consumption is a concern.</li>
<li><strong>GPUs:</strong> GPUs consume significantly more power than CPUs, requiring robust cooling solutions.</li>
<li><strong>TPUs:</strong> TPUs are also power-hungry devices. Power consumption is a significant factor in large-scale data centers.</li>
</ul>
<p><strong>6. Development and Deployment Considerations:</strong></p>
<ul>
<li><strong>Ease of Use:</strong> CPUs are generally easier to program and debug for basic machine learning tasks.</li>
<li><strong>Framework Integration:</strong> The choice of hardware can influence the choice of machine learning framework. TensorFlow and JAX are tightly integrated with TPUs.</li>
<li><strong>Deployment Environment:</strong> The deployment environment (e.g., cloud, edge device) will impact the available hardware options. CPUs are more ubiquitous, while GPUs and TPUs may have limited availability in certain environments.</li>
</ul>
<p><strong>7. Throughput and Latency:</strong></p>
<ul>
<li><strong>Training Throughput:</strong> GPUs and TPUs generally offer higher training throughput (samples processed per unit time) compared to CPUs, significantly reducing training time for complex models.
<ul>
<li><em>Throughput <span class="math inline">\(\propto\)</span> (Number of Operations) / (Time)</em></li>
</ul></li>
<li><strong>Inference Latency:</strong> The choice of hardware can impact inference latency, which is the time it takes to process a single input. GPUs and TPUs can provide lower latency for complex models, enabling real-time or near-real-time applications.</li>
</ul>
<p><strong>Decision-Making Process Summary:</strong></p>
<ol type="1">
<li><strong>Profile your model and workload:</strong> Determine the model size, type of operations, batch size, and data size.</li>
<li><strong>Assess hardware availability and cost:</strong> Evaluate the cost of CPU, GPU, and TPU instances on cloud platforms.</li>
<li><strong>Consider memory constraints:</strong> Ensure that the model, data, and intermediate activations fit into the available memory.</li>
<li><strong>Evaluate framework support:</strong> Choose a hardware platform that is well-supported by your preferred machine learning framework.</li>
<li><strong>Optimize for throughput or latency:</strong> Select hardware that meets the required throughput and latency requirements.</li>
<li><strong>Factor in power consumption:</strong> Consider the power consumption of the hardware, especially for large-scale deployments.</li>
</ol>
<p>In summary, while CPUs remain relevant for simpler tasks and early-stage development, GPUs and TPUs are essential for accelerating the training and inference of complex deep learning models. The specific choice depends on a careful evaluation of the model, workload, hardware availability, cost, and deployment environment.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><strong>Start with a Broad Overview:</strong>
<ul>
<li>“Choosing between CPUs, GPUs, and TPUs for machine learning depends on many factors. There’s no single right answer; it’s a trade-off based on the specific application requirements, the model itself, and available resources.”</li>
</ul></li>
<li><strong>Discuss Model and Workload Characteristics:</strong>
<ul>
<li>“One of the primary considerations is the nature of the model and the workload it will handle. For smaller, simpler models, a CPU is often sufficient. However, for large, complex models, GPUs or TPUs become necessary to achieve reasonable training times. The computational demands change depending on if you are doing image classification, language modeling, or other complex tasks.”</li>
<li>“Focus on the type of operations the model relies on. GPUs and TPUs excel at linear algebra, specifically matrix multiplications, which are at the heart of deep learning. This advantage is important for neural network training.”</li>
</ul></li>
<li><strong>Address Hardware Availability and Cost:</strong>
<ul>
<li>“The availability and cost of the hardware are significant factors. CPUs are ubiquitous and generally cheaper for basic tasks. GPUs offer a performance boost at a higher price point, while TPUs are specialized and available through Google Cloud, potentially offering the best performance for very large models but at a higher cost and with a learning curve.”</li>
</ul></li>
<li><strong>Explain Memory Considerations:</strong>
<ul>
<li>“Memory constraints are crucial. GPUs have limited VRAM, and the model and data must fit within it. CPUs often have more accessible RAM, which can be advantageous for large datasets. However, if memory transfer between CPU and GPU becomes the bottleneck, a CPU may not be ideal.”</li>
</ul></li>
<li><strong>Cover Software Ecosystem and Framework Support:</strong>
<ul>
<li>“The software ecosystem and framework support are important. CPUs have the most mature and comprehensive software support. GPUs are well-supported by major frameworks like TensorFlow and PyTorch. TPUs are best integrated with TensorFlow and JAX, requiring some adaptation.”</li>
</ul></li>
<li><strong>Mention Power Consumption:</strong>
<ul>
<li>“Don’t forget about power consumption, especially for large-scale deployments. GPUs and TPUs are more power-hungry than CPUs.”</li>
</ul></li>
<li><strong>Summarize the Decision-Making Process:</strong>
<ul>
<li>“In short, the decision-making process involves profiling your model and workload, assessing hardware availability and cost, considering memory constraints, evaluating framework support, and optimizing for throughput or latency, while also factoring in power consumption.”</li>
</ul></li>
<li><strong>Handling Equations (if you choose to include them):</strong>
<ul>
<li>“I can illustrate this with a few equations. For example, the memory required is roughly proportional to the batch size.”</li>
<li>“For example, <insert latex="" equation="">. This shows that…” (Explain the implications simply.)</insert></li>
<li><strong>Caution:</strong> Only include equations if you are very comfortable explaining them concisely and accurately. Avoid overwhelming the interviewer with too much math. Focus on the intuitive meaning.</li>
</ul></li>
<li><strong>Interaction Tips:</strong>
<ul>
<li>Pause between points to check for understanding. “Does that make sense so far?”</li>
<li>Use real-world examples if possible to illustrate your points.</li>
<li>Be prepared to answer follow-up questions about specific scenarios or applications.</li>
<li>Maintain a confident and professional tone.</li>
</ul></li>
</ol>
<p>By following this structure, you can effectively demonstrate your understanding of the factors involved in choosing between CPU, GPU, and TPU acceleration for machine learning applications.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>