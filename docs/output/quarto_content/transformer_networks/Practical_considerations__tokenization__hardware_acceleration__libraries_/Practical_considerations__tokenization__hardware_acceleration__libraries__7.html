<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>practical_considerations__tokenization__hardware_acceleration__libraries__7</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-how-would-you-address-the-challenge-of-handling-messy-or-noisy-input-data-during-tokenization-especially-when-transitioning-from-research-to-a-production-environment" class="level2">
<h2 class="anchored" data-anchor-id="question-how-would-you-address-the-challenge-of-handling-messy-or-noisy-input-data-during-tokenization-especially-when-transitioning-from-research-to-a-production-environment">Question: How would you address the challenge of handling messy or noisy input data during tokenization, especially when transitioning from research to a production environment?</h2>
<p><strong>Best Answer</strong></p>
<p>Handling messy or noisy input data during tokenization is a crucial challenge, especially when moving machine learning models from a controlled research environment to a real-world production setting. Noisy data can significantly degrade the performance of downstream tasks. A comprehensive strategy involves a multi-faceted approach, focusing on robust preprocessing, tokenizer training, and error handling.</p>
<p><strong>1. Preprocessing and Data Cleaning:</strong></p>
<p>The first line of defense is a robust preprocessing pipeline. This can include the following steps:</p>
<ul>
<li><p><strong>Character Encoding Normalization:</strong> Ensure consistent character encoding (e.g., UTF-8). Inconsistent encodings can lead to incorrect tokenization.</p></li>
<li><p><strong>Whitespace Handling:</strong> Standardize whitespace. Multiple spaces, tabs, and newline characters should be collapsed into single spaces. Leading and trailing whitespace should be removed.</p></li>
<li><p><strong>Lowercasing/Case Normalization:</strong> Converting all text to lowercase can reduce vocabulary size and improve generalization, but consider whether case information is important for your task. If case information is important, consider more sophisticated case normalization techniques. For example, converting to lowercase except for acronyms or proper nouns.</p></li>
<li><p><strong>Punctuation Removal/Normalization:</strong> Decide how to handle punctuation. Sometimes punctuation is important (e.g., for sentiment analysis or question answering), while other times it’s not. If removing, use a consistent approach. If retaining, normalize different types of dashes or quotation marks to a standard representation.</p></li>
<li><p><strong>Special Character Handling:</strong> Address special characters and symbols, such as emojis or mathematical symbols. This might involve removing them, replacing them with textual representations, or adding them to the tokenizer’s vocabulary.</p></li>
<li><p><strong>Typos and Spelling Correction:</strong> Implement a spelling correction module to fix common typos. This can use techniques like edit distance, n-gram models, or pre-trained spell checkers.</p>
<ul>
<li>Edit distance (Levenshtein distance) calculates the minimum number of single-character edits required to change one string into the other. <span class="math display">\[
\text{lev}(a, b) = \begin{cases}
|a| &amp; \text{if } |b| = 0, \\
|b| &amp; \text{if } |a| = 0, \\
\text{lev}(a[1:], b[1:]) &amp; \text{if } a[0] = b[0], \\
1 + \min \begin{cases}
\text{lev}(a[1:], b), \\
\text{lev}(a, b[1:]), \\
\text{lev}(a[1:], b[1:])
\end{cases} &amp; \text{otherwise.}
\end{cases}
\]</span> where <span class="math inline">\(lev(a,b)\)</span> is the Levenshtein distance between strings <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, <span class="math inline">\(|a|\)</span> is the length of <span class="math inline">\(a\)</span>, <span class="math inline">\(a[0]\)</span> is the first character of <span class="math inline">\(a\)</span> and <span class="math inline">\(a[1:]\)</span> is the rest of the string.</li>
</ul></li>
<li><p><strong>Number Handling:</strong> Decide how to represent numbers. You might normalize them to a common format (e.g., replacing all numbers with a <code>&lt;NUMBER&gt;</code> token) or keep them as they are.</p></li>
<li><p><strong>URL/Email Handling:</strong> Replace URLs and email addresses with special tokens (e.g., <code>&lt;URL&gt;</code>, <code>&lt;EMAIL&gt;</code>).</p></li>
<li><p><strong>Language Detection:</strong> Use a language detection library to identify the language of the input text. This is especially important in multilingual environments.</p></li>
</ul>
<p><strong>2. Robust Tokenizer Training:</strong></p>
<p>The tokenizer itself must be robust to noisy data.</p>
<ul>
<li><p><strong>Training Data:</strong> Train the tokenizer on a large, diverse corpus of text that includes examples of noisy data. This will help the tokenizer learn to handle variations in spelling, grammar, and formatting. Data augmentation techniques (e.g., randomly introducing typos or noise) can also be helpful.</p></li>
<li><p><strong>Subword Tokenization:</strong> Use subword tokenization algorithms like Byte Pair Encoding (BPE) or WordPiece. These algorithms break words into smaller units (subwords), which can handle out-of-vocabulary words and rare tokens more effectively. For instance, BPE merges the most frequent pairs of characters/tokens iteratively until a desired vocabulary size is reached. If we have a corpus with counts: ‘lo’ (5), ‘ow’ (5), ‘low’ (2), ‘ne’ (3), ‘ew’ (3), ‘new’ (2), then BPE will first merge ‘lo’ and ‘ow’ since they are the most frequent, creating ‘low’.</p></li>
<li><p><strong>Vocabulary Size:</strong> Choose an appropriate vocabulary size. A larger vocabulary can capture more rare tokens, but it can also increase memory usage and training time.</p></li>
<li><p><strong>Unknown Token Handling:</strong> Define a special <code>&lt;UNK&gt;</code> token to represent words that are not in the vocabulary. The tokenizer should be trained to handle <code>&lt;UNK&gt;</code> tokens gracefully.</p></li>
<li><p><strong>Normalization During Tokenization:</strong> Integrate some normalization steps (e.g., lowercasing, punctuation removal) directly into the tokenization process.</p></li>
</ul>
<p><strong>3. Error Handling and Monitoring:</strong></p>
<p>Even with robust preprocessing and tokenizer training, some errors are inevitable.</p>
<ul>
<li><p><strong>Logging and Monitoring:</strong> Implement logging and monitoring to track tokenization errors and identify areas for improvement. Pay attention to the frequency of <code>&lt;UNK&gt;</code> tokens, which can be an indicator of noisy data or a vocabulary that is not comprehensive enough.</p></li>
<li><p><strong>Fallback Mechanisms:</strong> Consider implementing fallback mechanisms to handle cases where tokenization fails. For example, you might try a different tokenization algorithm or revert to a character-based representation.</p></li>
<li><p><strong>Human Review:</strong> In some cases, it may be necessary to manually review and correct tokenization errors. This is especially important for high-stakes applications.</p></li>
</ul>
<p><strong>4. Specific Noise Types and Mitigation:</strong></p>
<ul>
<li><p><strong>Mixed-Language Text:</strong> Use language identification and then apply language-specific tokenizers or normalization. Another strategy is to use a multilingual tokenizer like mBERT or XLM-RoBERTa, which are trained on text from multiple languages.</p></li>
<li><p><strong>Typos and Misspellings:</strong> Incorporate spell checking or approximate string matching to correct common errors before or during tokenization.</p></li>
<li><p><strong>Rare Symbols:</strong> If rare symbols are important, add them to the tokenizer’s vocabulary. Otherwise, replace them with a standard symbol or remove them.</p></li>
<li><p><strong>Contextual Disambiguation:</strong> For words with multiple meanings or spellings, consider using contextual information to disambiguate them before tokenization. This can involve using a pre-trained language model to predict the correct meaning or spelling.</p></li>
</ul>
<p><strong>5. Evaluation:</strong></p>
<ul>
<li><p><strong>Intrinsic Evaluation:</strong> Evaluate the tokenizer’s performance on a held-out set of noisy data. Metrics like the percentage of correctly tokenized words or the frequency of <code>&lt;UNK&gt;</code> tokens can be used.</p></li>
<li><p><strong>Extrinsic Evaluation:</strong> Evaluate the impact of the tokenizer on the performance of downstream tasks. For example, if you are using the tokenizer for sentiment analysis, evaluate the accuracy of the sentiment analysis model on noisy data.</p></li>
</ul>
<p><strong>Example (Python):</strong></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.metrics <span class="im">import</span> edit_distance</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_text(text):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""Preprocesses text by removing special characters, lowercasing,</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">     and correcting common typos."""</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> re.sub(<span class="vs">r"[^a-zA-Z0-9\s]"</span>, <span class="st">""</span>, text)  <span class="co"># Remove special characters</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  text <span class="op">=</span> text.lower()  <span class="co"># Lowercase</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Simple typo correction (replace with closest word in vocabulary)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  words <span class="op">=</span> text.split()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  corrected_words <span class="op">=</span> []</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  vocabulary <span class="op">=</span> <span class="bu">set</span>(nltk.corpus.words.words()) <span class="co"># Example Vocabulary</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> vocabulary:</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>      closest_word <span class="op">=</span> <span class="bu">min</span>(vocabulary, key<span class="op">=</span><span class="kw">lambda</span> v: edit_distance(word, v))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>      corrected_words.append(closest_word)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>      corrected_words.append(word)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="st">" "</span>.join(corrected_words)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"This is some mssy text with tyypos."</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>cleaned_text <span class="op">=</span> preprocess_text(text)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cleaned text: </span><span class="sc">{</span>cleaned_text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>) <span class="co"># Load pre-trained tokenizer</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer.tokenize(cleaned_text) <span class="co"># Tokenize cleaned text</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>By implementing these strategies, you can build a robust tokenization pipeline that is resilient to noisy data and performs well in a production environment.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to present this answer effectively during an interview:</p>
<ol type="1">
<li><p><strong>Start with the Importance:</strong> “Handling noisy data during tokenization is crucial for ensuring the reliability of our models in real-world scenarios. A model is only as good as the data you feed into it, and messy data can have drastic effects. My approach focuses on a layered strategy of preprocessing, robust tokenizer training, and continuous monitoring.”</p></li>
<li><p><strong>Explain Preprocessing (High-Level First):</strong> “The first step is a comprehensive preprocessing pipeline. This involves cleaning and normalizing the input data to reduce noise and inconsistencies. This makes the tokenization process easier and the results better.”</p></li>
<li><p><strong>Describe Key Preprocessing Steps (Give Examples):</strong> “Specifically, this includes things like normalizing character encodings to UTF-8, standardizing whitespace, and handling punctuation consistently. For example, different types of dashes (em dash, en dash, hyphen) can all be converted to a single standard representation. Other important steps may include language detection, typo correction and number handling.”</p></li>
<li><p><strong>Briefly Mention Math (Only if Comfortable):</strong> “For typo correction, one technique we can use is edit distance, sometimes called Levenshtein distance. This quantifies the number of single character changes that must be made to transform one string into the other”. (Optionally, show the equation briefly if the interviewer seems interested, but don’t dwell on it).</p></li>
<li><p><strong>Move to Tokenizer Training:</strong> “Next, we need to train the tokenizer itself to be robust to noisy data. I like to use subword tokenization algorithms, like Byte Pair Encoding, where frequent pairs of characters or tokens get merged together. This is an iterative process that builds up a useful vocabulary from a training corpus.</p></li>
<li><p><strong>Discuss <code>&lt;UNK&gt;</code> Token Handling:</strong> “A crucial aspect is how the tokenizer handles out-of-vocabulary words. We use a special token, usually called <code>&lt;UNK&gt;</code>, to represent these words. Monitoring the frequency of this token in the production environment can be very helpful.”</p></li>
<li><p><strong>Address Error Handling and Monitoring:</strong> “Even with robust preprocessing and training, errors will still occur. Therefore, it’s vital to implement logging and monitoring to track these errors and identify areas for improvement. If our rate of <code>&lt;UNK&gt;</code> tokens shoots up, that indicates problems with our data or our tokenizer’s vocabulary.”</p></li>
<li><p><strong>Discuss Edge Cases:</strong> “There are some specific types of noise that need tailored solutions. Mixed-language text, for example, can be handled using language detection followed by language-specific tokenization, or we can use a multilingual tokenizer.”</p></li>
<li><p><strong>Explain Evaluation:</strong> “Finally, it’s critical to evaluate the performance of the tokenizer using both intrinsic metrics (like the <code>&lt;UNK&gt;</code> token rate) and extrinsic metrics (like the accuracy of downstream models). This helps us identify areas where the tokenization pipeline can be improved further.”</p></li>
<li><p><strong>Conclude Confidently:</strong> “By combining these techniques, we can build a robust and reliable tokenization pipeline that can handle the challenges of noisy data in a production environment. I have experience implementing similar pipelines in [mention your previous projects or experience]. I believe this multi-layered approach provides the best chance for success when transitioning from research to production.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Use a structured approach:</strong> Clearly outline the steps in your approach (preprocessing, training, monitoring).</li>
<li><strong>Give examples:</strong> Illustrate your points with concrete examples of noisy data and how you would handle them.</li>
<li><strong>Quantify impact:</strong> Explain how your approach improves the performance of downstream tasks.</li>
<li><strong>Be prepared to delve deeper:</strong> The interviewer may ask you to elaborate on specific techniques or edge cases. Be ready to provide more details and justify your choices.</li>
<li><strong>Don’t be afraid to admit limitations:</strong> If you don’t know the answer to a question, be honest and explain how you would go about finding the solution.</li>
<li><strong>Show Enthusiasm:</strong> Conclude with a summary of the importance of this work in real-world deployments.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>