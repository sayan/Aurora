<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>practical_considerations__tokenization__hardware_acceleration__libraries__3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-can-you-explain-how-hardware-acceleration-e.g.-gpus-tpus-improves-the-performance-of-deep-learning-models-and-what-factors-you-consider-when-optimizing-algorithms-for-such-hardware" class="level2">
<h2 class="anchored" data-anchor-id="question-can-you-explain-how-hardware-acceleration-e.g.-gpus-tpus-improves-the-performance-of-deep-learning-models-and-what-factors-you-consider-when-optimizing-algorithms-for-such-hardware">Question: Can you explain how hardware acceleration (e.g., GPUs, TPUs) improves the performance of deep learning models, and what factors you consider when optimizing algorithms for such hardware?</h2>
<p><strong>Best Answer</strong></p>
<p>Deep learning models, especially large neural networks, require substantial computational resources for training and inference. Hardware accelerators like GPUs (Graphics Processing Units) and TPUs (Tensor Processing Units) provide significant performance improvements compared to CPUs (Central Processing Units) due to their architectural designs optimized for parallel processing.</p>
<ul>
<li><p><strong>Parallel Processing:</strong></p>
<ul>
<li><p>CPUs are designed for general-purpose computing, excelling at sequential tasks with complex control flows. They typically have a few powerful cores.</p></li>
<li><p>GPUs, on the other hand, are massively parallel architectures with thousands of smaller cores designed to perform the same operation on multiple data points simultaneously. This Single Instruction, Multiple Data (SIMD) architecture is ideally suited for the matrix operations that are fundamental to deep learning. TPUs are further optimized for deep learning workloads.</p></li>
<li><p>Consider matrix multiplication, a core operation in neural networks. If <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times k\)</span> matrix and <span class="math inline">\(B\)</span> is a <span class="math inline">\(k \times n\)</span> matrix, the resulting matrix <span class="math inline">\(C = AB\)</span> has dimensions <span class="math inline">\(m \times n\)</span>, where each element <span class="math inline">\(C_{ij}\)</span> is calculated as:</p>
<p><span class="math display">\[C_{ij} = \sum_{l=1}^{k} A_{il}B_{lj}\]</span></p>
<p>A CPU would typically compute this sequentially or with limited parallelism. A GPU can perform many of these element-wise multiplications and summations in parallel, dramatically reducing the computation time.</p></li>
</ul></li>
<li><p><strong>Memory Bandwidth:</strong></p>
<ul>
<li>Memory bandwidth refers to the rate at which data can be read from or written to memory. Deep learning models often require accessing large amounts of data (weights, activations, gradients) during training and inference.</li>
<li>GPUs and TPUs typically have much higher memory bandwidth compared to CPUs. High bandwidth is crucial to keep the processing cores fed with data, preventing them from stalling and reducing overall performance. For example, high-end GPUs utilize High Bandwidth Memory (HBM) to achieve significantly higher bandwidth than traditional DRAM used in CPUs. Sustaining the peak compute capability of the accelerator critically depends on being able to feed the accelerator at a sufficient rate, governed by the achievable memory bandwidth.</li>
</ul></li>
<li><p><strong>Architectural Optimization (TPUs):</strong></p>
<ul>
<li>TPUs are specifically designed by Google for deep learning workloads. They feature a Matrix Multiply Unit (MXU) that can perform a large number of multiply-accumulate operations in a single cycle.</li>
<li>The TPU architecture also includes a large amount of on-chip memory, reducing the need to access external memory and further improving performance. This systolic array architecture allows for highly efficient data reuse.</li>
</ul></li>
<li><p><strong>Factors to Consider When Optimizing Algorithms for Hardware Accelerators:</strong></p>
<ul>
<li><p><strong>Batch Size:</strong> Increasing the batch size can improve hardware utilization by processing more data in parallel. However, it also increases memory consumption and can affect model convergence. Finding the optimal batch size involves trade-offs. Larger batch sizes tend to lead to more stable gradient estimates, but can also flatten the loss landscape and reduce the model’s ability to generalize.</p>
<ul>
<li>The relationship between batch size (<span class="math inline">\(B\)</span>), learning rate (<span class="math inline">\(\eta\)</span>), and gradient noise can be approximated as: <span class="math inline">\(\text{Noise} \propto \frac{1}{\sqrt{B}}\)</span>. Larger batches effectively reduce noise, allowing for potentially higher learning rates.</li>
</ul></li>
<li><p><strong>Data Precision:</strong> Using lower precision data types (e.g., FP16 instead of FP32) can significantly reduce memory usage and improve performance, as the hardware can perform more operations per cycle. However, it can also lead to reduced accuracy and instability during training. Techniques like mixed-precision training can mitigate these issues.</p>
<ul>
<li>The bit-width (<span class="math inline">\(w\)</span>) impacts both memory footprint and compute throughput. The memory footprint is directly proportional to <span class="math inline">\(w\)</span>. However, specialized hardware like NVIDIA’s Tensor Cores are designed to accelerate FP16 operations, potentially leading to a super-linear speedup compared to FP32.</li>
</ul></li>
<li><p><strong>Memory Management:</strong> Efficient memory management is crucial to avoid performance bottlenecks. This includes minimizing data transfers between the host (CPU) and the accelerator (GPU/TPU) and optimizing memory layout for efficient access. Techniques like memory pooling and pinned memory can help.</p></li>
<li><p><strong>Algorithm Parallelization:</strong> Algorithms need to be designed or modified to take advantage of the parallel processing capabilities of the hardware. This may involve restructuring the code to use vectorized operations or distributing the computation across multiple cores or devices.</p></li>
<li><p><strong>Communication Overhead:</strong> In distributed training scenarios, the communication overhead between devices can become a bottleneck. Techniques like gradient compression and asynchronous training can help reduce this overhead.</p></li>
<li><p><strong>Library and Framework Selection:</strong> Choosing the right deep learning framework (e.g., TensorFlow, PyTorch) and libraries (e.g., cuDNN, cuBLAS) is important. These frameworks and libraries provide optimized implementations of common deep learning operations for hardware accelerators.</p>
<ul>
<li>For example, cuDNN is a library of primitives for deep neural networks. It provides highly optimized implementations of operations like convolution, pooling, and recurrent neural networks.</li>
</ul></li>
<li><p><strong>Kernel Fusion:</strong> Many frameworks automatically fuse multiple operations into a single kernel to reduce memory access and improve performance. This is especially helpful for operations that are memory-bound.</p></li>
<li><p><strong>Quantization:</strong> Converting the model weights and activations to lower precision integer formats (e.g., INT8) can significantly reduce memory footprint and improve inference speed, especially on hardware with specialized integer arithmetic units. This usually comes with some accuracy loss, which may need to be mitigated by fine-tuning or quantization-aware training.</p></li>
</ul></li>
<li><p><strong>Common Pitfalls:</strong></p>
<ul>
<li><strong>Memory Constraints:</strong> GPUs and TPUs have limited memory compared to CPUs. Large models or large batch sizes can easily exceed the available memory, leading to out-of-memory errors. Techniques like model parallelism, gradient accumulation, and activation checkpointing can help address this issue.</li>
<li><strong>Data Transfer Bottlenecks:</strong> Frequent data transfers between the CPU and the accelerator can become a bottleneck. Minimizing these transfers and using asynchronous data loading can improve performance.</li>
<li><strong>Incorrect Data Types:</strong> Using the wrong data types can lead to performance degradation. For example, using FP64 (double-precision floating-point) when FP32 (single-precision floating-point) is sufficient can significantly slow down computation.</li>
</ul></li>
</ul>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to deliver this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a high-level overview:</strong></p>
<ul>
<li>“Deep learning models benefit significantly from hardware acceleration due to the parallel nature of their computations. GPUs and TPUs are specifically designed to handle these workloads more efficiently than CPUs.”</li>
<li>Emphasize the shift from CPU-centric to accelerator-centric paradigm.</li>
</ul></li>
<li><p><strong>Explain Parallel Processing:</strong></p>
<ul>
<li>“CPUs are good for sequential tasks, but deep learning relies heavily on matrix operations that can be parallelized. GPUs have thousands of cores that can perform the same operation on different data simultaneously – think SIMD architecture. TPUs are further tailored for deep learning with specialized units.”</li>
<li>Use the matrix multiplication example to illustrate parallelization. Keep the explanation of the equation <span class="math inline">\(C_{ij} = \sum_{l=1}^{k} A_{il}B_{lj}\)</span> simple: “Each element of the output matrix is a sum of products. The GPU can compute many of these sums of products at the <em>same time</em>.”</li>
</ul></li>
<li><p><strong>Discuss Memory Bandwidth:</strong></p>
<ul>
<li>“Another crucial factor is memory bandwidth. Deep learning models need to access large amounts of data quickly. GPUs and TPUs have significantly higher memory bandwidth than CPUs, which helps prevent processing cores from being starved of data.”</li>
<li>Mention HBM for high-end GPUs, if the interviewer seems engaged.</li>
</ul></li>
<li><p><strong>Explain TPUs’ unique architecture (if appropriate):</strong></p>
<ul>
<li>“TPUs are specifically designed by Google for deep learning. They have a Matrix Multiply Unit (MXU) for highly efficient matrix operations and a large amount of on-chip memory to minimize external memory accesses.”</li>
</ul></li>
<li><p><strong>Transition to optimization factors:</strong></p>
<ul>
<li>“Optimizing algorithms for these accelerators requires considering several factors…”</li>
</ul></li>
<li><p><strong>Address Optimization Factors (Batch Size, Data Precision, Memory Management, etc.):</strong></p>
<ul>
<li>For each factor, briefly explain what it is, why it’s important, and how it affects performance.
<ul>
<li><strong>Batch Size:</strong> “Increasing batch size can improve hardware utilization, but it also affects memory consumption and model convergence. So, it’s a trade-off.”</li>
<li><strong>Data Precision:</strong> “Using lower precision data types like FP16 can reduce memory usage and speed up computation, but it can also impact accuracy. Techniques like mixed-precision training can help.”</li>
<li><strong>Memory Management:</strong> “Efficient memory management is crucial to avoid bottlenecks. Minimizing data transfers between the CPU and the accelerator, and optimizing memory layout are important.”</li>
</ul></li>
<li>Don’t go into too much detail unless the interviewer asks for it. For instance, you could briefly mention quantization and Kernel Fusion if the time allows.</li>
</ul></li>
<li><p><strong>Mention Common Pitfalls:</strong></p>
<ul>
<li>“There are also some common pitfalls to watch out for, such as memory constraints and data transfer bottlenecks.”</li>
<li>For memory constraints, briefly mention techniques like model parallelism or gradient accumulation.</li>
</ul></li>
<li><p><strong>Highlight Library and Framework Selection:</strong></p>
<ul>
<li>“Choosing the right deep learning framework (e.g., TensorFlow, PyTorch) and libraries (e.g., cuDNN, cuBLAS) is important. These frameworks and libraries provide optimized implementations of common deep learning operations for hardware accelerators.”</li>
</ul></li>
<li><p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use clear and concise language:</strong> Avoid jargon unless you’re confident the interviewer understands it.</li>
<li><strong>Check for understanding:</strong> Periodically ask if the interviewer has any questions.</li>
<li><strong>Be prepared to elaborate:</strong> If the interviewer shows interest in a particular area, be prepared to provide more detail.</li>
<li><strong>Stay practical:</strong> Connect your explanations to real-world scenarios whenever possible.</li>
<li>For equations, say: “…where C<sub>ij</sub> is computed as the <em>sum</em> of all the products of A<sub>il</sub> and B<sub>lj</sub>”. Avoid reading it like a formula.</li>
<li>Don’t be afraid to say “It depends” when discussing optimal batch size or precision. It shows you understand the trade-offs.</li>
</ul></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>