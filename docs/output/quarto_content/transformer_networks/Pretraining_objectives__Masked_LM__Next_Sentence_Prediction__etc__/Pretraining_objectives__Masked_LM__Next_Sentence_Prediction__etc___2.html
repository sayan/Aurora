<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>pretraining_objectives__masked_lm__next_sentence_prediction__etc___2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-3.-how-does-mlm-differ-from-causal-or-autoregressive-language-modeling-in-terms-of-training-objectives-and-downstream-performance" class="level2">
<h2 class="anchored" data-anchor-id="question-3.-how-does-mlm-differ-from-causal-or-autoregressive-language-modeling-in-terms-of-training-objectives-and-downstream-performance">Question: 3. How does MLM differ from Causal or Autoregressive Language Modeling in terms of training objectives and downstream performance?</h2>
<p><strong>Best Answer</strong></p>
<p>Masked Language Modeling (MLM) and Causal/Autoregressive Language Modeling represent fundamentally different approaches to pre-training language models, each with its own strengths and weaknesses. The key distinctions lie in their training objectives, the type of contextual information they capture, and their suitability for various downstream tasks.</p>
<p><strong>1. Training Objectives</strong></p>
<ul>
<li><p><strong>Masked Language Modeling (MLM):</strong> The objective is to predict randomly masked words in a sentence given the surrounding words. Specifically, given a sentence <span class="math inline">\(x = (x_1, x_2, ..., x_n)\)</span>, a portion of the tokens are masked. The model then learns to predict the original masked tokens based on the context provided by the unmasked tokens. The loss function is typically a cross-entropy loss, calculated as follows:</p>
<p><span class="math display">\[L_{MLM} = - \sum_{i \in M} log \, P(x_i | x_{\setminus M})\]</span></p>
<p>where <span class="math inline">\(M\)</span> is the set of masked token indices and <span class="math inline">\(x_{\setminus M}\)</span> represents the unmasked tokens. A classic example is BERT, where typically 15% of the tokens are masked. Note that some additional tricks are often implemented, such as replacing the masked tokens with a random token or the original token a certain percentage of the time, to reduce the discrepancy between pre-training and fine-tuning.</p></li>
<li><p><strong>Causal/Autoregressive Language Modeling:</strong> The objective is to predict the next word in a sequence given all the preceding words. Formally, the objective is to model the joint probability of a sequence <span class="math inline">\(x = (x_1, x_2, ..., x_n)\)</span> as a product of conditional probabilities:</p>
<p><span class="math display">\[P(x) = \prod_{i=1}^n P(x_i | x_1, x_2, ..., x_{i-1})\]</span></p>
<p>The loss function is again typically cross-entropy:</p>
<p><span class="math display">\[L_{AR} = - \sum_{i=1}^n log \, P(x_i | x_1, x_2, ..., x_{i-1})\]</span></p>
<p>Examples include GPT series, where the model learns to generate text by predicting the next token given the previous tokens.</p></li>
</ul>
<p><strong>2. Contextual Information Captured</strong></p>
<ul>
<li><p><strong>MLM (Bidirectional Context):</strong> MLM allows the model to leverage both left and right context when predicting a masked word. This bidirectional context is crucial for understanding the nuances of language and capturing complex relationships between words in a sentence. The masked word is conditioned on all other words, allowing the model to integrate information from all directions.</p></li>
<li><p><strong>Autoregressive LM (Unidirectional Context):</strong> Autoregressive models, by design, only consider the preceding words when predicting the next word. This unidirectional context makes them naturally suited for text generation tasks, as they can sequentially generate text in a coherent manner. However, it limits their ability to fully understand the context in the same way as MLM, especially for tasks that require understanding the relationships between words separated by a large distance.</p></li>
</ul>
<p><strong>3. Downstream Performance</strong></p>
<ul>
<li><strong>MLM:</strong>
<ul>
<li><strong>Strengths:</strong> MLM excels at tasks that require a deep understanding of context, such as:
<ul>
<li><strong>Text classification:</strong> The bidirectional context helps in capturing the overall meaning and sentiment of a text.</li>
<li><strong>Named Entity Recognition (NER):</strong> Understanding the context around a word is crucial for identifying named entities.</li>
<li><strong>Question Answering:</strong> The model can reason about the question and the context provided in the text.</li>
<li><strong>Sentence Similarity:</strong> Comparing sentence representations learned with MLM can capture subtle differences in meaning.</li>
</ul></li>
<li><strong>Limitations:</strong> MLM is not ideal for text generation because it doesn’t naturally produce sequential outputs. Although BERT can be adapted for generation tasks, it typically requires additional fine-tuning or architectural modifications.</li>
</ul></li>
<li><strong>Autoregressive LM:</strong>
<ul>
<li><strong>Strengths:</strong> Autoregressive models are the go-to choice for text generation tasks, including:
<ul>
<li><strong>Machine Translation:</strong> Generating text in a different language.</li>
<li><strong>Text Summarization:</strong> Creating a concise summary of a longer text.</li>
<li><strong>Creative Writing:</strong> Generating stories, poems, or scripts.</li>
<li><strong>Code Generation:</strong> Producing code based on a natural language description.</li>
</ul></li>
<li><strong>Limitations:</strong> Autoregressive models may not perform as well as MLM-based models on tasks requiring a deep understanding of bidirectional context.</li>
</ul></li>
</ul>
<p><strong>4. Representation Learning</strong></p>
<ul>
<li><p><strong>MLM:</strong> MLM tends to produce better contextualized word embeddings because it leverages both left and right contexts. The resulting embeddings can then be used for a wide range of downstream tasks. BERT’s embeddings, for example, are widely used as features in many NLP pipelines.</p></li>
<li><p><strong>Autoregressive LM:</strong> While autoregressive models also produce contextualized word embeddings, the embeddings are biased towards the preceding context. This might be sufficient for generation tasks, but it might not be as effective for tasks requiring bidirectional context understanding.</p></li>
</ul>
<p><strong>5. Real-world Considerations</strong></p>
<ul>
<li><p><strong>Computational Cost:</strong> MLM can be more computationally expensive during pre-training due to the need to process bidirectional context. Autoregressive models, on the other hand, can be trained more efficiently because they only need to consider the preceding context.</p></li>
<li><p><strong>Implementation Details:</strong> When implementing MLM, it’s important to carefully choose the masking strategy. A higher masking ratio can lead to faster training but might also result in lower performance. Similarly, for autoregressive models, techniques like beam search can be used to improve the quality of generated text.</p></li>
</ul>
<p>In summary, MLM and autoregressive language models represent different trade-offs between bidirectional context understanding and sequential text generation. The choice of which model to use depends on the specific downstream task.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Comparison:</strong></p>
<ul>
<li>“MLM and autoregressive language models are distinct approaches to pre-training, each optimized for different aspects of language understanding and generation. The key differences lie in their training objectives and how they capture context.”</li>
</ul></li>
<li><p><strong>Explain MLM Training Objective:</strong></p>
<ul>
<li>“MLM, exemplified by BERT, aims to predict masked words within a sentence, given the surrounding context. We can represent this mathematically as…” (Write the <span class="math inline">\(L_{MLM}\)</span> equation). “So, the model is trying to minimize the error in predicting masked words, which forces it to learn deep contextual representations.”</li>
<li><em>Communication Tip:</em> Avoid diving <em>too</em> deeply into the equation immediately. Briefly introduce the concept (masked words, surrounding context), <em>then</em> introduce the math as a formalization of that idea.</li>
</ul></li>
<li><p><strong>Explain Autoregressive Training Objective:</strong></p>
<ul>
<li>“Autoregressive models, like GPT, predict the next word in a sequence based on the preceding words. This can be formalized as…” (Write the <span class="math inline">\(P(x)\)</span> and <span class="math inline">\(L_{AR}\)</span> equations). “The objective here is to model the probability distribution of text sequences, making them naturally suited for text generation.”
<ul>
<li><em>Communication Tip:</em> Similar to MLM, explain the concept (predicting the next word) <em>before</em> showing the equations. Walk the interviewer through each symbol in the equation if they seem engaged.</li>
</ul></li>
</ul></li>
<li><p><strong>Discuss Contextual Information:</strong></p>
<ul>
<li>“MLM benefits from bidirectional context, meaning it considers both left and right context when making predictions. This is crucial for nuanced language understanding.”</li>
<li>“Autoregressive models, on the other hand, only use unidirectional context, which makes them great for generation but can limit their understanding in certain scenarios.”
<ul>
<li><em>Communication Tip:</em> Use simple examples to illustrate the difference. For instance, “Consider the sentence ‘The _ bank is next to the river.’ MLM can use both ‘The’ and ‘is’ <em>and</em> ‘is next’ to predict ‘bank.’ Autoregressive models only have ‘The’ to work with initially.”</li>
</ul></li>
</ul></li>
<li><p><strong>Elaborate on Downstream Performance:</strong></p>
<ul>
<li>“Due to its bidirectional context, MLM excels at tasks like text classification, NER, and question answering.”</li>
<li>“Autoregressive models are the standard for text generation tasks like machine translation, summarization, and creative writing.”
<ul>
<li><em>Communication Tip:</em> Provide concrete examples of tasks where each excels.</li>
</ul></li>
</ul></li>
<li><p><strong>Mention Representation Learning (Briefly):</strong></p>
<ul>
<li>“MLM tends to generate better contextualized word embeddings due to its bidirectional nature.”</li>
<li>“Autoregressive models also produce embeddings, but they are biased toward the preceding context.”</li>
</ul></li>
<li><p><strong>Address Real-World Considerations (Briefly):</strong></p>
<ul>
<li>“MLM can be more computationally expensive, but implementation tricks like masking strategy are important.”</li>
<li>“Autoregressive models are generally more efficient and benefit from techniques like beam search.”</li>
</ul></li>
<li><p><strong>Conclude with a Summary:</strong></p>
<ul>
<li>“In summary, MLM and autoregressive models offer different trade-offs. MLM provides deeper contextual understanding, while autoregressive models excel at sequential generation. The best choice depends on the task at hand.”</li>
</ul></li>
</ol>
<ul>
<li><strong>General Communication Tips:</strong>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to process the information.</li>
<li><strong>Check for Understanding:</strong> Periodically ask, “Does that make sense?” or “Do you have any questions so far?”</li>
<li><strong>Focus on Key Concepts:</strong> Don’t get bogged down in minor details. Highlight the most important ideas.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you’re interviewing in person, use a whiteboard to draw diagrams or write down key equations.</li>
<li><strong>Be Prepared for Follow-Up Questions:</strong> The interviewer may ask you to elaborate on certain aspects of your answer or to compare the two approaches in more detail. Be ready to provide additional examples and insights.</li>
</ul></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>