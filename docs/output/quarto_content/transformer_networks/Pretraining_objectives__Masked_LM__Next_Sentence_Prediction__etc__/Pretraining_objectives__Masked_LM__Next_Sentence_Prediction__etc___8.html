<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>pretraining_objectives__masked_lm__next_sentence_prediction__etc___8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-9.-in-settings-with-noisy-or-domain-specific-text-e.g.-medical-records-or-informal-social-media-what-modifications-to-pretraining-objectives-would-you-consider-to-ensure-robust-performance" class="level2">
<h2 class="anchored" data-anchor-id="question-9.-in-settings-with-noisy-or-domain-specific-text-e.g.-medical-records-or-informal-social-media-what-modifications-to-pretraining-objectives-would-you-consider-to-ensure-robust-performance">Question: 9. In settings with noisy or domain-specific text (e.g., medical records or informal social media), what modifications to pretraining objectives would you consider to ensure robust performance?</h2>
<p><strong>Best Answer</strong></p>
<p>When dealing with noisy or domain-specific text during pretraining, the standard pretraining objectives like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) may not be sufficient to ensure robust performance. Several modifications can be considered to address the challenges posed by noise and domain specificity.</p>
<section id="domain-adaptation-techniques" class="level3">
<h3 class="anchored" data-anchor-id="domain-adaptation-techniques">1. Domain Adaptation Techniques:</h3>
<p>Fine-tuning on domain-specific data is a crucial step, but adapting the pretraining phase itself can significantly improve performance. Here are a few approaches:</p>
<ul>
<li><p><strong>a) Continued Pretraining:</strong> After initial pretraining on a large general-purpose corpus, continue pretraining on the domain-specific data. This allows the model to adapt its parameters specifically to the new domain’s nuances and vocabulary. This is especially useful when there’s limited domain-specific data available.</p></li>
<li><p><strong>b) Multi-task Pretraining:</strong> Train the model with a combination of the original pretraining objectives (MLM, NSP) and auxiliary tasks relevant to the target domain. For example, in the medical domain, one could add a task to predict medical codes or entities from the text. The loss function becomes a weighted sum: <span class="math display">\[
L = \lambda_1 L_{MLM} + \lambda_2 L_{NSP} + \lambda_3 L_{auxiliary}
\]</span> where <span class="math inline">\(\lambda_i\)</span> are weights controlling the contribution of each task.</p></li>
<li><p><strong>c) Adversarial Domain Adaptation:</strong> Use adversarial training to make the model invariant to domain differences. A domain discriminator is trained to distinguish between the general-purpose and domain-specific data, while the main model is trained to fool the discriminator. This encourages the model to learn domain-invariant features.</p></li>
</ul>
</section>
<section id="adjusting-masking-strategies" class="level3">
<h3 class="anchored" data-anchor-id="adjusting-masking-strategies">2. Adjusting Masking Strategies:</h3>
<ul>
<li><p><strong>a) Domain-Specific Vocabulary Masking:</strong> Instead of randomly masking tokens, prioritize masking domain-specific terms. This forces the model to learn the context and relationships between these important terms. The masking probability can be adjusted based on the term frequency or importance. For example, in medical text, rare medical terms should be masked more frequently.</p></li>
<li><p><strong>b) N-gram Masking:</strong> Masking consecutive n-grams instead of single tokens can be beneficial, especially when dealing with domain-specific phrases or entities. This encourages the model to learn longer-range dependencies and contextual information.</p></li>
<li><p><strong>c) Unmasking Important Tokens:</strong> In noisy data, some tokens might be crucial for understanding the context. A strategy to prevent masking of certain high-information tokens (e.g., named entities, key medical terms) could be beneficial. This can be implemented by adjusting the masking probability based on token importance.</p></li>
</ul>
</section>
<section id="denoising-objectives" class="level3">
<h3 class="anchored" data-anchor-id="denoising-objectives">3. Denoising Objectives:</h3>
<ul>
<li><p><strong>a) Denoising Autoencoders (DAE):</strong> Introduce noise to the input text (e.g., random character swaps, deletions, insertions) and train the model to reconstruct the original text. This helps the model become robust to noise and learn more reliable representations. The objective is to minimize the reconstruction loss: <span class="math display">\[
L_{DAE} = \mathbb{E}_{x \sim p_{data}(x), \tilde{x} \sim q(\tilde{x}|x)} [||f(\tilde{x}) - x||^2]
\]</span> where <span class="math inline">\(x\)</span> is the original text, <span class="math inline">\(\tilde{x}\)</span> is the noisy version, and <span class="math inline">\(f(\tilde{x})\)</span> is the model’s output.</p></li>
<li><p><strong>b) Back-Translation:</strong> Use a machine translation model to translate the noisy text into a cleaner version and then back to the original language. Train the model to predict the original noisy text from the back-translated text. This encourages the model to learn robust representations that are invariant to noise.</p></li>
<li><p><strong>c) Sequence-to-Sequence Denoising:</strong> Treat the noisy text as the input sequence and the clean or corrected text as the target sequence. Train the model to generate the clean text from the noisy text. This requires a parallel dataset of noisy and clean text, which can be created through data augmentation or manual correction.</p></li>
</ul>
</section>
<section id="handling-data-heterogeneity" class="level3">
<h3 class="anchored" data-anchor-id="handling-data-heterogeneity">4. Handling Data Heterogeneity:</h3>
<ul>
<li><p><strong>a) Weighted Sampling:</strong> If the dataset contains different types of text with varying levels of noise or relevance, use weighted sampling to ensure that the model is trained on a balanced representation of each type. Assign higher weights to cleaner or more relevant data samples.</p></li>
<li><p><strong>b) Mixture of Experts:</strong> Use a mixture of experts architecture where each expert is trained on a specific subset of the data (e.g., based on noise level or domain). A gating network learns to route each input to the appropriate expert.</p></li>
</ul>
</section>
<section id="implementation-details-and-corner-cases" class="level3">
<h3 class="anchored" data-anchor-id="implementation-details-and-corner-cases">5. Implementation Details and Corner Cases:</h3>
<ul>
<li><p><strong>Computational Cost:</strong> Many of these techniques, such as continued pretraining and multi-task pretraining, can be computationally expensive. Careful consideration should be given to the resources available and the trade-offs between performance and cost.</p></li>
<li><p><strong>Hyperparameter Tuning:</strong> The learning rates, masking probabilities, and weights for the different loss functions should be carefully tuned. A validation set should be used to evaluate the performance of the model and optimize these hyperparameters.</p></li>
<li><p><strong>Data Augmentation:</strong> Creating synthetic data through data augmentation techniques can be helpful, especially when the amount of domain-specific data is limited. However, it is important to ensure that the augmented data is realistic and does not introduce new biases.</p></li>
<li><p><strong>Evaluation Metrics:</strong> Standard evaluation metrics like perplexity may not be sufficient to evaluate the robustness of the model. Consider using metrics that are specifically designed to measure robustness, such as adversarial accuracy or the ability to generalize to unseen noise patterns.</p></li>
</ul>
<p>By carefully considering these modifications to pretraining objectives and implementation details, one can significantly improve the performance of language models on noisy or domain-specific text.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to articulate this in an interview:</p>
<ol type="1">
<li><strong>Start with the Problem:</strong>
<ul>
<li>“When dealing with noisy or domain-specific text, standard pretraining objectives like MLM and NSP often fall short. The challenge lies in adapting the model to the specific characteristics of the data, such as domain-specific vocabulary, noise patterns, and data heterogeneity.”</li>
</ul></li>
<li><strong>Discuss Domain Adaptation:</strong>
<ul>
<li>“One crucial area is domain adaptation. We can consider approaches like continued pretraining, where we fine-tune the pretrained model on the domain-specific data. Alternatively, multi-task pretraining allows us to train the model with auxiliary tasks relevant to the domain. For instance, in the medical domain, we could add a task to predict medical codes, using a loss function that combines MLM, NSP, and the auxiliary task with appropriate weights.”</li>
<li>(If the interviewer seems interested in mathematical details) “Formally, the loss function becomes a weighted sum: <span class="math inline">\(L = \lambda_1 L_{MLM} + \lambda_2 L_{NSP} + \lambda_3 L_{auxiliary}\)</span> where the lambdas control each task’s contribution.” (Pause briefly for the interviewer to absorb the equation before moving on).</li>
</ul></li>
<li><strong>Explain Masking Strategies:</strong>
<ul>
<li>“Adjusting masking strategies is another key aspect. Instead of randomly masking tokens, we can prioritize masking domain-specific terms to force the model to learn their context. We can use N-gram masking to help the model understand domain specific phrases. Conversely, unmasking important tokens can prevent the model from discarding valuable information.”</li>
</ul></li>
<li><strong>Elaborate on Denoising Objectives:</strong>
<ul>
<li>“Denoising objectives can also be very useful. Techniques like denoising autoencoders involve introducing noise into the input and training the model to reconstruct the original text, improving robustness. Or, consider back-translation, which involves translating the noisy text into a cleaner version and back, training the model to predict the original text. We can represent DAE process with the following equation: <span class="math inline">\(L_{DAE} = \mathbb{E}_{x \sim p_{data}(x), \tilde{x} \sim q(\tilde{x}|x)} [||f(\tilde{x}) - x||^2]\)</span>.”</li>
</ul></li>
<li><strong>Address Data Heterogeneity:</strong>
<ul>
<li>“To handle data heterogeneity, we can use techniques like weighted sampling to balance the representation of different types of text. Alternatively, a mixture of experts architecture can be used where each expert is trained on a specific subset of data, and a gating network routes each input to the appropriate expert.”</li>
</ul></li>
<li><strong>Discuss Implementation and Caveats:</strong>
<ul>
<li>“It’s important to consider implementation details. Many of these techniques can be computationally expensive, so careful consideration should be given to the available resources. Also, the hyperparameters, learning rates, and weights should be carefully tuned using a validation set. We also need to carefully evaluate the performance of the model. Standard metrics may not be sufficient. You need to think about robustness.”</li>
</ul></li>
<li><strong>Conclude Confidently:</strong>
<ul>
<li>“By carefully considering these modifications to pretraining objectives and implementation details, we can significantly improve the performance and robustness of language models on challenging, real-world datasets.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Check for Understanding:</strong> Periodically ask the interviewer if they have any questions or if they would like you to elaborate on a specific point.</li>
<li><strong>Use Examples:</strong> Illustrate your points with concrete examples from the medical or social media domains to make the concepts more tangible.</li>
<li><strong>Tailor Your Response:</strong> Adjust the level of detail based on the interviewer’s background and interest. If they seem particularly interested in a specific technique, delve deeper into it.</li>
<li><strong>Be Prepared to Justify Your Choices:</strong> Be ready to explain why you chose specific modifications to the pretraining objectives and why they are appropriate for the given scenario.</li>
<li><strong>Show Enthusiasm:</strong> Demonstrate your passion for the topic and your eagerness to tackle challenging problems in the field of NLP.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>