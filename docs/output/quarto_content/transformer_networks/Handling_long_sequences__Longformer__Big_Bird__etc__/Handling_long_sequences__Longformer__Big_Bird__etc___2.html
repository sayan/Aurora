<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>handling_long_sequences__longformer__big_bird__etc___2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-3.-can-you-discuss-the-key-differences-between-longformer-and-big-bird-in-terms-of-their-attention-mechanisms-and-scalability" class="level2">
<h2 class="anchored" data-anchor-id="question-3.-can-you-discuss-the-key-differences-between-longformer-and-big-bird-in-terms-of-their-attention-mechanisms-and-scalability">Question: 3. Can you discuss the key differences between Longformer and Big Bird in terms of their attention mechanisms and scalability?</h2>
<p><strong>Best Answer</strong></p>
<p>Transformer models have revolutionized NLP, but their quadratic complexity with respect to sequence length (<span class="math inline">\(O(n^2)\)</span>) limits their application to long sequences. Longformer and Big Bird are two prominent models designed to address this limitation by introducing sparse attention mechanisms. While both aim to reduce the computational cost, they employ different strategies with distinct trade-offs.</p>
<p><strong>1. Attention Mechanisms:</strong></p>
<ul>
<li><p><strong>Longformer:</strong> Longformer employs a combination of three attention mechanisms:</p>
<ul>
<li><strong>Sliding Window Attention:</strong> Each token attends to its <span class="math inline">\(w/2\)</span> neighbors on either side, where <span class="math inline">\(w\)</span> is the window size. This captures local context effectively. The complexity is <span class="math inline">\(O(n*w)\)</span>, linear with sequence length <span class="math inline">\(n\)</span>.</li>
<li><strong>Global Attention:</strong> A few pre-selected tokens (e.g., CLS token for classification) attend to all tokens and are attended <em>by</em> all tokens. This allows the model to gather information from the entire sequence. If <span class="math inline">\(g\)</span> is the number of global tokens, the complexity is <span class="math inline">\(O(n*g)\)</span>, linear with <span class="math inline">\(n\)</span>.</li>
<li><strong>Task-Specific Attention:</strong> Certain task-specific tokens also attend to all tokens in the sequence.</li>
</ul>
<p>The overall complexity of Longformer’s attention is <span class="math inline">\(O(n*w + n*g)\)</span>, which is linear in sequence length.</p></li>
<li><p><strong>Big Bird:</strong> Big Bird combines three different attention mechanisms:</p>
<ul>
<li><strong>Random Attention:</strong> Each token attends to a small number (<span class="math inline">\(r\)</span>) of randomly chosen tokens. This provides a form of global context. The complexity is <span class="math inline">\(O(n*r)\)</span>.</li>
<li><strong>Windowed Attention:</strong> Similar to Longformer, each token attends to its <span class="math inline">\(w/2\)</span> neighbors on either side, capturing local context. The complexity is <span class="math inline">\(O(n*w)\)</span>.</li>
<li><strong>Global Attention:</strong> Similar to Longformer, a set of global tokens attend to all tokens and are attended <em>by</em> all tokens.</li>
</ul>
<p>The overall complexity of Big Bird’s attention is <span class="math inline">\(O(n*r + n*w + n*g)\)</span>, also linear in sequence length.</p></li>
</ul>
<p><strong>2. Scalability and Computational Complexity:</strong></p>
<p>Both Longformer and Big Bird achieve linear complexity, enabling them to process much longer sequences than standard Transformers. However, the specific constants within the complexity (<span class="math inline">\(w\)</span>, <span class="math inline">\(r\)</span>, <span class="math inline">\(g\)</span>) influence actual performance.</p>
<ul>
<li><strong>Longformer:</strong> The window size <span class="math inline">\(w\)</span> is a crucial hyperparameter. A larger <span class="math inline">\(w\)</span> allows capturing more local context but increases computation. The number of global tokens <span class="math inline">\(g\)</span> is typically small (e.g., 1 for CLS token). Longformer’s sliding window attention is highly efficient on hardware due to its regular structure.</li>
<li><strong>Big Bird:</strong> The number of random connections <span class="math inline">\(r\)</span> is a key parameter. More random connections provide better approximation of full attention but also increase computational cost. The random attention in Big Bird can be less hardware-friendly than Longformer’s sliding window because of memory access patterns. Theoretical justification relies on approximating the full attention matrix with a sparse matrix and uses theorems such as the ETC (Eulerian Tour Cover) theorem to prove universal approximation capabilities of the model. The ETC theorem can be formalized as follows: <span class="math display">\[
\exists \text{ a graph } G = (V, E) \text{ such that } \forall u, v \in V, \exists \text{ a path from } u \text{ to } v \text{ of length at most } L
\]</span> This ensures that information can propagate between any two nodes in a limited number of steps. BigBird leverages this property by ensuring a connected graph of attention through random, local, and global connections.</li>
</ul>
<p><strong>3. Implementation Details and Trade-offs:</strong></p>
<ul>
<li><strong>Longformer:</strong> Implementation benefits from efficient CUDA kernels for sliding window attention. It is relatively straightforward to implement and integrate into existing Transformer architectures.</li>
<li><strong>Big Bird:</strong> Implementation is more complex due to the random attention pattern, which can be less amenable to hardware acceleration. Efficient implementations often rely on custom CUDA kernels and careful memory management.</li>
</ul>
<p><strong>4. Performance Differences:</strong></p>
<p>The choice between Longformer and Big Bird depends on the specific task and dataset.</p>
<ul>
<li><strong>Longformer:</strong> Often performs well on tasks where local context is crucial, such as document classification, question answering, and summarization. The sliding window captures local dependencies well, and the global attention allows for gathering relevant information from the entire sequence.</li>
<li><strong>Big Bird:</strong> Can be effective on tasks where long-range dependencies and global context are important, such as genomics or tasks requiring reasoning over very long documents. The random attention helps capture distant relationships.</li>
</ul>
<p><strong>5. Mathematical Intuition Behind Sparse Attention:</strong></p>
<p>The motivation behind sparse attention mechanisms can be understood from the perspective of approximating the full attention matrix. Let <span class="math inline">\(A\)</span> be the full attention matrix, where <span class="math inline">\(A_{ij}\)</span> represents the attention weight between token <span class="math inline">\(i\)</span> and token <span class="math inline">\(j\)</span>. In a standard Transformer, <span class="math inline">\(A\)</span> is dense. Sparse attention methods aim to approximate <span class="math inline">\(A\)</span> with a sparse matrix <span class="math inline">\(\tilde{A}\)</span> such that: <span class="math display">\[
\tilde{A} \approx A
\]</span> The specific sparsity pattern (e.g., sliding window, random) determines how well <span class="math inline">\(\tilde{A}\)</span> approximates <span class="math inline">\(A\)</span>. In Longformer, the sliding window captures local dependencies, while global tokens capture global information. In Big Bird, the random attention provides a probabilistic approximation of the full attention matrix. BigBird leverages an ETC graph-based attention mechanism. Specifically it combines <span class="math inline">\(r\)</span> random attention, <span class="math inline">\(w\)</span> window attention and <span class="math inline">\(g\)</span> global attention. By ETC theorem, such an attention mechanism can approximate the full attention with a relatively small cost.</p>
<p><strong>6. Practical Considerations:</strong></p>
<ul>
<li><strong>Memory Usage:</strong> Both models significantly reduce memory usage compared to standard Transformers but still require substantial memory for very long sequences. Techniques like gradient checkpointing are often used to further reduce memory consumption.</li>
<li><strong>Hyperparameter Tuning:</strong> The window size <span class="math inline">\(w\)</span> (Longformer), number of random connections <span class="math inline">\(r\)</span> (Big Bird), and number of global tokens <span class="math inline">\(g\)</span> are critical hyperparameters that need to be carefully tuned for each task.</li>
<li><strong>Hardware Acceleration:</strong> Optimizing these models for specific hardware (e.g., GPUs, TPUs) is essential for achieving good performance.</li>
</ul>
<p><strong>In summary,</strong> Longformer and Big Bird are both effective approaches for handling long sequences with linear complexity. Longformer’s sliding window attention is efficient for capturing local context, while Big Bird’s random attention can capture long-range dependencies. The choice between the two depends on the specific task, dataset, and hardware constraints. The mathematical justification for these models lies in their ability to approximate the full attention mechanism with a sparse alternative, trading off some accuracy for significant computational gains.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to deliver this answer in an interview, walking the interviewer through the complexities without overwhelming them:</p>
<ol type="1">
<li><strong>Start with the Problem:</strong>
<ul>
<li>“Standard Transformers have quadratic complexity, making them impractical for long sequences. Longformer and Big Bird address this using sparse attention.”</li>
<li><em>Communication Tip:</em> Frame the answer in the context of solving a real problem.</li>
</ul></li>
<li><strong>Explain Attention Mechanisms (High Level):</strong>
<ul>
<li>“Both models use a combination of attention mechanisms. Longformer uses sliding window and global attention, while Big Bird uses random, windowed, and global attention.”</li>
<li><em>Communication Tip:</em> Avoid diving into too much detail immediately. Give a broad overview first.</li>
</ul></li>
<li><strong>Delve into Longformer:</strong>
<ul>
<li>“Longformer’s sliding window attention is like looking at nearby words, capturing local context very efficiently. Global attention lets certain tokens ‘see’ the entire sequence.”</li>
<li>“The complexity is O(n<em>w + n</em>g), linear in sequence length because the window size <em>w</em> and number of global tokens <em>g</em> are fixed.”</li>
<li><em>Communication Tip:</em> Use analogies (“like looking at nearby words”) to make concepts more accessible.</li>
</ul></li>
<li><strong>Explain Big Bird:</strong>
<ul>
<li>“Big Bird uses a combination of random attention, which connects each token to a few random tokens, as well as windowed and global attention.”</li>
<li>“The random attention is inspired by approximation and graph connectivity theorems such as the ETC theorem, which demonstrates that full attention can be approximated with a sparse model. The complexity is O(n<em>r + n</em>w + n*g), which is also linear.”</li>
<li><em>Communication Tip:</em> Break down random attention and ETC Theorem into digestible parts.</li>
</ul></li>
<li><strong>Discuss Scalability and Trade-offs:</strong>
<ul>
<li>“Both models are linear, but the constants matter. Longformer’s sliding window is hardware-friendly. Big Bird’s random attention can be harder to optimize.”</li>
<li><em>Communication Tip:</em> Acknowledge that the theoretical complexity is only part of the story.</li>
</ul></li>
<li><strong>Mention Performance and Applications:</strong>
<ul>
<li>“Longformer is good for tasks needing local context like document classification. Big Bird is better for long-range dependencies, like genomics or reasoning over very long documents.”</li>
<li><em>Communication Tip:</em> Connect the models to specific use cases to show practical understanding.</li>
</ul></li>
<li><strong>Address Implementation and Math (If Asked):</strong>
<ul>
<li>“Efficient implementation often involves custom CUDA kernels and memory management. The sparse structure allows us to approximate the attention matrix, trading some accuracy for significant computational gains.”</li>
<li><em>Communication Tip:</em> Only dive into the math if the interviewer seems interested or asks directly. Briefly explain the underlying idea without getting bogged down in formulas unless prompted. You can say something like, “The core idea, if you’re interested, can be formulated as…”</li>
</ul></li>
<li><strong>Summarize and Offer More Detail:</strong>
<ul>
<li>“In short, both Longformer and Big Bird are ways to make Transformers work on long sequences. The choice depends on the task, the data, and hardware.”</li>
<li>“I’m happy to go into more detail about any specific aspect you’d like to discuss further.”</li>
<li><em>Communication Tip:</em> End with a summary and an invitation for further questions. This demonstrates confidence and mastery.</li>
</ul></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>