<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>handling_long_sequences__longformer__big_bird__etc___9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-10.-what-are-some-deployment-considerations-when-using-models-like-longformer-or-big-bird-in-a-production-environment-particularly-with-respect-to-latency-and-hardware-requirements" class="level2">
<h2 class="anchored" data-anchor-id="question-10.-what-are-some-deployment-considerations-when-using-models-like-longformer-or-big-bird-in-a-production-environment-particularly-with-respect-to-latency-and-hardware-requirements">Question: 10. What are some deployment considerations when using models like Longformer or Big Bird in a production environment, particularly with respect to latency and hardware requirements?</h2>
<p><strong>Best Answer</strong></p>
<p>Deploying models like Longformer and Big Bird in a production environment presents unique challenges due to their architecture designed to handle long sequences. These challenges primarily revolve around latency, hardware requirements, and the need for optimized inference pipelines.</p>
<p>Here’s a breakdown of the key considerations:</p>
<ul>
<li><p><strong>Latency:</strong></p>
<ul>
<li><p><strong>Sequence Length Dependence:</strong> The inference time of Longformer and Big Bird scales super-linearly with the input sequence length. While they are more efficient than standard Transformers (which have <span class="math inline">\(O(N^2)\)</span> complexity where <span class="math inline">\(N\)</span> is the sequence length), they still require significant computational resources for long sequences. The exact complexity of Longformer depends on the configuration (e.g., window size for local attention, number of global attention tokens), but it often scales as <span class="math inline">\(O(N \cdot w)\)</span> for local attention with window size <em>w</em>, plus <span class="math inline">\(O(N \cdot g)\)</span> where <span class="math inline">\(g\)</span> is the number of global tokens. Big Bird similarly has reduced complexity, but still faces challenges.</p></li>
<li><p><strong>Attention Mechanisms:</strong> The sparse attention mechanisms used (e.g., sliding window, global tokens, random attention) introduce overhead. Implementing these efficiently on hardware requires careful consideration.</p></li>
<li><p><strong>Real-time vs.&nbsp;Batch Processing:</strong></p>
<ul>
<li><em>Real-time scenarios</em> (e.g., live chat analysis) demand low latency, potentially necessitating smaller sequence lengths or model quantization to reduce computational load.</li>
<li><em>Batch processing scenarios</em> (e.g., overnight document summarization) offer more flexibility in terms of latency but still require efficient resource management.</li>
</ul></li>
</ul></li>
<li><p><strong>Hardware Requirements:</strong></p>
<ul>
<li><p><strong>Memory Footprint:</strong> Longformer and Big Bird models, especially when dealing with long sequences, have a large memory footprint. This can be a bottleneck, particularly when deploying on resource-constrained devices or serving multiple models concurrently.</p></li>
<li><p><strong>GPU Acceleration:</strong> GPUs are almost essential for achieving acceptable inference speeds with these models. The size and number of GPUs depend on the expected throughput and latency requirements. Considerations include:</p>
<ul>
<li><em>GPU Memory:</em> Ensure sufficient GPU memory to accommodate the model and intermediate activations during inference. Model parallelism might be required to distribute the model across multiple GPUs if it doesn’t fit on a single GPU.</li>
<li><em>GPU Compute:</em> Sufficient compute power to handle the attention calculations.</li>
</ul></li>
<li><p><strong>CPU Inference (less common):</strong> While possible, CPU inference will typically be significantly slower. Optimized libraries (e.g., Intel MKL) and quantization can help improve performance.</p></li>
</ul></li>
<li><p><strong>Optimization Techniques:</strong></p>
<ul>
<li><p><strong>Model Quantization:</strong> Reducing the precision of model weights and activations (e.g., from FP32 to FP16 or INT8) can significantly reduce memory footprint and improve inference speed, often with minimal loss in accuracy. Techniques include:</p>
<ul>
<li><em>Post-Training Quantization:</em> Quantizing a pre-trained model.</li>
<li><em>Quantization-Aware Training:</em> Training the model with quantization in mind.</li>
</ul></li>
<li><p><strong>Knowledge Distillation:</strong> Training a smaller, faster model to mimic the behavior of the larger Longformer or Big Bird model. The smaller model can then be deployed in production.</p></li>
<li><p><strong>Kernel Fusion:</strong> Combining multiple operations into a single kernel to reduce memory access and improve computational efficiency. Frameworks like TensorRT can automatically perform kernel fusion.</p></li>
<li><p><strong>Custom CUDA Kernels:</strong> Writing specialized CUDA kernels for the sparse attention operations can provide significant performance gains, especially if the default implementations are not optimized for the specific hardware.</p></li>
<li><p><strong>Pruning:</strong> Removing less important connections (weights) in the network to reduce model size and computational complexity.</p></li>
<li><p><strong>Dynamic Batching:</strong> Dynamically grouping incoming requests into batches of varying sizes based on sequence length. This can improve throughput but requires careful management to avoid excessive latency for short sequences. For instance, longer sequences could be grouped together to maximize GPU utilization for those computationally intensive examples.</p></li>
</ul></li>
<li><p><strong>Input Handling:</strong></p>
<ul>
<li><strong>Variable Sequence Lengths:</strong> Real-world data often contains sequences of varying lengths. Padding shorter sequences to the maximum length can be inefficient. Techniques for handling variable sequence lengths include:
<ul>
<li><em>Bucketing:</em> Grouping sequences of similar lengths together to minimize padding.</li>
<li><em>Dynamic Unrolling:</em> Unrolling the computational graph based on the actual sequence length.</li>
</ul></li>
<li><strong>Truncation:</strong> Setting a maximum sequence length and truncating longer sequences. This is a simple but potentially lossy approach. Considerations include:
<ul>
<li><em>Where to truncate:</em> Truncating at the beginning, end, or using more sophisticated methods based on content.</li>
<li><em>Impact on downstream tasks:</em> Evaluate the impact of truncation on the accuracy of the task.</li>
</ul></li>
</ul></li>
<li><p><strong>Frameworks and Tools:</strong></p>
<ul>
<li><p><strong>TensorRT:</strong> NVIDIA’s TensorRT is a high-performance inference optimizer and runtime that can significantly accelerate inference on NVIDIA GPUs. It supports model quantization, kernel fusion, and other optimization techniques.</p></li>
<li><p><strong>ONNX Runtime:</strong> A cross-platform inference engine that supports a wide range of hardware and frameworks.</p></li>
<li><p><strong>Transformers Library:</strong> Hugging Face’s Transformers library provides optimized implementations of Longformer and Big Bird, as well as tools for quantization and other optimization techniques.</p></li>
</ul></li>
<li><p><strong>Monitoring and Profiling:</strong></p>
<ul>
<li><strong>Latency Monitoring:</strong> Track inference latency to identify performance bottlenecks.</li>
<li><strong>Resource Utilization Monitoring:</strong> Monitor CPU, GPU, and memory utilization to ensure efficient resource allocation.</li>
<li><strong>Profiling:</strong> Use profiling tools to identify hotspots in the code and guide optimization efforts.</li>
</ul></li>
</ul>
<p><strong>Example Mathematical Considerations:</strong></p>
<p>Let’s consider the computational complexity of a standard Transformer layer and contrast it with Longformer.</p>
<ul>
<li><p><strong>Standard Transformer:</strong> The self-attention mechanism has a complexity of <span class="math inline">\(O(N^2)\)</span>, where <span class="math inline">\(N\)</span> is the sequence length. This arises from the dot product attention calculation.</p></li>
<li><p><strong>Longformer (with sliding window attention):</strong> Assume a window size of <span class="math inline">\(w\)</span> around each token. The complexity becomes <span class="math inline">\(O(N \cdot w)\)</span>. If we also have <span class="math inline">\(g\)</span> global attention tokens, we add <span class="math inline">\(O(N \cdot g)\)</span> complexity. The total complexity is <span class="math inline">\(O(N \cdot (w + g))\)</span>. If <span class="math inline">\(w\)</span> and <span class="math inline">\(g\)</span> are much smaller than <span class="math inline">\(N\)</span>, this represents a significant improvement over the quadratic complexity of the standard Transformer.</p></li>
</ul>
<p>The memory requirements also change drastically. A full attention matrix has size <span class="math inline">\(N^2\)</span>. In LongFormer, with a sliding window of <span class="math inline">\(w\)</span>, memory becomes <span class="math inline">\(O(N \cdot w)\)</span>.</p>
<p><strong>Real-World Considerations:</strong></p>
<ul>
<li><strong>Cost:</strong> GPU instances can be expensive. Balancing performance requirements with cost is crucial.</li>
<li><strong>Maintenance:</strong> Maintaining custom CUDA kernels or optimized inference pipelines requires specialized expertise.</li>
<li><strong>Reproducibility:</strong> Ensure that the optimized inference pipeline is reproducible across different environments.</li>
<li><strong>Explainability:</strong> Quantization and other optimization techniques can sometimes affect the explainability of the model.</li>
<li><strong>Security:</strong> Be mindful of security implications, especially when deploying models that handle sensitive data.</li>
</ul>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to approach this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong></p>
<ul>
<li>“Deploying models like Longformer and Big Bird presents unique challenges primarily due to their architecture designed to handle long sequences, particularly in terms of latency and hardware resources.”</li>
</ul></li>
<li><p><strong>Discuss Latency:</strong></p>
<ul>
<li>“One major consideration is latency. Unlike standard Transformers, these models have a sub-quadratic complexity, but the inference time still increases significantly with sequence length. I’d discuss the different components that contribute to latency: the sequence length itself, the attention mechanism, and whether you’re dealing with a real-time or batch processing scenario.”</li>
<li>“In <em>real-time scenarios</em>, low latency is critical. You might need to use smaller sequence lengths or model quantization to speed things up. <em>Batch processing</em> offers more flexibility.”</li>
</ul></li>
<li><p><strong>Explain Hardware Requirements:</strong></p>
<ul>
<li>“Hardware is another key factor. These models have a large memory footprint, so GPUs are almost essential for achieving acceptable performance. Consider GPU memory and compute power.”</li>
<li>“If the model is too large for a single GPU, you might need to use model parallelism to distribute it across multiple GPUs.”</li>
<li>“It’s also <em>possible</em> to use CPUs, but it will be significantly slower, so optimizations like using Intel MKL or quantization become even more important.”</li>
</ul></li>
<li><p><strong>Detail Optimization Techniques (Pick 2-3 and go deep):</strong></p>
<ul>
<li>“To address these challenges, several optimization techniques can be applied. I’ll focus on model quantization, knowledge distillation, and dynamic batching since they are common and effective.
<ul>
<li><strong>Quantization</strong>: Reducing the precision of weights and activations can substantially lower memory usage and improve speed, usually with minimal accuracy impact. Techniques such as Post-Training Quantization or Quantization-Aware Training can be considered.</li>
<li><strong>Knowledge Distillation:</strong> Another effective approach is Knowledge Distillation, where we train a smaller, faster model to replicate the behavior of the larger Longformer or Big Bird model for deployment.</li>
<li><strong>Dynamic Batching:</strong> Implementing Dynamic Batching can improve throughput by grouping requests into variable-sized batches based on sequence length, which maximizes GPU utilization.”</li>
</ul></li>
</ul></li>
<li><p><strong>Address Input Handling:</strong></p>
<ul>
<li>“Real-world data often contains sequences of variable lengths. You can use techniques like bucketing or dynamic unrolling to handle this efficiently.”</li>
<li>“Truncation is another option, but you need to be careful about where you truncate and how it affects accuracy.”</li>
</ul></li>
<li><p><strong>Mention Frameworks and Tools:</strong></p>
<ul>
<li>“Frameworks like TensorRT and ONNX Runtime can significantly accelerate inference. Hugging Face’s Transformers library also provides optimized implementations and tools.”</li>
</ul></li>
<li><p><strong>Emphasize Monitoring and Profiling:</strong></p>
<ul>
<li>“Finally, it’s crucial to monitor latency and resource utilization to identify performance bottlenecks and guide optimization efforts.”</li>
</ul></li>
<li><p><strong>Mathematical Considerations (Optional - gauge the interviewer’s interest):</strong></p>
<ul>
<li>“To give you an idea of why these optimizations are crucial, let’s consider the computational complexity. Standard Transformers have a complexity of <span class="math inline">\(O(N^2)\)</span>, while Longformer with sliding window attention has a complexity of <span class="math inline">\(O(N \cdot w)\)</span>, where <span class="math inline">\(N\)</span> is the sequence length and <span class="math inline">\(w\)</span> is the window size. This reduction in complexity is why Longformer can handle much longer sequences.” <em>[Only say this if they seem interested in details.]</em> Adjust based on the interviewer. Don’t overwhelm them with equations right away.</li>
</ul></li>
<li><p><strong>Real-World Considerations:</strong></p>
<ul>
<li>“Keep in mind the cost of GPU instances, the maintenance overhead of custom kernels, and the need for reproducibility and security.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to absorb the information.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you’re in a virtual interview, consider sharing your screen to show diagrams or code snippets.</li>
<li><strong>Engage the Interviewer:</strong> Ask if they have any questions or if they’d like you to elaborate on a specific point.</li>
<li><strong>Don’t Be Afraid to Say “It Depends”:</strong> The best approach often depends on the specific application and constraints.</li>
<li><strong>Be Honest About Your Knowledge:</strong> If you’re not familiar with a particular technique, it’s better to be honest than to try to bluff your way through it.</li>
<li><strong>Show Enthusiasm:</strong> Let your passion for the topic shine through.</li>
</ul>
<p>By following these guidelines, you can deliver a comprehensive and compelling answer that demonstrates your senior-level expertise in deploying long sequence models.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>