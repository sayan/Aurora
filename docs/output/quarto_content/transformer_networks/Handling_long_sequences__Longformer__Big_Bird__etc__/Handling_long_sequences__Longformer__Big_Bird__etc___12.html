<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>handling_long_sequences__longformer__big_bird__etc___12</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-13.-how-would-you-integrate-domain-specific-knowledge-into-a-long-sequence-model-for-example-adjusting-tokenization-strategies-or-attention-patterns-when-processing-specialized-texts-such-as-legal-or-medical-documents." class="level2">
<h2 class="anchored" data-anchor-id="question-13.-how-would-you-integrate-domain-specific-knowledge-into-a-long-sequence-model-for-example-adjusting-tokenization-strategies-or-attention-patterns-when-processing-specialized-texts-such-as-legal-or-medical-documents.">Question: 13. How would you integrate domain-specific knowledge into a long-sequence model? For example, adjusting tokenization strategies or attention patterns when processing specialized texts such as legal or medical documents.</h2>
<p><strong>Best Answer</strong></p>
<p>Integrating domain-specific knowledge into long-sequence models, particularly when dealing with specialized texts like legal or medical documents, is crucial for achieving state-of-the-art performance. The key is to tailor the model’s architecture and training process to the unique characteristics of the target domain. Here’s a multi-faceted approach encompassing tokenization, attention mechanisms, and fine-tuning:</p>
<section id="customized-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="customized-tokenization">1. Customized Tokenization</h3>
<p>Standard tokenization methods, such as WordPiece or Byte-Pair Encoding (BPE), often fall short when dealing with domain-specific terminology. Legal and medical documents are rife with jargon, abbreviations, and complex multi-word expressions. Therefore, a customized tokenization strategy is essential.</p>
<ul>
<li><p><strong>Subword Tokenization with Domain-Specific Vocabulary:</strong> Extend the base vocabulary of a standard tokenizer (e.g., SentencePiece) with a domain-specific vocabulary. This enriched vocabulary should include frequent medical terms, legal citations, or specialized acronyms. The enriched vocabulary can be created using frequency analysis on a large domain-specific corpus. If we have a corpus <span class="math inline">\(C_d\)</span> from the domain <span class="math inline">\(d\)</span>, the vocabulary <span class="math inline">\(V_d\)</span> can be created by selecting the top <span class="math inline">\(N\)</span> most frequent tokens or token combinations.</p></li>
<li><p><strong>Rule-Based Tokenization:</strong> Implement rule-based tokenizers that can handle specific patterns in the domain. For example:</p>
<ul>
<li><strong>Legal Documents:</strong> Rules to correctly tokenize legal citations (e.g., “18 U.S.C. § 2252” should be treated as a single token).</li>
<li><strong>Medical Documents:</strong> Rules to tokenize drug names (e.g., “acetaminophen” or complex chemical names) and medical abbreviations (e.g., “MRI,” “CT scan”). This could involve regular expressions or predefined dictionaries.</li>
</ul>
<p>The rule based tokenizer can be formalized as a function <span class="math inline">\(R(text)\)</span> which preprocesses the text before applying the general subword tokenization scheme.</p></li>
<li><p><strong>Character-Level Tokenization for Rare Terms:</strong> For handling out-of-vocabulary (OOV) domain-specific terms, especially long chemical names or rare legal terms, consider character-level tokenization or hybrid approaches. This approach mitigates the OOV problem by representing words as sequences of characters. For instance, instead of <unk> token, we can represent “hydroxychloroquine” as [‘h’, ‘y’, ‘d’, ‘r’, ‘o’, ‘x’, ‘y’, ‘c’, ‘h’, ‘l’, ‘o’, ‘r’, ‘o’, ‘q’, ‘u’, ‘i’, ‘n’, ‘e’].</unk></p></li>
</ul>
</section>
<section id="adapting-attention-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="adapting-attention-mechanisms">2. Adapting Attention Mechanisms</h3>
<p>Long-sequence models like Longformer, Big Bird, and Reformer address the computational challenges of the standard Transformer architecture. However, integrating domain-specific knowledge into their attention mechanisms can further enhance performance.</p>
<ul>
<li><p><strong>Domain-Specific Global Attention:</strong> Designate specific tokens as “global tokens” that attend to all other tokens in the sequence and are attended to by all other tokens. These global tokens can represent key domain-specific concepts or categories. For instance, in legal documents, you might have global tokens for “contract,” “negligence,” or “statute.” Similarly, medical documents could have global tokens for “diagnosis,” “treatment,” or “symptom.”</p>
<p>In the attention mechanism, this can be represented as:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<p>where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> are the query, key, and value matrices. The global tokens modify the attention weights such that certain tokens have increased attention scores.</p></li>
<li><p><strong>Structured Attention Patterns:</strong> Implement structured attention patterns that reflect the hierarchical structure of legal or medical documents. For example, in legal documents, clauses within a contract are related, and sections within a legal code are related. You can design attention patterns that prioritize attention within these related segments.</p></li>
<li><p><strong>Knowledge Graph Integration:</strong> Incorporate knowledge graphs (e.g., UMLS for medicine) to guide attention. Use the knowledge graph to identify related concepts and bias the attention mechanism to prioritize those relationships. This can be achieved through attention weighting schemes that are influenced by the graph relationships. Given a knowledge graph <span class="math inline">\(G = (V, E)\)</span>, the edges <span class="math inline">\(E\)</span> can be used to adjust the attention weights between tokens representing concepts in <span class="math inline">\(V\)</span>.</p></li>
</ul>
</section>
<section id="fine-tuning-on-domain-specific-corpora" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-on-domain-specific-corpora">3. Fine-Tuning on Domain-Specific Corpora</h3>
<p>Pre-trained language models (PLMs) like BERT, RoBERTa, and especially long-sequence PLMs, provide a strong foundation. However, fine-tuning on a large domain-specific corpus is critical to adapt the model to the nuances of the target domain.</p>
<ul>
<li><p><strong>Continued Pre-training (Domain Adaptation):</strong> Before fine-tuning for a specific task, continue pre-training the PLM on a massive corpus of legal or medical texts using masked language modeling (MLM) or other self-supervised objectives. This allows the model to better understand the language patterns and terminology of the domain.</p>
<p>The MLM loss can be expressed as:</p>
<p><span class="math display">\[
L_{MLM} = - \sum_{i \in M} log \, P(w_i | w_{\setminus i})
\]</span></p>
<p>where <span class="math inline">\(M\)</span> is the set of masked tokens and <span class="math inline">\(w_{\setminus i}\)</span> represents the context surrounding the masked token <span class="math inline">\(w_i\)</span>.</p></li>
<li><p><strong>Task-Specific Fine-Tuning:</strong> After domain adaptation, fine-tune the model on a specific task, such as legal contract review, medical report summarization, or clinical note classification. Use labeled data specific to the domain and task.</p></li>
<li><p><strong>Data Augmentation:</strong> Augment the training data with techniques tailored to the domain. For example, in the legal domain, paraphrase legal clauses or generate synthetic legal cases. In the medical domain, use techniques like back-translation or synonym replacement to increase the diversity of the training data.</p></li>
</ul>
</section>
<section id="hybrid-approaches" class="level3">
<h3 class="anchored" data-anchor-id="hybrid-approaches">4. Hybrid Approaches</h3>
<p>Combine different strategies for optimal performance. For example:</p>
<ul>
<li><strong>Rule-Based Preprocessing + Fine-Tuned Model:</strong> Use rule-based tokenization and preprocessing to clean and structure the input text, then fine-tune a pre-trained model on the processed data.</li>
<li><strong>Knowledge-Enhanced Attention + Domain-Specific Vocabulary:</strong> Integrate a knowledge graph to guide attention and use a domain-specific vocabulary to improve tokenization.</li>
</ul>
</section>
<section id="implementation-details-and-corner-cases" class="level3">
<h3 class="anchored" data-anchor-id="implementation-details-and-corner-cases">5. Implementation Details and Corner Cases</h3>
<ul>
<li><strong>Computational Resources:</strong> Fine-tuning long-sequence models on large domain-specific corpora can be computationally expensive. Utilize techniques like gradient accumulation or mixed-precision training to reduce memory usage and accelerate training.</li>
<li><strong>Data Privacy:</strong> When working with sensitive data like medical records, ensure compliance with privacy regulations (e.g., HIPAA). Consider techniques like federated learning or differential privacy to protect patient data.</li>
<li><strong>Evaluation Metrics:</strong> Use evaluation metrics that are appropriate for the specific task and domain. For example, in legal information retrieval, use metrics like precision, recall, and F1-score. In medical text classification, use metrics like accuracy, sensitivity, and specificity.</li>
<li><strong>Overfitting:</strong> Monitor the model for overfitting, especially when fine-tuning on small datasets. Use regularization techniques like dropout or weight decay, and consider using early stopping.</li>
</ul>
<p>By addressing these key areas – tokenization, attention mechanisms, fine-tuning, and practical considerations – we can effectively integrate domain-specific knowledge into long-sequence models and achieve superior performance on specialized text processing tasks.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to present this information in an interview:</p>
<ol type="1">
<li><strong>Start with a High-Level Overview:</strong>
<ul>
<li>“Integrating domain-specific knowledge is essential for long-sequence models to perform well on specialized text like legal or medical documents. This involves adapting tokenization, attention, and fine-tuning.”</li>
</ul></li>
<li><strong>Tokenization Deep Dive:</strong>
<ul>
<li>“First, let’s discuss tokenization. Standard methods often fail with domain-specific jargon. We can customize tokenization in a few ways…”</li>
<li>“One approach is to enrich the vocabulary with frequent domain-specific terms. For example, adding legal citations like ‘18 U.S.C. § 2252’ or medical abbreviations like ‘MRI.’”</li>
<li>“Another is to use rule-based tokenizers to handle patterns specific to the domain, like chemical names or legal clauses.” Mention regular expressions briefly.</li>
<li>“For rare terms, we can use character-level tokenization to avoid out-of-vocabulary issues.”</li>
</ul></li>
<li><strong>Attention Mechanism Adaptation:</strong>
<ul>
<li>“Next, we can adapt the attention mechanisms. One method is to introduce domain-specific ‘global tokens’ that attend to all other tokens and vice versa. For instance, ‘contract’ in legal documents or ‘diagnosis’ in medical documents.”</li>
<li>“We can also implement structured attention patterns that reflect the hierarchical organization of documents, like prioritizing attention within clauses of a contract.”</li>
<li>“Even more advanced, knowledge graphs can be used to bias the attention mechanism based on relationships between concepts in the graph. You can briefly mention the equation if you think the interviewer would be interested and you can easily explain it, otherwise leave this out.”</li>
</ul></li>
<li><strong>Fine-Tuning Process:</strong>
<ul>
<li>“The third key component is fine-tuning. We start with a pre-trained language model, and then…”</li>
<li>“Ideally, we first <em>continue pre-training</em> on a large domain-specific corpus using masked language modeling to get the model familiar with the language. This loss function can be formalized by [explain the MLM loss]”</li>
<li>“After that, we fine-tune the model on a specific task with labeled data. The more labeled data, the better.”</li>
<li>“Data augmentation can also improve performance, by paraphrasing legal clauses or generating synthetic cases for example.”</li>
</ul></li>
<li><strong>Hybrid Approaches and Practical Considerations:</strong>
<ul>
<li>“Often, the best results come from combining these strategies. For example, using rule-based preprocessing before fine-tuning.”</li>
<li>“Finally, there are implementation considerations. Fine-tuning can be expensive, requiring techniques like gradient accumulation. Data privacy is also crucial, and we might use federated learning or differential privacy to protect sensitive information.”</li>
<li>“It’s also important to use appropriate evaluation metrics and monitor for overfitting.”</li>
</ul></li>
<li><strong>Closing:</strong>
<ul>
<li>“By carefully addressing tokenization, attention, fine-tuning, and these practical details, we can build long-sequence models that truly understand and excel at processing specialized texts.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow time for the interviewer to process the information.</li>
<li><strong>Use Examples:</strong> Concrete examples help illustrate abstract concepts.</li>
<li><strong>Check for Understanding:</strong> Periodically ask, “Does that make sense?” or “Are there any questions about that?”</li>
<li><strong>Be Flexible:</strong> Adapt your explanation based on the interviewer’s background and interest. If they seem particularly interested in one area, delve deeper. If they seem less familiar with a concept, simplify your explanation.</li>
<li><strong>Mathematical Notation:</strong> Only introduce mathematical notation if it enhances understanding and if you are comfortable explaining it thoroughly. Don’t assume the interviewer will be familiar with every detail.</li>
<li><strong>End with a Summary:</strong> Reiterate the key takeaways to reinforce your understanding.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>