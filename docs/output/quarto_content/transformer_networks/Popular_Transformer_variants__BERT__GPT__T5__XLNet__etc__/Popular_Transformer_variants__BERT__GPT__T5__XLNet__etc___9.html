<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>popular_transformer_variants__bert__gpt__t5__xlnet__etc___9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-10.-can-you-provide-an-analysis-of-the-trade-offs-between-model-size-performance-and-inference-speed-in-these-popular-transformer-variants-where-might-a-balance-be-struck-especially-in-resource-constrained-environments" class="level2">
<h2 class="anchored" data-anchor-id="question-10.-can-you-provide-an-analysis-of-the-trade-offs-between-model-size-performance-and-inference-speed-in-these-popular-transformer-variants-where-might-a-balance-be-struck-especially-in-resource-constrained-environments">Question: 10. Can you provide an analysis of the trade-offs between model size, performance, and inference speed in these popular Transformer variants? Where might a balance be struck, especially in resource-constrained environments?</h2>
<p><strong>Best Answer</strong></p>
<p>Transformer models have revolutionized Natural Language Processing, but their size often presents a challenge, especially in resource-constrained environments. The core trade-off is between model size, performance (accuracy, F1-score, etc.), and inference speed (latency, throughput). Different Transformer variants make different choices along this spectrum.</p>
<p><strong>1. Transformer Variants and their Characteristics:</strong></p>
<ul>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> BERT is primarily an encoder-only model. It excels at tasks requiring a deep understanding of context, like sentiment analysis, named entity recognition, and question answering. Variants include BERT-Base (110M parameters) and BERT-Large (340M parameters).
<ul>
<li><em>Size vs.&nbsp;Performance:</em> BERT-Large generally outperforms BERT-Base, but at a higher computational cost.</li>
<li><em>Inference Speed:</em> While powerful, BERT’s bidirectional attention can be computationally intensive.</li>
</ul></li>
<li><strong>GPT (Generative Pre-trained Transformer):</strong> GPT is a decoder-only model designed for text generation. It uses masked self-attention which enables to focus on the text tokens before the current token, ignoring the tokens after the current token in the input sequence. GPT models come in several sizes, such as GPT-2, GPT-3, and GPT-4 with each model significantly larger than its predecessor.
<ul>
<li><em>Size vs.&nbsp;Performance:</em> Larger GPT models (e.g., GPT-3 with 175B parameters) exhibit emergent capabilities, showing impressive few-shot and zero-shot learning.</li>
<li><em>Inference Speed:</em> Decoder-only models can be slower during generation since they produce text token by token.</li>
</ul></li>
<li><strong>T5 (Text-to-Text Transfer Transformer):</strong> T5 recasts all NLP tasks into a text-to-text format, using a single model for translation, summarization, question answering, etc. It comes in various sizes from T5-Small (60M) to T5-XXL (11B).
<ul>
<li><em>Size vs.&nbsp;Performance:</em> T5’s unified approach is beneficial, but larger variants are needed to achieve state-of-the-art performance across many tasks.</li>
<li><em>Inference Speed:</em> As an encoder-decoder model, T5’s inference speed depends on the sequence lengths of both input and output.</li>
</ul></li>
<li><strong>XLNet:</strong> XLNet is another encoder-only model that improves upon BERT by using a permutation language modeling objective. This enables XLNet to capture bidirectional contexts more effectively than BERT’s masked language modeling approach.
<ul>
<li><em>Size vs.&nbsp;Performance:</em> XLNet often outperforms BERT, particularly on longer sequences, but can be computationally more expensive to train.</li>
<li><em>Inference Speed:</em> Similar to BERT, XLNet’s inference speed is affected by the bidirectional attention mechanism.</li>
</ul></li>
</ul>
<p><strong>2. Trade-offs Analysis:</strong></p>
<p>The relationship between model size, performance, and inference speed isn’t linear.</p>
<ul>
<li><p><strong>Model Size and Performance:</strong> Generally, larger models have a greater capacity to learn complex patterns and achieve higher accuracy. The performance gain diminishes as the model size increases, exhibiting diminishing returns. Beyond a certain size, simply scaling up the model might not significantly improve performance and can even lead to overfitting if not properly regularized. This relationship can be empirically shown by plotting the model size (number of parameters) against the performance metric (e.g., accuracy on a benchmark dataset). The plot will typically show an increasing curve that flattens out.</p></li>
<li><p><strong>Model Size and Inference Speed:</strong> Inference speed is inversely proportional to model size. Larger models require more computational resources and time to process each input. The time complexity of the self-attention mechanism in Transformers is <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length. Therefore, longer sequences and larger models will drastically increase inference time. Consider the forward pass of a Transformer layer:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> are the query, key, and value matrices, respectively, and <span class="math inline">\(d_k\)</span> is the dimension of the key vectors. The matrix multiplication <span class="math inline">\(QK^T\)</span> is a major computational bottleneck, especially for long sequences and large models.</p></li>
<li><p><strong>Performance and Inference Speed:</strong> A direct trade-off often exists between performance and inference speed. To achieve higher performance, one might use a larger model or more complex architecture, which typically slows down inference. However, optimizations like quantization, pruning, and knowledge distillation can help to mitigate this trade-off.</p></li>
</ul>
<p><strong>3. Balancing Trade-offs in Resource-Constrained Environments:</strong></p>
<p>In resource-constrained environments (e.g., mobile devices, edge computing), striking the right balance is critical. Here are several strategies:</p>
<ul>
<li><p><strong>Model Distillation:</strong> Transfer knowledge from a large, high-performing teacher model to a smaller student model. The student model learns to mimic the teacher’s behavior, achieving comparable performance with a fraction of the parameters. Loss function for distillation often involves minimizing the difference between the teacher’s and student’s output probabilities or hidden states:</p>
<p><span class="math display">\[
L_{\text{distillation}} = \alpha L_{\text{student}} + (1 - \alpha) L_{\text{KL}}(P_{\text{teacher}} || P_{\text{student}})
\]</span></p>
<p>where <span class="math inline">\(L_{\text{student}}\)</span> is the standard loss function for the task, <span class="math inline">\(L_{\text{KL}}\)</span> is the Kullback-Leibler divergence between the teacher’s and student’s probability distributions, and <span class="math inline">\(\alpha\)</span> is a weighting factor.</p></li>
<li><p><strong>Model Pruning:</strong> Remove less important weights or neurons from the model. This reduces the model’s size and computational complexity without significantly impacting performance. Common pruning techniques include weight pruning (setting individual weights to zero) and neuron pruning (removing entire neurons).</p></li>
<li><p><strong>Quantization:</strong> Reduce the precision of the model’s weights and activations (e.g., from 32-bit floating point to 8-bit integers). This significantly reduces memory footprint and can speed up computation on hardware that supports low-precision arithmetic.</p></li>
<li><p><strong>Architecture Search (NAS):</strong> Neural Architecture Search automates the process of designing efficient neural network architectures. NAS algorithms can explore a wide range of architectural choices to find a model that achieves the desired performance with minimal resources.</p></li>
<li><p><strong>Efficient Attention Mechanisms:</strong> Explore alternatives to the standard self-attention mechanism, such as:</p>
<ul>
<li><em>Linear Attention:</em> Reduces the complexity from <span class="math inline">\(O(n^2)\)</span> to <span class="math inline">\(O(n)\)</span>.</li>
<li><em>Sparse Attention:</em> Attends to only a subset of the input sequence.</li>
</ul></li>
<li><p><strong>Prompt Engineering (for few-shot learning):</strong> Carefully crafting prompts for smaller models can significantly boost their performance. With a well-designed prompt, a smaller model can achieve performance comparable to a larger model with a naive prompt.</p></li>
<li><p><strong>Layer Reduction/Sharing:</strong> Reducing the number of layers or sharing parameters between layers reduces the model size. Techniques like parameter tying can be employed.</p></li>
<li><p><strong>Hardware Acceleration:</strong> Utilize specialized hardware like GPUs, TPUs, or dedicated AI accelerators to speed up inference. These accelerators are optimized for matrix multiplication and other operations common in Transformer models.</p></li>
</ul>
<p><strong>4. Real-World Considerations:</strong></p>
<ul>
<li><strong>Task Specificity:</strong> The optimal trade-off depends on the specific task. Some tasks may require high accuracy, while others prioritize low latency.</li>
<li><strong>Data Availability:</strong> If data is limited, smaller models with strong regularization might be preferable to prevent overfitting.</li>
<li><strong>Hardware Constraints:</strong> The available memory, compute power, and energy consumption of the target device must be considered.</li>
<li><strong>Regulatory Considerations</strong>: The size of the models may also come into play due to regulatory hurdles.</li>
<li><strong>Edge vs Cloud:</strong> The cost of running the models on the cloud must be balanced against edge deployments. The cloud deployments may seem less constrained but costs and latency may be higher.</li>
</ul>
<p><strong>Conclusion:</strong></p>
<p>Choosing the right Transformer variant and optimization techniques involves carefully balancing model size, performance, and inference speed. In resource-constrained environments, techniques like distillation, pruning, and quantization are essential for deploying these powerful models effectively. The best approach depends on the specific application, available resources, and desired performance characteristics.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to present this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Big Picture:</strong> “Transformer models offer a great deal of flexibility, but their size often creates a trade-off between performance, model size, and inference speed. The trick is finding the right balance, especially when resources are limited.”</p></li>
<li><p><strong>Introduce Key Transformer Variants:</strong> Briefly describe BERT, GPT, T5, and XLNet, highlighting their architectural differences (encoder-only, decoder-only, encoder-decoder) and typical applications. “For example, BERT excels at understanding context, GPT is great for generation, and T5 frames everything as a text-to-text problem.”</p></li>
<li><p><strong>Explain the Trade-offs:</strong></p>
<ul>
<li>“Generally, larger models perform better, but the relationship isn’t linear. We see diminishing returns as we scale up.”</li>
<li>“Inference speed is inversely proportional to model size. The self-attention mechanism’s <span class="math inline">\(O(n^2)\)</span> complexity becomes a bottleneck, especially for long sequences.” <em>Pause here. If the interviewer seems interested, elaborate on the formula and its implications. Otherwise, keep it brief.</em></li>
<li>“There’s often a direct trade-off between accuracy and latency, but we can use optimizations to mitigate this.”</li>
</ul></li>
<li><p><strong>Discuss Strategies for Resource-Constrained Environments:</strong></p>
<ul>
<li>“When resources are limited, techniques like model distillation, pruning, and quantization become essential.”</li>
<li>Explain each technique concisely. For distillation: “We train a smaller model to mimic a larger one. We can represent the distillation loss as <span class="math inline">\(&lt;equation&gt;L_{distillation} = \alpha L_{student} + (1 - \alpha) L_{KL}(P_{teacher} || P_{student})&lt;/equation&gt;\)</span>, where we balance the student’s original loss with the KL divergence between the teacher’s and student’s predictions.” <em>Avoid diving too deep into the equations unless prompted.</em></li>
<li>Mention prompt engineering and efficient attention mechanisms as alternative strategies.</li>
<li>You can give examples with actual numbers, like ‘quantization can reduce the model size by 4x’ or ‘distillation can produce a student model with 90% of the teacher accuracy but 50% of the number of parameters’.</li>
</ul></li>
<li><p><strong>Address Real-World Considerations:</strong></p>
<ul>
<li>“The optimal approach depends on the specific task, data availability, and hardware constraints.”</li>
<li>Give examples: “If we’re working with limited data, we might prefer a smaller, regularized model. If we need very low latency, we might sacrifice some accuracy for speed.”</li>
</ul></li>
<li><p><strong>Conclude with a Summary:</strong> “In short, the best way to deploy Transformer models in resource-constrained environments is to carefully analyze the trade-offs and apply the appropriate optimization techniques, tailored to the specific requirements of the application.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> This is a complex topic. Speak clearly and at a moderate pace.</li>
<li><strong>Use Signposting:</strong> Use phrases like “First,” “Second,” “In addition,” “However,” and “Therefore” to guide the interviewer through your explanation.</li>
<li><strong>Check for Understanding:</strong> After explaining a complex concept or equation, ask “Does that make sense?” or “Would you like me to elaborate on that?”</li>
<li><strong>Be Prepared to Dive Deeper:</strong> The interviewer may ask follow-up questions about specific techniques or trade-offs. Be ready to provide more details and examples.</li>
<li><strong>Show Enthusiasm:</strong> Demonstrate your passion for the field and your excitement about the potential of Transformer models.</li>
<li><strong>Relate to Experience:</strong> If you have experience applying these techniques in real-world projects, mention it briefly to add credibility to your answer. For instance: “In my previous role at X, we faced a similar challenge deploying BERT on mobile devices. We successfully used quantization and pruning to reduce the model size without significant performance degradation.”</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>