<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>popular_transformer_variants__bert__gpt__t5__xlnet__etc___8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-9.-discuss-the-role-of-transfer-learning-in-the-evolution-of-transformer-variants.-how-does-fine-tuning-a-pre-trained-model-differ-across-bert-gpt-t5-and-xlnet" class="level2">
<h2 class="anchored" data-anchor-id="question-9.-discuss-the-role-of-transfer-learning-in-the-evolution-of-transformer-variants.-how-does-fine-tuning-a-pre-trained-model-differ-across-bert-gpt-t5-and-xlnet">Question: 9. Discuss the role of transfer learning in the evolution of Transformer variants. How does fine-tuning a pre-trained model differ across BERT, GPT, T5, and XLNet?</h2>
<p><strong>Best Answer</strong></p>
<p>Transfer learning has been absolutely pivotal in the evolution and widespread adoption of Transformer models. The paradigm shift it introduced – from training models from scratch for each specific task to pre-training on massive datasets and then fine-tuning – drastically improved performance, reduced training time and data requirements, and democratized access to state-of-the-art NLP.</p>
<p><strong>The Role of Transfer Learning</strong></p>
<p>Prior to Transformers and transfer learning, training NLP models typically involved training task-specific models from scratch. This required large, labeled datasets for each individual task and significant computational resources. Transfer learning addressed these limitations by leveraging knowledge gained from pre-training on a massive, unlabeled dataset (e.g., the entirety of Wikipedia, books, and web pages). This pre-training phase allows the model to learn general language representations and then fine-tune those representations for specific downstream tasks.</p>
<p>The core idea is that the model learns a general “understanding” of language during pre-training. This understanding includes things like:</p>
<ul>
<li><strong>Word embeddings:</strong> Representing words as vectors in a high-dimensional space, capturing semantic relationships.</li>
<li><strong>Syntactic structures:</strong> Learning the grammatical rules and dependencies between words.</li>
<li><strong>World knowledge:</strong> Acquiring facts and relationships about the world.</li>
</ul>
<p>By pre-training on a large corpus, the model becomes initialized with useful parameters, allowing it to learn a downstream task with significantly less data and faster convergence. This is especially impactful for tasks with limited labeled data.</p>
<p><strong>Mathematical Foundation of Transfer Learning</strong></p>
<p>Let’s denote:</p>
<ul>
<li><span class="math inline">\(D_{source}\)</span>: The source dataset (e.g., a massive corpus of text for pre-training).</li>
<li><span class="math inline">\(T_{source}\)</span>: The task associated with the source dataset (e.g., masked language modeling).</li>
<li><span class="math inline">\(D_{target}\)</span>: The target dataset (e.g., a dataset for sentiment analysis).</li>
<li><span class="math inline">\(T_{target}\)</span>: The task associated with the target dataset (e.g., sentiment classification).</li>
<li><span class="math inline">\(\theta_{source}\)</span>: The parameters of the model trained on <span class="math inline">\(D_{source}\)</span> and <span class="math inline">\(T_{source}\)</span>.</li>
<li><span class="math inline">\(\theta_{target}\)</span>: The parameters of the model trained on <span class="math inline">\(D_{target}\)</span> and <span class="math inline">\(T_{target}\)</span>.</li>
</ul>
<p>The goal of transfer learning is to leverage <span class="math inline">\(\theta_{source}\)</span> to improve the performance of the model on <span class="math inline">\(D_{target}\)</span> and <span class="math inline">\(T_{target}\)</span>. Specifically, we initialize the model for <span class="math inline">\(T_{target}\)</span> with <span class="math inline">\(\theta_{source}\)</span> (or a subset of <span class="math inline">\(\theta_{source}\)</span>) and then fine-tune the model on <span class="math inline">\(D_{target}\)</span> and <span class="math inline">\(T_{target}\)</span> to obtain <span class="math inline">\(\theta_{target}\)</span>.</p>
<p>The key benefit can be viewed in terms of optimization. Instead of starting from a random initialization in the parameter space, we start from a point that’s already “close” to a good solution for related tasks. This can be seen as a form of regularization, guiding the model towards solutions that generalize well.</p>
<p><strong>Fine-tuning Differences Across BERT, GPT, T5, and XLNet</strong></p>
<p>While all these models leverage transfer learning, their architectures and pre-training objectives differ significantly, leading to variations in how they are fine-tuned:</p>
<ol type="1">
<li><p><strong>BERT (Bidirectional Encoder Representations from Transformers):</strong></p>
<ul>
<li><strong>Architecture:</strong> Encoder-only. BERT’s encoder-only architecture is designed to produce contextualized embeddings of the input sequence.</li>
<li><strong>Pre-training Objective:</strong> Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).</li>
<li><strong>Fine-tuning:</strong> BERT is versatile and can be fine-tuned for a wide range of tasks, including classification, question answering, and sequence tagging. Typically, a task-specific layer (e.g., a linear classifier) is added on top of the BERT encoder, and the entire model is fine-tuned end-to-end.</li>
<li><strong>Input Format:</strong> Since BERT is bidirectional, it requires the entire input sequence to be available at once. For tasks like classification, the input is typically the text to be classified, optionally concatenated with special tokens like <code>[CLS]</code> (for classification) and <code>[SEP]</code> (to separate sentences).</li>
<li><strong>Example:</strong> For sentiment classification, one could use the <code>[CLS]</code> token’s output representation as input to a linear layer for predicting the sentiment label.</li>
</ul></li>
<li><p><strong>GPT (Generative Pre-trained Transformer):</strong></p>
<ul>
<li><strong>Architecture:</strong> Decoder-only. GPT is designed for generative tasks.</li>
<li><strong>Pre-training Objective:</strong> Language Modeling (predicting the next word in a sequence).</li>
<li><strong>Fine-tuning:</strong> GPT is primarily fine-tuned for text generation and related tasks. Unlike BERT, GPT uses a causal (unidirectional) attention mask, meaning that each token can only attend to previous tokens in the sequence.</li>
<li><strong>Input Format:</strong> GPT uses prompts for fine-tuning. The input is a prompt, and the model generates the completion of the prompt.</li>
<li><strong>In-Context Learning:</strong> A key development with larger GPT models is their ability to perform few-shot or zero-shot learning, where the model can perform tasks with only a few examples or even without any explicit fine-tuning by providing the task instructions within the prompt.</li>
<li><strong>Example:</strong> For text summarization, the input could be the original text, and the model would generate the summary.</li>
</ul></li>
<li><p><strong>T5 (Text-to-Text Transfer Transformer):</strong></p>
<ul>
<li><strong>Architecture:</strong> Encoder-decoder.</li>
<li><strong>Pre-training Objective:</strong> A denoising objective where parts of the input text are masked, and the model is trained to reconstruct the original text.</li>
<li><strong>Fine-tuning:</strong> T5 frames <em>all</em> NLP tasks as text-to-text problems. This means that both the input and output are always text strings. This simplifies the fine-tuning process because only one architecture and training objective are needed for all tasks.</li>
<li><strong>Input Format:</strong> The input is a text string that describes the task and the input data. For example, for translation, the input could be “translate English to German: The cat sat on the mat.” The output would be the German translation.</li>
<li><strong>Example:</strong> For sentiment classification, the input could be “sentiment: This movie was great!” and the output would be “positive”.</li>
</ul></li>
<li><p><strong>XLNet (eXtreme Learning by re-arranging the Next position):</strong></p>
<ul>
<li><strong>Architecture:</strong> Uses a Transformer architecture but introduces permutation language modeling.</li>
<li><strong>Pre-training Objective:</strong> Permutation Language Modeling. XLNet overcomes BERT’s limitations by training on all possible permutations of the input sequence.</li>
<li><strong>Fine-tuning:</strong> Similar to BERT, XLNet can be fine-tuned for a wide variety of tasks. It often outperforms BERT, especially on tasks that require understanding long-range dependencies.</li>
<li><strong>Input Format:</strong> XLNet uses the same input format as BERT, with special tokens like <code>[CLS]</code> and <code>[SEP]</code>. However, the attention mechanism is more complex due to the permutation-based training.</li>
<li><strong>Example:</strong> For question answering, the input could be the question and the context passage, separated by <code>[SEP]</code>.</li>
</ul></li>
</ol>
<p><strong>Summary Table of Fine-Tuning Differences</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>BERT</th>
<th>GPT</th>
<th>T5</th>
<th>XLNet</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Architecture</td>
<td>Encoder-only</td>
<td>Decoder-only</td>
<td>Encoder-decoder</td>
<td>Transformer-based</td>
</tr>
<tr class="even">
<td>Pre-training</td>
<td>MLM, NSP</td>
<td>Language Modeling</td>
<td>Denoising</td>
<td>Permutation LM</td>
</tr>
<tr class="odd">
<td>Fine-tuning</td>
<td>Add task layer</td>
<td>Prompt-based</td>
<td>Text-to-text</td>
<td>Add task layer</td>
</tr>
<tr class="even">
<td>Input Format</td>
<td>[CLS] + text + [SEP]</td>
<td>Prompt</td>
<td>Task description + text</td>
<td>[CLS] + text + [SEP]</td>
</tr>
<tr class="odd">
<td>Task Versatility</td>
<td>High</td>
<td>Text Generation</td>
<td>Very High</td>
<td>High</td>
</tr>
</tbody>
</table>
<p><strong>Real-world considerations:</strong></p>
<ul>
<li><strong>Computational Resources:</strong> Fine-tuning large Transformer models can be computationally expensive, requiring GPUs or TPUs.</li>
<li><strong>Data Requirements:</strong> While transfer learning reduces the need for large labeled datasets, a sufficiently large and representative dataset is still important for fine-tuning.</li>
<li><strong>Hyperparameter Tuning:</strong> Fine-tuning requires careful selection of hyperparameters, such as learning rate, batch size, and number of epochs.</li>
<li><strong>Catastrophic Forgetting:</strong> Fine-tuning can sometimes lead to catastrophic forgetting of the knowledge learned during pre-training. Techniques like knowledge distillation and regularization can help mitigate this issue.</li>
<li><strong>Prompt Engineering:</strong> For GPT models, the choice of prompt can have a significant impact on performance. Prompt engineering is an active area of research.</li>
</ul>
<p>In conclusion, transfer learning has been revolutionary for NLP, and the Transformer architecture has been a key enabler. Understanding the nuances of fine-tuning different Transformer variants is crucial for achieving optimal performance on specific tasks.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this to an interviewer:</p>
<ol type="1">
<li><p><strong>Start with the Big Picture (30 seconds):</strong></p>
<ul>
<li>“Transfer learning has fundamentally changed NLP. It’s the idea of pre-training on a massive dataset and then adapting that knowledge to specific tasks. This has led to significant improvements in performance and efficiency.”</li>
</ul></li>
<li><p><strong>Explain the Core Concept (1 minute):</strong></p>
<ul>
<li>“Before transfer learning, we’d train models from scratch for each task. This was data-hungry and computationally expensive. Transfer learning allows us to leverage the general language understanding learned during pre-training to improve performance on downstream tasks with limited labeled data.”</li>
<li>You can mention word embeddings, syntactic structures, and world knowledge as examples of what the model learns during pre-training.</li>
</ul></li>
<li><p><strong>Highlight the Mathematical Foundation (1 minute, optional):</strong></p>
<ul>
<li>“The goal is to use the parameters learned during pre-training, <span class="math inline">\(\theta_{source}\)</span>, to initialize the model for the target task and then fine-tune it to obtain <span class="math inline">\(\theta_{target}\)</span>. This is a key benefit in terms of optimization because instead of starting from a random initialization in the parameter space, we start from a point that’s already ‘close’ to a good solution.”</li>
<li><em>Note:</em> Only include the formulas if the interviewer seems interested in more technical depth. You can gauge this by their reactions.</li>
</ul></li>
<li><p><strong>Discuss the Models (3-4 minutes):</strong></p>
<ul>
<li>“Now, let’s talk about how fine-tuning differs across BERT, GPT, T5, and XLNet. The main differences stem from their architectures and pre-training objectives.”</li>
<li><strong>BERT:</strong> “BERT is encoder-only and pre-trained with masked language modeling and next sentence prediction. It’s very versatile. For fine-tuning, we typically add a task-specific layer on top and fine-tune the entire model. Input formatting is key, with <code>[CLS]</code> and <code>[SEP]</code> tokens used to mark the beginning and separation of sentences.”</li>
<li><strong>GPT:</strong> “GPT is decoder-only and focused on language modeling. It’s primarily fine-tuned for text generation. A crucial aspect is prompt engineering, where we craft specific prompts to guide the model’s generation. Newer, larger GPT models can even do few-shot or zero-shot learning by providing the task instructions within the prompt.”</li>
<li><strong>T5:</strong> “T5 is an encoder-decoder model that frames <em>all</em> NLP tasks as text-to-text problems. This simplifies fine-tuning because you use the same architecture and training objective for every task. The input is a text string that describes the task and the input data.”</li>
<li><strong>XLNet:</strong> “XLNet is another powerful model that uses permutation language modeling to overcome some of BERT’s limitations, particularly for long-range dependencies. Fine-tuning is similar to BERT but the attention mechanism is more complex.”</li>
<li><em>Tip:</em> For each model, focus on <em>why</em> their architecture and pre-training objective lead to specific fine-tuning approaches.</li>
</ul></li>
<li><p><strong>Summarize with a Table (Optional, 30 seconds):</strong></p>
<ul>
<li>Consider mentally going through the summary table in the answer to highlight the key differences if time and the interviewer’s interest permit.</li>
</ul></li>
<li><p><strong>Address Real-World Considerations (1 minute):</strong></p>
<ul>
<li>“Finally, it’s important to consider real-world factors like computational resources, data requirements, hyperparameter tuning, and potential issues like catastrophic forgetting. Prompt engineering is also a crucial consideration for GPT models.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use clear and concise language:</strong> Avoid jargon where possible.</li>
<li><strong>Emphasize the “why”:</strong> Focus on the underlying reasons and motivations behind the different approaches.</li>
<li><strong>Check for understanding:</strong> Pause occasionally to ask if the interviewer has any questions.</li>
<li><strong>Tailor your response:</strong> Pay attention to the interviewer’s background and adjust your level of detail accordingly. If they ask for more details on a specific area, be prepared to dive deeper.</li>
<li><strong>Be enthusiastic:</strong> Show your passion for the subject matter!</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>