<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>popular_transformer_variants__bert__gpt__t5__xlnet__etc___1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-2.-how-do-the-pre-training-objectives-differ-between-bert-gpt-and-xlnet-and-what-are-the-implications-of-these-differences-for-downstream-tasks" class="level2">
<h2 class="anchored" data-anchor-id="question-2.-how-do-the-pre-training-objectives-differ-between-bert-gpt-and-xlnet-and-what-are-the-implications-of-these-differences-for-downstream-tasks">Question: 2. How do the pre-training objectives differ between BERT, GPT, and XLNet, and what are the implications of these differences for downstream tasks?</h2>
<p><strong>Best Answer</strong></p>
<p>Let’s delve into the pre-training objectives of BERT, GPT, and XLNet and their implications. These models represent significant advancements in Natural Language Processing (NLP), each leveraging the Transformer architecture but employing distinct pre-training strategies.</p>
<p><strong>1. BERT (Bidirectional Encoder Representations from Transformers)</strong></p>
<ul>
<li><p><strong>Pre-training Objectives:</strong> BERT employs two primary pre-training objectives:</p>
<ul>
<li><p><strong>Masked Language Modeling (MLM):</strong> A percentage (typically 15%) of the input tokens are randomly masked. The model’s objective is to predict the original tokens based on the surrounding unmasked tokens. This can be represented mathematically as:</p>
<p><span class="math display">\[
\mathcal{L}_{MLM} = - \mathbb{E}_{x \sim D} \sum_{i \in M} \log P(x_i | x_{\setminus i})
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x\)</span> is the input sequence from dataset <span class="math inline">\(D\)</span>.</li>
<li><span class="math inline">\(M\)</span> is the set of masked token indices.</li>
<li><span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>-th token.</li>
<li><span class="math inline">\(x_{\setminus i}\)</span> denotes the input sequence without the i-th token.</li>
<li><span class="math inline">\(P(x_i | x_{\setminus i})\)</span> is the probability of predicting token <span class="math inline">\(x_i\)</span> given the unmasked context.</li>
</ul></li>
<li><p><strong>Next Sentence Prediction (NSP):</strong> The model is given pairs of sentences (A, B) and tasked with predicting whether sentence B is the actual next sentence following sentence A in the original corpus. This is a binary classification task.</p>
<p><span class="math display">\[
\mathcal{L}_{NSP} = - \mathbb{E}_{(A,B) \sim D} \left[ y \log P(B \text{ is next } | A) + (1-y) \log (1 - P(B \text{ is next } | A)) \right]
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(y = 1\)</span> if B is the next sentence following A, and 0 otherwise.</li>
<li><span class="math inline">\(P(B \text{ is next } | A)\)</span> is the probability that B is the next sentence given A.</li>
<li><span class="math inline">\(D\)</span> is the data consisting of sentence pairs.</li>
</ul></li>
<li><p><strong>Overall BERT objective:</strong> <span class="math inline">\(\mathcal{L} = \mathcal{L}_{MLM} + \mathcal{L}_{NSP}\)</span></p></li>
</ul></li>
<li><p><strong>Implications:</strong></p>
<ul>
<li><strong>Bidirectional Context:</strong> MLM allows BERT to learn representations that consider both left and right context, which is crucial for understanding nuanced relationships between words.</li>
<li><strong>Sentence-Level Understanding:</strong> NSP aims to improve the model’s ability to understand relationships between sentences, benefiting tasks like question answering and natural language inference.</li>
<li><strong>Drawbacks of NSP:</strong> Later research has suggested that NSP might not be as effective as initially believed and can sometimes hinder performance. Many subsequent BERT variants have removed NSP or replaced it with more effective inter-sentence objectives.</li>
</ul></li>
</ul>
<p><strong>2. GPT (Generative Pre-trained Transformer)</strong></p>
<ul>
<li><p><strong>Pre-training Objective:</strong> GPT uses a unidirectional (left-to-right) language modeling objective. The model predicts the next token in a sequence given all preceding tokens. This is autoregressive language modeling.</p>
<p><span class="math display">\[
\mathcal{L}_{LM} = - \mathbb{E}_{x \sim D} \sum_{i=1}^{n} \log P(x_i | x_1, x_2, ..., x_{i-1})
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x = (x_1, x_2, ..., x_n)\)</span> is a sequence of tokens from dataset <span class="math inline">\(D\)</span>.</li>
<li><span class="math inline">\(P(x_i | x_1, x_2, ..., x_{i-1})\)</span> is the probability of predicting the <span class="math inline">\(i\)</span>-th token given the previous tokens.</li>
</ul></li>
<li><p><strong>Implications:</strong></p>
<ul>
<li><strong>Text Generation:</strong> GPT excels at text generation tasks because it is trained to predict the next word in a sequence. This makes it naturally suited for tasks like creative writing, chatbots, and code generation.</li>
<li><strong>Unidirectional Context:</strong> The unidirectional nature limits its ability to capture bidirectional context, which can be a disadvantage for tasks requiring a deep understanding of the entire input sequence.</li>
<li><strong>Fine-tuning Adaptation:</strong> Because it is designed as an autoregressive model, fine-tuning to different tasks requires adaptation in the input and output layers to function effectively in a variety of text generation environments.</li>
</ul></li>
</ul>
<p><strong>3. XLNet</strong></p>
<ul>
<li><p><strong>Pre-training Objective:</strong> XLNet aims to combine the benefits of both autoregressive language modeling (like GPT) and bidirectional context (like BERT) without using masking. It achieves this through <em>permutation language modeling</em>.</p>
<ul>
<li><p><strong>Permutation Language Modeling:</strong> Instead of masking tokens, XLNet considers all possible permutations of the input sequence. For each permutation, the model predicts the next token in the sequence according to the permutation order. Let <span class="math inline">\(Z_t\)</span> be the t-th element in the permutation. The objective is to maximize the log-likelihood over all possible permutations <span class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}_{PLM} = \mathbb{E}_{z \sim \mathcal{Z}} \left[ \sum_{t=1}^{n} \log P(x_{Z_t} | x_{Z_1}, ..., x_{Z_{t-1}}) \right]
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\mathcal{Z}\)</span> is the set of all possible permutations of the indices <span class="math inline">\(\{1, 2, ..., n\}\)</span>.</li>
<li><span class="math inline">\(z\)</span> is a specific permutation.</li>
<li><span class="math inline">\(x_{Z_t}\)</span> is the token at the <span class="math inline">\(t\)</span>-th position in the permuted sequence.</li>
<li><span class="math inline">\(P(x_{Z_t} | x_{Z_1}, ..., x_{Z_{t-1}})\)</span> is the conditional probability of predicting <span class="math inline">\(x_{Z_t}\)</span> given the previous tokens in the permuted order.</li>
</ul></li>
<li><p><strong>Two-Stream Self-Attention:</strong> XLNet uses a two-stream self-attention mechanism to avoid the target token “seeing itself” during training, which would trivialize the prediction task. The content stream (<span class="math inline">\(h\)</span>) is standard self-attention, and the query stream (<span class="math inline">\(g\)</span>) only has access to positional information.</p></li>
</ul></li>
<li><p><strong>Implications:</strong></p>
<ul>
<li><strong>Bidirectional Context without Masking:</strong> By considering all permutations, XLNet can capture bidirectional context without the need for masking, addressing the pretrain-finetune discrepancy in BERT (where masking is only present during pre-training).</li>
<li><strong>Improved Performance:</strong> XLNet often achieves better performance than BERT on various downstream tasks, especially those requiring deep contextual understanding.</li>
<li><strong>Computational Complexity:</strong> The permutation process can be computationally expensive, especially for long sequences.</li>
<li><strong>Ability to handle longer sequences:</strong> The memory requirements can be very demanding for long inputs, especially when using long input documents.</li>
</ul></li>
</ul>
<p><strong>Summary Table:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>BERT</th>
<th>GPT</th>
<th>XLNet</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pre-training Objective</td>
<td>MLM + NSP</td>
<td>Autoregressive Language Modeling</td>
<td>Permutation Language Modeling</td>
</tr>
<tr class="even">
<td>Context</td>
<td>Bidirectional</td>
<td>Unidirectional</td>
<td>Bidirectional</td>
</tr>
<tr class="odd">
<td>Advantages</td>
<td>Strong contextual understanding</td>
<td>Excellent for text generation</td>
<td>Captures bidirectional context effectively</td>
</tr>
<tr class="even">
<td>Disadvantages</td>
<td>Pretrain-finetune discrepancy (masking)</td>
<td>Limited bidirectional context</td>
<td>Computational complexity</td>
</tr>
<tr class="odd">
<td>Best Suited For</td>
<td>Tasks requiring understanding entire context (QA, NLI)</td>
<td>Text generation, language modeling</td>
<td>Tasks needing deep context understanding</td>
</tr>
</tbody>
</table>
<p><strong>Real-World Considerations:</strong></p>
<ul>
<li><strong>Compute Resources:</strong> Training these models requires significant computational resources. Cloud-based platforms (AWS, GCP, Azure) offer specialized hardware (TPUs, GPUs) that can accelerate training.</li>
<li><strong>Data Requirements:</strong> Large datasets are crucial for effective pre-training. Using publicly available datasets (e.g., BookCorpus, Wikipedia) or creating domain-specific datasets is essential.</li>
<li><strong>Fine-tuning Strategies:</strong> Adapting these models to specific downstream tasks often requires careful fine-tuning. Techniques like learning rate scheduling, early stopping, and regularization can improve performance.</li>
<li><strong>Model Size vs.&nbsp;Performance:</strong> There’s a trend toward larger models (e.g., GPT-3, PaLM). While larger models can achieve better performance, they also require more resources and can be more challenging to deploy. Strategies like model distillation can help to reduce the size of large models without sacrificing too much performance.</li>
</ul>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested approach to narrating this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a high-level overview:</strong> “BERT, GPT, and XLNet are all Transformer-based models that have revolutionized NLP. They differ significantly in their pre-training objectives, which influences their strengths and weaknesses on downstream tasks.”</p></li>
<li><p><strong>Discuss BERT:</strong> “BERT uses Masked Language Modeling and Next Sentence Prediction. In MLM, we randomly mask some tokens and train the model to predict them based on the surrounding context. Mathematically, we’re minimizing the negative log-likelihood of the masked tokens given the unmasked context…” ( Briefly explain the formula, emphasizing the key components: masked tokens, conditional probability, and optimization objective. You can write the formula down if a whiteboard is available. ) “…The NSP task helps BERT learn relationships between sentences. This bidirectional approach gives BERT a strong understanding of context.” Mention the potential drawbacks of NSP and the impact of subsequent research</p></li>
<li><p><strong>Transition to GPT:</strong> “GPT, on the other hand, uses a unidirectional language modeling objective. It predicts the next word in a sequence given the previous words. This makes it particularly good at text generation. Again, the objective function is defined as the negative log-likelihood of predicting a target word given the previous words….” ( Briefly explain the formula, emphasizing the conditional probability in a left-to-right context. ) “…However, its unidirectional nature can limit its ability to capture bidirectional context.”</p></li>
<li><p><strong>Introduce XLNet:</strong> “XLNet attempts to combine the benefits of both BERT and GPT by using permutation language modeling. Instead of masking, it considers all possible permutations of the input sequence and predicts tokens based on the permuted order. The objective function here is to maximize the log-likelihood of each token, considering all possible permutation of the input tokens…”(Again, briefly explain the formula, highlighting the consideration of all permutations and the conditional probability given the permuted context.) “…This allows it to capture bidirectional context without the pretrain-finetune discrepancy introduced by masking. XLNet also employs a two-stream self-attention mechanism. However, the permutation process adds computational complexity.”</p></li>
<li><p><strong>Summarize and compare:</strong> “In summary, BERT is strong in contextual understanding due to its bidirectional approach, GPT excels at text generation due to its autoregressive nature, and XLNet aims to combine the best of both worlds with permutation language modeling.” (Refer to the summary table mentally to compare the models.)</p></li>
<li><p><strong>Discuss Real-world considerations:</strong> “When working with these models, factors like compute resources, data requirements, and fine-tuning strategies are essential. Larger models generally achieve better performance but require more resources. Techniques like model distillation can help address this.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Give the interviewer time to absorb the information.</li>
<li><strong>Use visual aids:</strong> If a whiteboard is available, use it to draw diagrams or write down key formulas.</li>
<li><strong>Check for understanding:</strong> Periodically ask the interviewer if they have any questions or if you should elaborate on anything.</li>
<li><strong>Focus on the “why”:</strong> Don’t just state facts. Explain the reasoning behind the different design choices and their implications.</li>
<li><strong>Relate to practical applications:</strong> Provide real-world examples to illustrate the concepts.</li>
<li><strong>Handle mathematical notations gracefully:</strong> If you’re discussing formulas, explain the notation clearly and concisely. Avoid getting bogged down in unnecessary mathematical details. Focus on conveying the intuition behind the equations.</li>
<li><strong>Demonstrate a balance of breadth and depth:</strong> Showcase both your broad understanding of the field and your deep knowledge of specific concepts.</li>
</ul>
<p>By following these guidelines, you can deliver a comprehensive and engaging answer that demonstrates your expertise in NLP and deep learning.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>