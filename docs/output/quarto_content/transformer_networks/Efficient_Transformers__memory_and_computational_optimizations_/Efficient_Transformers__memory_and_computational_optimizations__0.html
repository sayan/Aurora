<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>efficient_transformers__memory_and_computational_optimizations__0</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-can-you-explain-the-key-differences-between-standard-transformers-and-efficient-transformers-particularly-in-terms-of-their-memory-and-computational-complexities" class="level2">
<h2 class="anchored" data-anchor-id="question-can-you-explain-the-key-differences-between-standard-transformers-and-efficient-transformers-particularly-in-terms-of-their-memory-and-computational-complexities">Question: Can you explain the key differences between standard Transformers and Efficient Transformers, particularly in terms of their memory and computational complexities?</h2>
<p><strong>Best Answer</strong></p>
<p>The standard Transformer architecture, introduced in the “Attention is All You Need” paper, revolutionized sequence modeling due to its reliance on the self-attention mechanism. However, its computational and memory complexities pose significant challenges when dealing with long sequences. Efficient Transformers address these limitations by employing various techniques to reduce these complexities, typically trading off some expressiveness for improved efficiency.</p>
<p><strong>Standard Transformers: Bottlenecks and Complexities</strong></p>
<p>The core bottleneck lies within the self-attention mechanism. Given a sequence of length <span class="math inline">\(n\)</span>, the self-attention mechanism involves computing attention weights between every pair of tokens. Specifically, for each token, we compute a query <span class="math inline">\(Q\)</span>, a key <span class="math inline">\(K\)</span>, and a value <span class="math inline">\(V\)</span>, where <span class="math inline">\(Q, K, V \in \mathbb{R}^{n \times d_k}\)</span> and <span class="math inline">\(d_k\)</span> is the dimension of the key/query vectors. The attention weights are computed as follows:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<ol type="1">
<li><p><strong>Computational Complexity:</strong> The computation of <span class="math inline">\(QK^T\)</span> involves a matrix multiplication of size <span class="math inline">\((n \times d_k) \times (d_k \times n)\)</span>, resulting in an <span class="math inline">\((n \times n)\)</span> attention matrix. The complexity of this operation is <span class="math inline">\(O(n^2d_k)\)</span>. The subsequent multiplication with <span class="math inline">\(V\)</span> has a complexity of <span class="math inline">\(O(n^2d_k)\)</span>. Thus, the overall computational complexity of the self-attention layer is <span class="math inline">\(O(n^2d_k)\)</span>. With multiple attention heads, this becomes <span class="math inline">\(O(n^2d)\)</span>, where <span class="math inline">\(d\)</span> is the model dimension.</p></li>
<li><p><strong>Memory Complexity:</strong> Storing the attention matrix <span class="math inline">\(QK^T\)</span> requires <span class="math inline">\(O(n^2)\)</span> memory. This quadratic memory requirement becomes a major bottleneck when dealing with long sequences.</p></li>
</ol>
<p><strong>Efficient Transformers: Strategies and Examples</strong></p>
<p>Efficient Transformers aim to reduce the quadratic complexity of standard Transformers by employing various approximation and sparsity techniques. Here are some key strategies and examples:</p>
<ol type="1">
<li><p><strong>Sparse Attention:</strong> Instead of computing attention between every pair of tokens, sparse attention mechanisms restrict attention to a subset of tokens. This can be achieved through:</p>
<ul>
<li><p><strong>Fixed Patterns:</strong> Attention is restricted to a fixed set of positions, such as neighboring tokens or tokens at specific intervals. Examples include:</p>
<ul>
<li><strong>Longformer:</strong> Introduces a combination of sliding window attention, dilated sliding window attention, and global attention for specific tokens. This reduces the complexity to <span class="math inline">\(O(n w)\)</span>, where <span class="math inline">\(w\)</span> is the window size, which is typically much smaller than <span class="math inline">\(n\)</span>.</li>
</ul></li>
<li><p><strong>Learnable Patterns:</strong> Attention patterns are learned during training.</p>
<ul>
<li><strong>Reformer:</strong> Employs Locality Sensitive Hashing (LSH) to group similar tokens together, allowing attention to be computed only within these groups. This can achieve a complexity close to <span class="math inline">\(O(n \log n)\)</span>.</li>
</ul></li>
</ul></li>
<li><p><strong>Low-Rank Approximations:</strong> Approximate the attention matrix <span class="math inline">\(QK^T\)</span> using a low-rank matrix factorization.</p>
<ul>
<li><p><strong>Linformer:</strong> Projects the key and value matrices to a lower dimension <span class="math inline">\(k\)</span> using linear projections <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span>, such that <span class="math inline">\(E, F \in \mathbb{R}^{k \times n}\)</span>. The attention mechanism becomes:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax(\frac{Q(KE)^T}{\sqrt{d_k}})VF
\]</span></p>
<p>The complexity becomes <span class="math inline">\(O(n k d_k)\)</span>, where <span class="math inline">\(k\)</span> is the reduced dimension. If <span class="math inline">\(k &lt; n\)</span>, this offers a reduction in computational cost.</p></li>
</ul></li>
<li><p><strong>Kernel-Based Methods:</strong> Reformulate the attention mechanism using kernel functions.</p>
<ul>
<li><p><strong>Performer:</strong> Uses Fast Attention Via positive Orthogonal Random features approach (FAVOR+) to approximate the attention mechanism with linear time and space complexity. It approximates the softmax kernel with random feature maps, allowing for efficient computation without explicitly computing the <span class="math inline">\(n \times n\)</span> attention matrix. The crucial trick is based on kernel decomposition and associativity.</p>
<p>Given a kernel <span class="math inline">\(K(q,k)\)</span> we can write:</p>
<p><span class="math display">\[
K(q,k) = \mathbb{E}_{\phi}[\phi(q)\phi(k)^T]
\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is a feature map. Performer uses this kernel approximation to reduce the complexity of the attention mechanism.</p></li>
</ul></li>
<li><p><strong>Recurrence:</strong> Utilizing recurrent mechanisms to process sequences sequentially.</p>
<ul>
<li><strong>Transformer-XL:</strong> Introduces recurrence to Transformers, allowing information to propagate across segments of the sequence. It employs a segment-level recurrence mechanism, where hidden states from previous segments are reused as memory for the current segment. This allows for modeling longer dependencies.</li>
</ul></li>
</ol>
<p><strong>Trade-offs</strong></p>
<p>Efficient Transformers offer significant improvements in terms of memory and computational efficiency. However, these improvements often come at the cost of:</p>
<ul>
<li><strong>Reduced Expressiveness:</strong> Approximations and sparsity techniques may limit the model’s ability to capture complex dependencies in the data.</li>
<li><strong>Increased Complexity:</strong> Implementing and tuning Efficient Transformer architectures can be more complex than standard Transformers. Choosing the appropriate technique depends on the specific task and the characteristics of the data.</li>
<li><strong>Hyperparameter Sensitivity:</strong> Many Efficient Transformer architectures introduce new hyperparameters that need to be carefully tuned. For instance, in Longformer, the window size needs to be selected appropriately.</li>
</ul>
<p><strong>Conclusion</strong></p>
<p>Efficient Transformers offer various strategies to mitigate the quadratic complexity of standard Transformers, enabling the processing of longer sequences. The choice of which Efficient Transformer architecture to use depends on the specific application, the available computational resources, and the desired trade-off between accuracy and efficiency. The field is rapidly evolving, with new techniques continuously being developed.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to present this information in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Problem:</strong></p>
<ul>
<li>Begin by stating the limitations of standard Transformers: “The standard Transformer architecture, while powerful, suffers from quadratic computational and memory complexity with respect to the sequence length. This makes it challenging to apply to long sequences.”</li>
<li>Clearly state the core issue: “The primary bottleneck is the self-attention mechanism, which requires computing interactions between every pair of tokens.”</li>
</ul></li>
<li><p><strong>Explain Standard Transformer Complexity:</strong></p>
<ul>
<li>“In a standard Transformer, the self-attention mechanism computes attention weights using the formula: <span class="math inline">\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span>.”</li>
<li>“The matrix multiplication <span class="math inline">\(QK^T\)</span> is the source of the <span class="math inline">\(O(n^2d_k)\)</span> computational complexity and the <span class="math inline">\(O(n^2)\)</span> memory complexity, where <span class="math inline">\(n\)</span> is the sequence length and <span class="math inline">\(d_k\)</span> is the dimension of the key/query vectors.”</li>
<li><strong>Pause:</strong> Check if the interviewer is following along. You can ask, “Does that make sense so far?”</li>
</ul></li>
<li><p><strong>Introduce Efficient Transformers:</strong></p>
<ul>
<li>“To address these limitations, Efficient Transformers employ various techniques to reduce the quadratic complexity. These techniques often involve trade-offs between computational efficiency and model expressiveness.”</li>
</ul></li>
<li><p><strong>Discuss Key Strategies and Examples:</strong></p>
<ul>
<li><strong>Sparse Attention:</strong> “One common strategy is sparse attention, where attention is restricted to a subset of tokens. For example, Longformer uses a combination of sliding window attention, dilated sliding window attention, and global attention, which reduces the complexity to <span class="math inline">\(O(n w)\)</span>, where <span class="math inline">\(w\)</span> is the window size.”</li>
<li><strong>Low-Rank Approximations:</strong> “Another approach involves low-rank approximations. Linformer projects the key and value matrices to a lower dimension using linear projections. The attention mechanism becomes <span class="math inline">\(Attention(Q, K, V) = softmax(\frac{Q(KE)^T}{\sqrt{d_k}})VF\)</span>, reducing the complexity to <span class="math inline">\(O(n k d_k)\)</span>.”</li>
<li><strong>Kernel-Based Methods:</strong> “Performer uses kernel-based methods and FAVOR+ to approximate the attention mechanism. It uses random feature maps, allowing for efficient computation without explicitly computing the full attention matrix, achieving a complexity close to linear.”</li>
<li><strong>Recurrence:</strong> “Transformer-XL introduces recurrence, allowing information to propagate across segments of the sequence. This helps in capturing longer dependencies.”</li>
<li><strong>Note on presenting equations:</strong> When presenting equations, focus on the intuition rather than getting bogged down in the minutiae. For instance, when discussing Linformer, say something like, “Linformer uses linear projections to reduce the dimensionality of the key and value matrices. This reduces the computational complexity because we’re now working with smaller matrices.”</li>
</ul></li>
<li><p><strong>Explain Trade-offs:</strong></p>
<ul>
<li>“It’s important to note that these efficiency gains often come at the cost of reduced expressiveness or increased implementation complexity. Approximations can limit the model’s ability to capture fine-grained dependencies.”</li>
</ul></li>
<li><p><strong>Conclude:</strong></p>
<ul>
<li>“In summary, Efficient Transformers offer various strategies to mitigate the quadratic complexity of standard Transformers. The choice of which architecture to use depends on the specific application and the desired trade-off between accuracy and efficiency.”</li>
<li>“The field is continuously evolving, and new techniques are constantly being developed.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you’re in a virtual interview, consider sharing your screen and drawing diagrams to illustrate the concepts.</li>
<li><strong>Check for Understanding:</strong> Periodically ask the interviewer if they have any questions or if you should elaborate on anything.</li>
<li><strong>Focus on Intuition:</strong> When discussing mathematical details, focus on the intuition behind the equations rather than getting bogged down in the minutiae.</li>
<li><strong>Stay High-Level:</strong> Avoid going into excessive detail unless the interviewer specifically asks for it.</li>
<li><strong>Be Confident:</strong> Project confidence in your knowledge of the topic.</li>
<li><strong>Be Ready to Adapt:</strong> If the interviewer steers the conversation in a different direction, be prepared to adjust your answer accordingly.</li>
</ul>
<p>By following these guidelines, you can effectively explain the key differences between standard and Efficient Transformers in a clear, concise, and informative manner, showcasing your senior-level expertise.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>