<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>efficient_transformers__memory_and_computational_optimizations__9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-can-you-mathematically-derive-or-describe-the-complexity-analysis-time-and-memory-of-a-kernel-based-attention-mechanism-compared-to-standard-quadratic-attention" class="level2">
<h2 class="anchored" data-anchor-id="question-can-you-mathematically-derive-or-describe-the-complexity-analysis-time-and-memory-of-a-kernel-based-attention-mechanism-compared-to-standard-quadratic-attention">Question: Can you mathematically derive or describe the complexity analysis (time and memory) of a kernel-based attention mechanism compared to standard quadratic attention?</h2>
<p><strong>Best Answer</strong></p>
<p>Let’s delve into the complexity analysis of standard attention and kernel-based attention mechanisms.</p>
<p><strong>1. Standard (Quadratic) Attention</strong></p>
<p>The standard attention mechanism, as introduced in the original Transformer paper, involves computing attention weights based on the following formula:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<p>where: * <span class="math inline">\(Q\)</span> is the query matrix of size <span class="math inline">\((n, d_k)\)</span> * <span class="math inline">\(K\)</span> is the key matrix of size <span class="math inline">\((n, d_k)\)</span> * <span class="math inline">\(V\)</span> is the value matrix of size <span class="math inline">\((n, d_v)\)</span> * <span class="math inline">\(n\)</span> is the sequence length * <span class="math inline">\(d_k\)</span> is the dimension of the keys/queries * <span class="math inline">\(d_v\)</span> is the dimension of the values</p>
<p>Let’s break down the computational complexity:</p>
<ul>
<li><strong><span class="math inline">\(QK^T\)</span></strong>: This matrix multiplication is of size <span class="math inline">\((n, d_k) \times (d_k, n)\)</span>, resulting in a <span class="math inline">\((n, n)\)</span> matrix. The computational complexity is <span class="math inline">\(O(n^2d_k)\)</span>.</li>
<li><strong><span class="math inline">\(softmax(\frac{QK^T}{\sqrt{d_k}})\)</span></strong>: The softmax operation is applied row-wise to the <span class="math inline">\((n, n)\)</span> matrix. The computational complexity is <span class="math inline">\(O(n^2)\)</span>. Note that dividing by <span class="math inline">\(\sqrt{d_k}\)</span> is simply elementwise division of an <span class="math inline">\((n,n)\)</span> matrix, and thus the computational complexity is <span class="math inline">\(O(n^2)\)</span>.</li>
<li><strong><span class="math inline">\(softmax(…)V\)</span></strong>: This matrix multiplication is of size <span class="math inline">\((n, n) \times (n, d_v)\)</span>, resulting in a <span class="math inline">\((n, d_v)\)</span> matrix. The computational complexity is <span class="math inline">\(O(n^2d_v)\)</span>.</li>
</ul>
<p>Therefore, the overall time complexity of standard attention is <span class="math inline">\(O(n^2d_k) + O(n^2) + O(n^2d_v)\)</span>. Since <span class="math inline">\(d_k\)</span> and <span class="math inline">\(d_v\)</span> are often considered constants (hyperparameters of the model), we can simplify this to <span class="math inline">\(O(n^2)\)</span>.</p>
<p>The memory complexity is dominated by storing the <span class="math inline">\((n, n)\)</span> attention matrix <span class="math inline">\(QK^T\)</span>, resulting in <span class="math inline">\(O(n^2)\)</span> memory usage. Storing <span class="math inline">\(Q, K, V\)</span> requires <span class="math inline">\(O(nd_k)\)</span> and <span class="math inline">\(O(nd_v)\)</span> space, which is less asymptotically complex than <span class="math inline">\(O(n^2)\)</span>.</p>
<p><strong>2. Kernel-Based Attention</strong></p>
<p>Kernel-based attention aims to reduce the quadratic complexity by approximating the attention mechanism using kernel functions and their associated feature maps. The core idea is to replace the dot product <span class="math inline">\(Q K^T\)</span> with a kernel function <span class="math inline">\(\kappa(Q, K)\)</span> that can be computed more efficiently.</p>
<p>A common approach is to use random feature maps. For instance, consider a radial basis function (RBF) kernel:</p>
<p><span class="math display">\[
\kappa(x, y) = exp(-\frac{||x - y||^2}{2\sigma^2})
\]</span></p>
<p>The idea is to approximate this kernel using random Fourier features. Specifically, Bochner’s theorem states that a shift-invariant kernel can be represented as the Fourier transform of a probability distribution. This allows us to approximate the kernel using a finite number of random samples from that distribution.</p>
<p>The RBF Kernel can be written as:</p>
<p><span class="math display">\[
\kappa(x, y) =  \mathbb{E}_{\omega \sim p(\omega)} [e^{i\omega^T x} e^{-i\omega^T y}] = \mathbb{E}_{\omega \sim p(\omega)} [z(x)^T z(y)]
\]</span> where <span class="math inline">\(z(x) = e^{i\omega^T x}\)</span> is the feature map. We can approximate the kernel by sampling <span class="math inline">\(D\)</span> random features, <span class="math inline">\(\omega_i\)</span>, from <span class="math inline">\(p(\omega)\)</span>: <span class="math display">\[
\kappa(x, y) \approx \frac{1}{D} \sum_{i=1}^D e^{i\omega_i^T x} e^{-i\omega_i^T y} = z'(x)^Tz'(y)
\]</span> where <span class="math inline">\(z'(x) \in \mathbb{C}^D\)</span> is the <em>approximated</em> feature map of <span class="math inline">\(x\)</span>. We can rewrite the complex exponential as trigonometric functions to yield real-valued random Fourier features.</p>
<p>Let <span class="math inline">\(\phi(x)\)</span> be the random feature map that approximates the kernel. The attention mechanism then becomes:</p>
<p><span class="math display">\[
Attention(Q, K, V) = softmax(\phi(Q)\phi(K)^T)V
\]</span></p>
<p>If <span class="math inline">\(\phi(x)\)</span> is of dimension <span class="math inline">\(D\)</span>, the computational complexity changes. The computational complexity of <span class="math inline">\(\phi(Q) \phi(K)^T\)</span> is <span class="math inline">\(O(n D d_k) + O(n D d_k) + O(n^2D)\)</span>, where <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> are of shape <span class="math inline">\((n, d_k)\)</span>, since we must first project <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> to the feature space of dimension <span class="math inline">\(D\)</span> via <span class="math inline">\(\phi\)</span>. The entire attention operation then becomes:</p>
<p><span class="math inline">\(O(n D d_k) + O(n D d_k) + O(n^2D) + O(n^2) + O(n^2d_v) \approx O(n^2 D)\)</span>, where <span class="math inline">\(D &lt;&lt; n\)</span></p>
<p><strong>Linear Attention</strong></p>
<p>For certain kernel choices and approximations, linear attention can achieve <span class="math inline">\(O(n)\)</span> complexity. This often involves restructuring the computation to avoid explicit computation of the <span class="math inline">\(n \times n\)</span> attention matrix. Instead of computing <span class="math inline">\(softmax(\phi(Q)\phi(K)^T)V\)</span> directly, we can compute:</p>
<p><span class="math display">\[
Attention(Q, K, V) =  (\phi(Q) \cdot (\phi(K)^T V))
\]</span></p>
<p>The key assumption here is that we can apply the softmax function in a stable manner directly on the kernel outputs. If we let <span class="math inline">\(z(Q) = \phi(Q)\)</span> and <span class="math inline">\(z(K) = \phi(K)\)</span>,</p>
<ul>
<li><span class="math inline">\(\phi(K)^T V\)</span> is <span class="math inline">\((D \times n)(n \times d_v) \rightarrow (D \times d_v)\)</span>. The computational complexity is <span class="math inline">\(O(n D d_v)\)</span>.</li>
<li><span class="math inline">\(\phi(Q) (\phi(K)^T V)\)</span> is <span class="math inline">\((n \times D)(D \times d_v) \rightarrow (n \times d_v)\)</span>. The computational complexity is <span class="math inline">\(O(n D d_v)\)</span>.</li>
</ul>
<p>Therefore, the overall complexity is <span class="math inline">\(O(n D d_v)\)</span>. If <span class="math inline">\(D\)</span> and <span class="math inline">\(d_v\)</span> are considered constants, the complexity becomes <span class="math inline">\(O(n)\)</span>.</p>
<p><strong>Memory Complexity:</strong></p>
<ul>
<li>Standard Attention: <span class="math inline">\(O(n^2)\)</span> due to the attention matrix.</li>
<li>Kernel-Based Attention with Random Features (without linear time tricks) : <span class="math inline">\(O(n^2)\)</span> (assuming <span class="math inline">\(D\)</span> is large).</li>
<li>Linear Attention: <span class="math inline">\(O(nD)\)</span>, because you need to store the transformed <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> matrices in feature space (<span class="math inline">\(D\)</span> dimensions).</li>
</ul>
<p><strong>Trade-offs and Considerations:</strong></p>
<ul>
<li><strong>Approximation Error:</strong> Kernel-based methods introduce approximation errors, as the kernel is estimated using a finite number of random features. The accuracy depends on the choice of kernel and the number of features (<span class="math inline">\(D\)</span>).</li>
<li><strong>Choice of Kernel:</strong> The performance heavily relies on the choice of kernel. Different kernels have different approximation properties and computational costs.</li>
<li><strong>Implementation Details:</strong> Efficient implementations often involve careful memory management and parallelization.</li>
<li><strong>Constant Factors:</strong> While asymptotic complexity is important, constant factors can significantly impact performance in practice. In many real-world scenarios, the constant factor associated with the linear or near-linear complexity might be large, making it less beneficial for smaller sequence lengths compared to the more straightforward quadratic attention.</li>
<li><strong>Kernel Trick Applicability</strong>: Certain Kernels permit more efficient computational strategies than others.</li>
</ul>
<p><strong>In Summary:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 39%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Attention Mechanism</th>
<th>Time Complexity</th>
<th>Memory Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Standard Attention</td>
<td><span class="math inline">\(O(n^2)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
<tr class="even">
<td>Kernel-Based Attention (Random Feature Approximation)</td>
<td><span class="math inline">\(O(n^2D)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
<tr class="odd">
<td>Linear Attention</td>
<td><span class="math inline">\(O(nD)\)</span></td>
<td><span class="math inline">\(O(nD)\)</span></td>
</tr>
</tbody>
</table>
<p>where: * <span class="math inline">\(n\)</span> is the sequence length * <span class="math inline">\(D\)</span> is the number of random features used in the kernel approximation.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to present this information clearly and effectively in an interview:</p>
<ol type="1">
<li><p><strong>Start with Standard Attention:</strong></p>
<ul>
<li>“Let’s begin by discussing the standard attention mechanism. The core computation involves calculating attention weights using the softmax of <span class="math inline">\(QK^T\)</span>, followed by multiplying with the value matrix <span class="math inline">\(V\)</span>.”</li>
<li>“The <span class="math inline">\(QK^T\)</span> operation, where <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> are matrices of shape <span class="math inline">\((n, d_k)\)</span>, results in a matrix of shape <span class="math inline">\((n, n)\)</span>. This multiplication has a computational complexity of <span class="math inline">\(O(n^2 d_k)\)</span>.”</li>
<li>“Since the subsequent softmax and multiplication with <span class="math inline">\(V\)</span> (which is of size <span class="math inline">\(n \times d_v\)</span>) also have complexities that are, at most, <span class="math inline">\(O(n^2)\)</span>, the <em>overall time complexity</em> of standard attention is <span class="math inline">\(O(n^2)\)</span>.”</li>
<li>“The <em>memory complexity</em> is dominated by storing the <span class="math inline">\(n \times n\)</span> attention matrix, making it <span class="math inline">\(O(n^2)\)</span>.”</li>
</ul></li>
<li><p><strong>Introduce Kernel-Based Attention:</strong></p>
<ul>
<li>“To address the quadratic complexity of standard attention, kernel-based attention provides an alternative. The key idea is to replace the dot product with a kernel function, allowing for a more efficient computation.”</li>
<li>“One common approach involves using random feature maps to approximate the kernel. The random features method enables us to approximate the Kernel function as an inner product of two feature maps: i.e.&nbsp;<span class="math inline">\(\kappa(x, y) \approx \phi(x)^T\phi(y)\)</span>.”</li>
</ul></li>
<li><p><strong>Explain Random Feature Maps (if you choose to do so):</strong></p>
<ul>
<li>“The random features rely on Bochner’s theorem, which links shift-invariant kernels to Fourier transforms. We can approximate the kernel by sampling <span class="math inline">\(D\)</span> random features from the Fourier transform of the kernel.”</li>
<li>“In this case the attention mechanism becomes <span class="math inline">\(softmax(\phi(Q)\phi(K)^T)V\)</span>.”</li>
<li>“The projection of <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> to their feature maps incurs a cost of <span class="math inline">\(O(n D d_k)\)</span> each. The inner product of the two feature maps has a cost of <span class="math inline">\(O(n^2D)\)</span>”</li>
<li>“When <span class="math inline">\(D &lt;&lt; n\)</span>, this significantly improves the computational cost.”</li>
</ul></li>
<li><p><strong>Discuss Linear Attention (Crucial):</strong></p>
<ul>
<li>“For even greater efficiency, linear attention restructures the computation to avoid forming the full <span class="math inline">\(n \times n\)</span> attention matrix. By computing attention as <span class="math inline">\((\phi(Q) \cdot (\phi(K)^T V))\)</span>, the complexity can be reduced to <span class="math inline">\(O(nD)\)</span>.”</li>
<li>“The <span class="math inline">\(O(nD)\)</span> complexity arises because <span class="math inline">\(\phi(K)^T V\)</span> is a <span class="math inline">\((D \times n)(n \times d_v) = (D \times d_v)\)</span> matrix multiply, which is an <span class="math inline">\(O(n D d_v)\)</span> operation. Then <span class="math inline">\(\phi(Q) (\phi(K)^T V)\)</span> is an <span class="math inline">\((n \times D)(D \times d_v) = (n \times d_v)\)</span> matrix multiply, which is an <span class="math inline">\(O(n D d_v)\)</span> operation.”</li>
<li>“In practice, this corresponds to storing the transformed <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> matrices, resulting in a memory complexity of <span class="math inline">\(O(nD)\)</span>.”</li>
</ul></li>
<li><p><strong>Highlight Trade-offs:</strong></p>
<ul>
<li>“It’s important to note that kernel-based methods introduce approximation errors. The accuracy depends on the kernel choice and the number of random features used (<span class="math inline">\(D\)</span>).”</li>
<li>“Constant factors can also play a significant role. While linear attention has a better asymptotic complexity, the constant factors might make it less beneficial for smaller sequence lengths.”</li>
<li>“The choice of kernel affects the overall applicability and computational feasibility of the algorithm.”</li>
</ul></li>
<li><p><strong>Summarize and Conclude:</strong></p>
<ul>
<li>“In summary, standard attention has a time complexity of <span class="math inline">\(O(n^2)\)</span> and a memory complexity of <span class="math inline">\(O(n^2)\)</span>. Kernel-based attention can reduce the time complexity to <span class="math inline">\(O(nD)\)</span>, but it introduces approximation errors and has its own implementation considerations.”</li>
<li>“The best approach depends on the specific application, the sequence length, and the desired trade-off between accuracy and efficiency.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow time for the interviewer to absorb the information.</li>
<li><strong>Check for Understanding:</strong> Pause periodically and ask if the interviewer has any questions.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you’re in a virtual interview, consider sharing a screen with the formulas or a diagram.</li>
<li><strong>Focus on the Key Concepts:</strong> Emphasize the core ideas behind each approach rather than getting bogged down in excessive detail.</li>
<li><strong>Acknowledge Limitations:</strong> Don’t be afraid to admit that certain aspects are complex or require further investigation. This shows intellectual honesty.</li>
<li><strong>Adapt to the Interviewer:</strong> If the interviewer seems less familiar with the mathematical details, focus on the high-level concepts and trade-offs. If they are more technically inclined, delve deeper into the derivations.</li>
<li><strong>Highlight Practical Implications:</strong> Explain how these complexity differences impact real-world applications and model performance.</li>
</ul>
<p>By following these guidelines, you can effectively demonstrate your understanding of attention mechanisms and their complexity analysis in a clear and professional manner.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>