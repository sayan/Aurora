<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>efficient_transformers__memory_and_computational_optimizations__2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-what-are-kernel-based-methods-in-the-context-of-efficient-transformers-and-how-do-they-help-in-reducing-computational-costs" class="level2">
<h2 class="anchored" data-anchor-id="question-what-are-kernel-based-methods-in-the-context-of-efficient-transformers-and-how-do-they-help-in-reducing-computational-costs">Question: What are kernel-based methods in the context of Efficient Transformers, and how do they help in reducing computational costs?</h2>
<p><strong>Best Answer</strong></p>
<p>Kernel-based methods offer a powerful approach to reduce the computational burden of the attention mechanism in Transformers, particularly when dealing with long sequences. The core idea revolves around approximating the softmax attention function with kernel functions, enabling a reformulation of the attention computation that scales linearly with sequence length instead of quadratically.</p>
<p>Let’s break down the concept:</p>
<ol type="1">
<li><p><strong>The Problem: Quadratic Complexity of Standard Attention</strong></p>
<p>The standard attention mechanism, as introduced in the original Transformer paper, involves calculating attention weights between all pairs of tokens in a sequence. Given a sequence of length <span class="math inline">\(n\)</span>, the attention weights are computed as:</p>
<p><span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Q\)</span> is the query matrix of shape <span class="math inline">\((n, d_k)\)</span></li>
<li><span class="math inline">\(K\)</span> is the key matrix of shape <span class="math inline">\((n, d_k)\)</span></li>
<li><span class="math inline">\(V\)</span> is the value matrix of shape <span class="math inline">\((n, d_v)\)</span></li>
<li><span class="math inline">\(d_k\)</span> is the dimension of the keys (and queries)</li>
</ul>
<p>The matrix multiplication <span class="math inline">\(QK^T\)</span> results in an <span class="math inline">\((n, n)\)</span> matrix, leading to <span class="math inline">\(O(n^2)\)</span> complexity in both time and memory. This quadratic scaling becomes a bottleneck for long sequences.</p></li>
<li><p><strong>The Kernel Trick: Linearizing Attention</strong></p>
<p>Kernel-based methods aim to approximate the softmax function using a kernel function <span class="math inline">\(\phi(x)\)</span>, such that the attention mechanism becomes:</p>
<p><span class="math display">\[Attention(Q, K, V) \approx (\phi(Q)\phi(K)^T)V\]</span></p>
<p>The key is to choose a kernel function <span class="math inline">\(\phi(x)\)</span> that allows us to rewrite the computation in a more efficient manner. Instead of explicitly computing the <span class="math inline">\(n \times n\)</span> attention matrix, we can leverage the properties of the kernel to reduce the complexity.</p></li>
<li><p><strong>Associativity of Matrix Multiplication</strong></p>
<p>The crucial observation is that matrix multiplication is associative. This allows us to rearrange the computation:</p>
<p><span class="math display">\[(\phi(Q)\phi(K)^T)V = \phi(Q)(\phi(K)^TV)\]</span></p>
<p>Now, let’s examine the computational cost. Assume <span class="math inline">\(\phi(x)\)</span> maps a vector in <span class="math inline">\(\mathbb{R}^{d_k}\)</span> to a vector in <span class="math inline">\(\mathbb{R}^{m}\)</span>. Then,</p>
<ul>
<li><span class="math inline">\(\phi(Q)\)</span> has shape <span class="math inline">\((n, m)\)</span></li>
<li><span class="math inline">\(\phi(K)\)</span> has shape <span class="math inline">\((n, m)\)</span></li>
<li><span class="math inline">\(V\)</span> has shape <span class="math inline">\((n, d_v)\)</span></li>
<li><span class="math inline">\(\phi(K)^T V\)</span> has shape <span class="math inline">\((m, d_v)\)</span>, and the computational cost is <span class="math inline">\(O(nmd_v)\)</span></li>
<li><span class="math inline">\(\phi(Q)(\phi(K)^T V)\)</span> has shape <span class="math inline">\((n, d_v)\)</span>, and the computational cost is <span class="math inline">\(O(nmd_v)\)</span></li>
</ul>
<p>The overall complexity becomes <span class="math inline">\(O(nmd_v)\)</span>. If <span class="math inline">\(m\)</span> (the dimensionality of the kernel feature map) is independent of <span class="math inline">\(n\)</span>, we achieve linear complexity with respect to the sequence length <span class="math inline">\(n\)</span>.</p></li>
<li><p><strong>Example: Performer and FAVOR+</strong></p>
<p>One prominent example of a kernel-based efficient Transformer is Performer. Performer uses a specific type of kernel approximation called <em>Fast Attention Via positive Orthogonal Random features</em> (FAVOR+). FAVOR+ constructs unbiased or almost unbiased estimators of kernel attention using random features. Specifically, Performer uses a random feature map <span class="math inline">\(\phi(x)\)</span> such that:</p>
<p><span class="math display">\[softmax(x_i - x_j) \approx \mathbb{E}_{\phi}[\phi(x_i)\phi(x_j)^T]\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are the rows of <span class="math inline">\(QK^T\)</span>.</p>
<p>The random feature map <span class="math inline">\(\phi(x)\)</span> is designed such that its inner product approximates the softmax kernel. The expectation is approximated using a finite number of random samples, which introduces a trade-off between accuracy and computational cost.</p>
<p>In Performer, the random feature map <span class="math inline">\(\phi\)</span> is constructed as:</p>
<p><span class="math display">\[\phi(x) = \frac{1}{\sqrt{m}}[h_1(x), h_2(x), ..., h_m(x)]\]</span></p>
<p>where <span class="math inline">\(h_i(x)\)</span> are random features. The specific form of <span class="math inline">\(h_i(x)\)</span> depends on the chosen kernel approximation method.</p></li>
<li><p><strong>Trade-offs and Considerations</strong></p>
<ul>
<li><p><strong>Approximation Accuracy:</strong> Kernel-based methods introduce approximations, which can lead to a reduction in model accuracy compared to standard attention, especially for complex tasks.</p></li>
<li><p><strong>Kernel Choice:</strong> The choice of the kernel function <span class="math inline">\(\phi(x)\)</span> is critical. Different kernels have different properties and may be more suitable for certain types of data or tasks. The kernel should be chosen to be positive definite, and its approximation should be efficiently computable.</p></li>
<li><p><strong>Dimensionality of the Kernel Feature Map (m):</strong> Increasing <span class="math inline">\(m\)</span> improves the accuracy of the approximation but also increases the computational cost. The appropriate value of <span class="math inline">\(m\)</span> depends on the specific application and the desired trade-off between accuracy and efficiency.</p></li>
<li><p><strong>Hardware Acceleration:</strong> The linear attention mechanism is amenable to hardware acceleration, making it possible to further improve the efficiency of these models.</p></li>
<li><p><strong>Memory Efficiency:</strong> Beyond computational complexity, kernel-based methods also address the memory bottleneck associated with the <span class="math inline">\(O(n^2)\)</span> attention matrix.</p></li>
</ul></li>
<li><p><strong>Beyond Performer: Other Kernel Methods</strong></p>
<p>While Performer is a notable example, other kernel-based methods exist for efficient Transformers. These methods often differ in the choice of kernel function, the approximation technique, and the specific trade-offs they make between accuracy and efficiency.</p>
<p>Examples include:</p>
<ul>
<li><strong>Linformer:</strong> Projects the key and value matrices to a lower-dimensional space.</li>
<li><strong>Nyströmformer:</strong> Uses the Nyström method to approximate the attention matrix.</li>
</ul></li>
</ol>
<p>In summary, kernel-based methods offer a way to significantly reduce the computational cost of attention in Transformers by approximating the softmax function with kernel functions. This enables linear complexity in sequence length, making it feasible to process very long sequences. The Performer model, with its FAVOR+ approximation, is a prominent example of this approach. However, it’s crucial to consider the trade-offs between approximation accuracy, computational cost, and the choice of the kernel function when implementing and using these methods.</p>
<hr>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to effectively explain kernel-based methods in Efficient Transformers during an interview:</p>
<ol type="1">
<li><p><strong>Start with the Problem:</strong></p>
<ul>
<li>“The standard attention mechanism in Transformers has a computational complexity of <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the sequence length. This quadratic scaling becomes a bottleneck for long sequences, limiting the model’s ability to process very long documents, audio, or video.”</li>
</ul></li>
<li><p><strong>Introduce Kernel-Based Methods:</strong></p>
<ul>
<li>“Kernel-based methods offer a solution by approximating the softmax attention function using kernel functions. This allows us to reformulate the attention computation in a way that scales linearly with sequence length.”</li>
</ul></li>
<li><p><strong>Explain the Core Idea:</strong></p>
<ul>
<li>“The core idea is to replace the softmax attention calculation with a kernel function <span class="math inline">\(\phi(x)\)</span>. Instead of calculating <span class="math inline">\(softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span>, we approximate it with an expression like <span class="math inline">\((\phi(Q)\phi(K)^T)V\)</span>.”</li>
<li>“The key here is the associativity of matrix multiplication. We can rewrite <span class="math inline">\((\phi(Q)\phi(K)^T)V\)</span> as <span class="math inline">\(\phi(Q)(\phi(K)^TV)\)</span>. By computing <span class="math inline">\(\phi(K)^TV\)</span> first, which has complexity <span class="math inline">\(O(nmd_v)\)</span>, and then multiplying by <span class="math inline">\(\phi(Q)\)</span>, we can achieve an overall complexity of <span class="math inline">\(O(nmd_v)\)</span> where <span class="math inline">\(m\)</span> is the dimension of the feature map. If <span class="math inline">\(m\)</span> is independent of <span class="math inline">\(n\)</span>, this becomes linear.”</li>
</ul></li>
<li><p><strong>Give an Example (Performer):</strong></p>
<ul>
<li>“A prominent example is the Performer model. Performer uses a technique called FAVOR+ (Fast Attention Via positive Orthogonal Random features) to approximate the softmax kernel with random features.”</li>
<li>“FAVOR+ constructs unbiased or almost unbiased estimators of kernel attention using random features, allowing for efficient computation while maintaining a good approximation of the original attention mechanism.”</li>
<li>“The random feature map <span class="math inline">\(\phi(x)\)</span> in Performer is designed such that its inner product approximates the softmax kernel: <span class="math inline">\(softmax(x_i - x_j) \approx \mathbb{E}_{\phi}[\phi(x_i)\phi(x_j)^T]\)</span>”. (You don’t necessarily need to show the equation, but mentioning the approximation helps.)</li>
</ul></li>
<li><p><strong>Discuss Trade-offs:</strong></p>
<ul>
<li>“It’s important to note that these approximations introduce trade-offs. We’re sacrificing some accuracy for the sake of efficiency.”</li>
<li>“Factors like the choice of kernel function and the dimensionality of the feature map (m) affect the accuracy and computational cost. Increasing ‘m’ improves accuracy but also increases the computation.”</li>
</ul></li>
<li><p><strong>Mention Other Methods (Briefly):</strong></p>
<ul>
<li>“While Performer is a good example, other approaches exist, such as Linformer and Nyströmformer, each with its own advantages and disadvantages.”</li>
</ul></li>
<li><p><strong>Conclude with Practical Implications:</strong></p>
<ul>
<li>“Kernel-based methods are crucial for handling very long sequences, which are common in many real-world applications. They allow us to train and deploy Transformers on tasks that would otherwise be computationally infeasible.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use Visual Cues (if possible):</strong> If you’re in a virtual interview, consider asking if you can share your screen to sketch out the matrix operations or show a simplified diagram.</li>
<li><strong>Check for Understanding:</strong> After explaining a key concept, pause and ask, “Does that make sense?” or “Would you like me to elaborate on that point?”</li>
<li><strong>Avoid Jargon:</strong> While technical terms are important, try to explain them in plain language when possible.</li>
<li><strong>Highlight the ‘Why’:</strong> Always emphasize the practical benefits of kernel-based methods, such as enabling the processing of longer sequences and reducing computational costs.</li>
<li><strong>Be Ready to Elaborate:</strong> The interviewer may ask follow-up questions about specific kernel functions, the implementation details of FAVOR+, or the trade-offs between different approximation techniques. Be prepared to provide more detail if asked.</li>
<li><strong>Keep it Concise:</strong> While a comprehensive answer is needed, avoid unnecessary details. Focus on the key concepts and the most important aspects of kernel-based methods.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>