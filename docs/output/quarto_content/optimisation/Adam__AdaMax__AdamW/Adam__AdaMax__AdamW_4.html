<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>adam__adamax__adamw_4</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-5.-suppose-you-are-deploying-a-machine-learning-model-on-streaming-noisy-data-in-a-production-environment.-given-the-characteristics-of-adam-adamax-and-adamw-how-would-you-choose-an-optimizer-for-this-scenario-discuss-aspects-related-to-scalability-robustness-to-noise-and-handling-of-non-stationary-data." class="level2">
<h2 class="anchored" data-anchor-id="question-5.-suppose-you-are-deploying-a-machine-learning-model-on-streaming-noisy-data-in-a-production-environment.-given-the-characteristics-of-adam-adamax-and-adamw-how-would-you-choose-an-optimizer-for-this-scenario-discuss-aspects-related-to-scalability-robustness-to-noise-and-handling-of-non-stationary-data.">Question: 5. Suppose you are deploying a machine learning model on streaming, noisy data in a production environment. Given the characteristics of Adam, AdaMax, and AdamW, how would you choose an optimizer for this scenario? Discuss aspects related to scalability, robustness to noise, and handling of non-stationary data.</h2>
<p><strong>Best Answer</strong></p>
<p>Choosing an optimizer for streaming, noisy data in a production environment requires careful consideration of the specific challenges posed by such data. Let’s analyze Adam, AdaMax, and AdamW, focusing on scalability, robustness to noise, and handling non-stationary data.</p>
<p><strong>1. Background of the Optimizers</strong></p>
<ul>
<li><strong>Adam (Adaptive Moment Estimation):</strong>
<ul>
<li>Adam is a popular adaptive learning rate optimization algorithm that combines the benefits of both AdaGrad and RMSProp. It computes adaptive learning rates for each parameter by maintaining estimates of both the first moment (mean) and the second moment (uncentered variance) of the gradients.</li>
<li>The update rule for Adam is as follows: <span class="math display">\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t = \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t = \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\]</span> Where:
<ul>
<li><span class="math inline">\(\theta_t\)</span> is the parameter vector at time <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(g_t\)</span> is the gradient at time <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> are the estimates of the first and second moments of the gradients, respectively.</li>
<li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are the exponential decay rates for the first and second moment estimates (typically set to 0.9 and 0.999, respectively).</li>
<li><span class="math inline">\(\hat{m}_t\)</span> and <span class="math inline">\(\hat{v}_t\)</span> are bias-corrected moment estimates.</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate.</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant added for numerical stability (e.g., <span class="math inline">\(10^{-8}\)</span>).</li>
</ul></li>
</ul></li>
<li><strong>AdaMax:</strong>
<ul>
<li>AdaMax is a variant of Adam based on infinity norm (<span class="math inline">\(L_\infty\)</span>). Instead of using the second moment directly, AdaMax uses an exponentially weighted infinity norm.</li>
<li>AdaMax update rule: <span class="math display">\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
u_t = \max(\beta_2 u_{t-1}, |g_t|) \\
\theta_{t+1} = \theta_t - \frac{\eta}{u_t} \hat{m}_t
\]</span> Where:
<ul>
<li><span class="math inline">\(u_t\)</span> is the exponentially weighted infinity norm.</li>
<li>All other symbols are as defined for Adam. The bias correction is not required for <span class="math inline">\(u_t\)</span>.</li>
</ul></li>
</ul></li>
<li><strong>AdamW:</strong>
<ul>
<li>AdamW is a modification of Adam that decouples the weight decay regularization from the gradient-based updates. In standard Adam (and other adaptive methods), weight decay is applied directly to the gradients, which can lead to suboptimal performance. AdamW applies weight decay directly to the weights, which is theoretically more sound and often leads to better generalization.</li>
<li>AdamW update rule: <span class="math display">\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t = \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t = \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} = \theta_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)
\]</span> Where:
<ul>
<li><span class="math inline">\(\lambda\)</span> is the weight decay parameter.</li>
<li>All other symbols are as defined for Adam.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>2. Considerations for Streaming, Noisy Data</strong></p>
<ul>
<li><p><strong>Scalability:</strong> All three optimizers (Adam, AdaMax, and AdamW) are generally scalable due to their per-parameter adaptive learning rates. They don’t require storing the entire history of gradients, making them suitable for large datasets and models. However, the computation of first and second moments does add a small overhead compared to SGD. In terms of memory requirements, they are comparable.</p></li>
<li><p><strong>Robustness to Noise:</strong></p>
<ul>
<li><strong>Adam:</strong> The adaptive learning rates in Adam can help mitigate the impact of noisy gradients. The moving averages of the gradients provide a form of smoothing, reducing the influence of individual noisy updates. However, in highly noisy environments, the adaptive learning rates can sometimes become too aggressive, leading to oscillations or divergence.</li>
<li><strong>AdaMax:</strong> By using the infinity norm, AdaMax can be more stable than Adam in situations where the gradients have very large or sparse values. The max operation in AdaMax tends to dampen the effect of extreme values, making it potentially more robust to outliers and noise. However, the infinity norm can also be less sensitive to subtle changes in the gradient distribution.</li>
<li><strong>AdamW:</strong> The decoupling of weight decay in AdamW doesn’t directly affect the robustness to noise in the gradients. However, the improved regularization provided by AdamW can lead to better generalization and, indirectly, better performance on noisy data. A well-regularized model is less likely to overfit to the noise in the training data.</li>
</ul></li>
<li><p><strong>Handling Non-Stationary Data:</strong></p>
<ul>
<li><strong>Adam:</strong> Adam’s adaptive learning rates can be beneficial for non-stationary data because they allow the optimizer to adjust to changes in the data distribution over time. If the data distribution shifts, the moving averages of the gradients will adapt, allowing the optimizer to continue learning effectively. However, in rapidly changing environments, the momentum terms can cause the optimizer to lag behind the true gradient, leading to slower convergence.</li>
<li><strong>AdaMax:</strong> Similar to Adam, AdaMax can adapt to changes in the data distribution. The use of the infinity norm might make it slightly less sensitive to abrupt changes, but it also might make it slower to adapt to gradual shifts.</li>
<li><strong>AdamW:</strong> The weight decay in AdamW can help prevent the model from overfitting to the most recent data, which can be particularly important in non-stationary environments. By regularizing the weights, AdamW encourages the model to maintain a more stable representation of the data, which can improve its ability to generalize to new data distributions.</li>
</ul></li>
</ul>
<p><strong>3. Choosing the Right Optimizer</strong></p>
<p>Given the characteristics of the data (streaming, noisy, non-stationary), and the nature of the problem:</p>
<ul>
<li><p><strong>AdamW</strong> is often a strong first choice. The decoupled weight decay provides better regularization, which is essential for noisy and non-stationary data. This prevents overfitting to transient patterns. The adaptive learning rates help to handle the non-stationarity and noise.</p></li>
<li><p><strong>Adam</strong> could be considered if computational efficiency is a primary concern and the regularization provided by AdamW is not strictly necessary. However, I would expect AdamW to perform better in most cases with noisy, streaming data due to its superior regularization.</p></li>
<li><p><strong>AdaMax</strong> is less commonly used than AdamW or Adam. However, it could be worth experimenting with if the gradients are known to be particularly sparse or have extreme values. Its robustness to outliers might provide an advantage in some cases. However, be mindful of the potential for slower adaptation to gradual data shifts due to the infinity norm.</p></li>
</ul>
<p><strong>4. Real-World Considerations and Implementation Details</strong></p>
<ul>
<li><strong>Hyperparameter Tuning:</strong> All three optimizers have hyperparameters that need to be tuned, such as the learning rate, the momentum decay rates (<span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>), and the weight decay parameter (for AdamW). It’s crucial to use a validation set or online evaluation metrics to tune these hyperparameters effectively. For non-stationary data, adaptive hyperparameter tuning methods could be considered.</li>
<li><strong>Learning Rate Warmup:</strong> Using a learning rate warmup schedule can help to stabilize training, especially in the early stages. This involves gradually increasing the learning rate from a small value to the desired value over a certain number of iterations.</li>
<li><strong>Gradient Clipping:</strong> To prevent exploding gradients due to noise, consider using gradient clipping. This involves scaling the gradients down if their norm exceeds a certain threshold.</li>
<li><strong>Monitoring and Logging:</strong> Implement robust monitoring and logging to track the performance of the model over time. Monitor metrics such as loss, accuracy, and gradient norms to detect potential problems such as divergence or overfitting.</li>
<li><strong>Batch Size:</strong> For streaming data, the “batch size” effectively becomes how often the model is updated. Smaller batch sizes (more frequent updates) might be beneficial for adapting to non-stationary data more quickly, but could also increase the variance of the gradients.</li>
<li><strong>Experimentation:</strong> Ultimately, the best optimizer for a specific problem will depend on the characteristics of the data and the model. It’s essential to experiment with different optimizers and hyperparameters to find the configuration that works best in practice.</li>
</ul>
<p><strong>In summary:</strong> AdamW is likely the best choice for this scenario, offering a balance of robustness, regularization, and adaptability. However, careful tuning of the hyperparameters and continuous monitoring of the model’s performance are crucial for successful deployment in a streaming, noisy environment.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested way to deliver this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a high-level overview:</strong> “For deploying a machine learning model on streaming, noisy, and non-stationary data, I would carefully consider the characteristics of Adam, AdaMax, and AdamW, focusing on scalability, robustness, and adaptability. I would likely choose AdamW as a starting point, but the final decision would depend on empirical testing.”</p></li>
<li><p><strong>Briefly explain each optimizer:</strong> “Let me briefly outline each optimizer. Adam is an adaptive learning rate method using estimates of the first and second moments of the gradients. AdaMax is a variant based on the infinity norm, potentially more stable with sparse gradients. AdamW is a modification of Adam with decoupled weight decay, which often leads to better generalization.”</p></li>
<li><p><strong>Discuss Scalability:</strong> “All three are generally scalable due to per-parameter adaptive learning rates, suitable for large datasets. The overhead compared to SGD is minor.”</p></li>
<li><p><strong>Deep dive into Robustness:</strong> “Now, let’s discuss robustness to noise. Adam’s adaptive learning rates smooth out noisy gradients to some extent. AdaMax, using the infinity norm, can be more resilient to extreme gradient values. However, AdamW’s advantage is the <em>decoupled weight decay</em>. This means that regularization is applied directly to the weights, not via the gradients, improving generalization and helping prevent overfitting to noisy data. Think of it as a way to make the model less sensitive to the ‘wiggles’ caused by the noise.”</p></li>
<li><p><strong>Address Handling of Non-Stationary Data:</strong> “For non-stationary data, Adam’s adaptive learning rates are generally beneficial, allowing the model to adjust to changes in the data distribution. AdaMax behaves similarly. AdamW’s weight decay plays a key role here. It helps the model maintain a more stable representation of the data, preventing it from overly adapting to recent, potentially transient, patterns. In other words, it avoids ‘chasing its tail’ as the data changes.”</p></li>
<li><p><strong>Justify your choice:</strong> “Considering these aspects, I would lean towards AdamW as my initial choice. Its superior regularization is particularly valuable for noisy and non-stationary data, helping to prevent overfitting and improve generalization. Although Adam is computationally efficient and AdaMax might be useful with sparse gradients, the benefits of AdamW outweigh the potential drawbacks in this specific scenario. It provides a better balance of robustness, regularization, and adaptability.”</p></li>
<li><p><strong>Mention Real-World Considerations:</strong> “However, it’s important to acknowledge real-world implementation details. This includes hyperparameter tuning (learning rate, momentum decay rates, weight decay), potentially using a learning rate warmup, gradient clipping to prevent exploding gradients, and robust monitoring of performance metrics. Also, experimenting with batch sizes (how frequently the model updates) can be beneficial.”</p></li>
<li><p><strong>Conclude Strong:</strong> “Ultimately, the best choice depends on the specific data and model. A thorough experimental approach is vital to determine the optimal optimizer and hyperparameter configuration for this particular production environment.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Give the interviewer time to absorb the information.</li>
<li><strong>Use analogies:</strong> Explain complex concepts using relatable analogies. For example, you can compare adaptive learning rates to adjusting the sensitivity of a microphone in a noisy environment.</li>
<li><strong>Visual aids:</strong> If you’re in a virtual interview, consider using a whiteboard to illustrate equations or diagrams.</li>
<li><strong>Engage the interviewer:</strong> Ask if they have any questions or if they’d like you to elaborate on any specific point.</li>
<li><strong>Be confident but not arrogant:</strong> Project confidence in your knowledge, but avoid sounding condescending.</li>
<li><strong>Focus on the “why”:</strong> Explain not just <em>what</em> the optimizers do, but <em>why</em> they work in certain situations.</li>
<li><strong>Address the trade-offs:</strong> Acknowledge that there are trade-offs between different optimizers, and explain how you would weigh these trade-offs in practice.</li>
<li><strong>Don’t be afraid to say “it depends”:</strong> Machine learning is an empirical field, and the best solution often depends on the specific problem. It’s okay to say that the optimal choice depends on the data and model characteristics.</li>
<li><strong>For equations:</strong> “I can illustrate the update rules. For example, Adam updates parameters using these equations [show them], where <span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> represent the estimates of the first and second moments… However, the key takeaway is that it adapts the learning rate for each parameter based on these moment estimates, and this adaptation helps navigate noisy gradients.”</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>