<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>stochastic_gradient_descent_4</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-5.-in-a-real-world-setting-with-high-dimensional-noisy-and-potentially-imbalanced-data-how-would-you-adapt-or-extend-traditional-sgd-to-handle-issues-such-as-scaling-robustness-and-convergence-reliability" class="level2">
<h2 class="anchored" data-anchor-id="question-5.-in-a-real-world-setting-with-high-dimensional-noisy-and-potentially-imbalanced-data-how-would-you-adapt-or-extend-traditional-sgd-to-handle-issues-such-as-scaling-robustness-and-convergence-reliability">Question: 5. In a real-world setting with high-dimensional, noisy, and potentially imbalanced data, how would you adapt or extend traditional SGD to handle issues such as scaling, robustness, and convergence reliability?</h2>
<p><strong>Best Answer</strong></p>
<p>Addressing the challenges of high-dimensional, noisy, and imbalanced data with Stochastic Gradient Descent (SGD) in real-world settings necessitates a multi-faceted approach. Traditional SGD, while fundamental, often falls short in such complex scenarios. Here’s a breakdown of strategies across data preprocessing, optimization algorithms, and robustness enhancements:</p>
<p><strong>1. Data Preprocessing and Normalization:</strong></p>
<p>The initial step involves handling the data itself. High dimensionality can lead to the “curse of dimensionality,” where data becomes sparse and distances lose meaning. Noise can obscure underlying patterns, and class imbalance can bias the model.</p>
<ul>
<li><p><strong>Dimensionality Reduction:</strong> Techniques like Principal Component Analysis (PCA) or feature selection methods can reduce dimensionality while retaining essential information. PCA projects the data into a lower-dimensional space by finding the principal components that capture the most variance:</p>
<p><span class="math display">\[
X_{reduced} = XW
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the original data matrix, <span class="math inline">\(W\)</span> is the matrix of principal components (eigenvectors of the covariance matrix of <span class="math inline">\(X\)</span>), and <span class="math inline">\(X_{reduced}\)</span> is the dimensionality-reduced data. Feature selection methods use statistical tests or model-based approaches to select the most relevant features.</p></li>
<li><p><strong>Data Normalization/Standardization:</strong> Normalization scales the data to a specific range (e.g., [0, 1]), while standardization transforms the data to have zero mean and unit variance. Standardization is generally preferred when outliers are present.</p>
<ul>
<li><em>Normalization</em>: <span class="math inline">\(x_{scaled} = \frac{x - x_{min}}{x_{max} - x_{min}}\)</span></li>
<li><em>Standardization</em>: <span class="math inline">\(x_{standardized} = \frac{x - \mu}{\sigma}\)</span>, where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma\)</span> is the standard deviation.</li>
</ul></li>
<li><p><strong>Handling Missing Values:</strong> Imputation techniques (mean, median, or model-based imputation) or explicit handling of missing data (e.g., using masking layers in neural networks) should be employed.</p></li>
<li><p><strong>Addressing Class Imbalance:</strong> Several strategies can mitigate the impact of imbalanced data.</p>
<ul>
<li><p><em>Resampling Techniques</em>: Oversampling the minority class (e.g., SMOTE - Synthetic Minority Oversampling Technique) or undersampling the majority class. SMOTE generates synthetic samples by interpolating between existing minority class samples.</p></li>
<li><p><em>Cost-Sensitive Learning</em>: Assigning higher misclassification costs to the minority class. This can be incorporated into the loss function. For example, a weighted cross-entropy loss:</p>
<p><span class="math display">\[
L = -\frac{1}{N} \sum_{i=1}^{N} [w \cdot y_i \cdot \log(\hat{y}_i) + (1 - y_i) \cdot \log(1 - \hat{y}_i)]
\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the true label, <span class="math inline">\(\hat{y}_i\)</span> is the predicted probability, and <span class="math inline">\(w\)</span> is the weight for the positive class.</p></li>
<li><p><em>Ensemble Methods</em>: Using ensemble methods like Balanced Random Forest or EasyEnsemble, which combine multiple classifiers trained on balanced subsets of the data.</p></li>
</ul></li>
</ul>
<p><strong>2. Adaptive Learning Rate Methods:</strong></p>
<p>Traditional SGD uses a fixed or manually decayed learning rate, which can be problematic in high-dimensional spaces with varying gradients. Adaptive learning rate methods adjust the learning rate for each parameter individually, often leading to faster convergence and better performance.</p>
<ul>
<li><p><strong>Adam (Adaptive Moment Estimation):</strong> Adam combines the benefits of both AdaGrad and RMSProp. It maintains estimates of both the first moment (mean) and the second moment (uncentered variance) of the gradients.</p>
<p><span class="math display">\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t = \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t = \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\]</span></p>
<p>where <span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> are the estimates of the first and second moments, <span class="math inline">\(g_t\)</span> is the gradient at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are the exponential decay rates for the moment estimates, <span class="math inline">\(\eta\)</span> is the learning rate, <span class="math inline">\(\epsilon\)</span> is a small constant to prevent division by zero, and <span class="math inline">\(\theta\)</span> represents the parameters of the model.</p></li>
<li><p><strong>RMSProp (Root Mean Square Propagation):</strong> RMSProp adapts the learning rate based on the moving average of the squared gradients. This helps to dampen oscillations and allows for a higher learning rate.</p>
<p><span class="math display">\[
v_t = \beta v_{t-1} + (1 - \beta) g_t^2 \\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} g_t
\]</span></p>
<p>where <span class="math inline">\(v_t\)</span> is the moving average of the squared gradients, <span class="math inline">\(\beta\)</span> is the decay rate, <span class="math inline">\(\eta\)</span> is the learning rate, and <span class="math inline">\(\epsilon\)</span> is a small constant.</p></li>
<li><p><strong>AdaGrad (Adaptive Gradient Algorithm):</strong> AdaGrad adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.</p>
<p><span class="math display">\[
s_t = s_{t-1} + g_t^2 \\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t} + \epsilon} g_t
\]</span></p>
<p>where <span class="math inline">\(s_t\)</span> is the sum of squared gradients up to time <span class="math inline">\(t\)</span>, <span class="math inline">\(\eta\)</span> is the learning rate, and <span class="math inline">\(\epsilon\)</span> is a small constant. However, AdaGrad’s learning rate can decay too aggressively, hindering convergence.</p></li>
</ul>
<p><strong>3. Variance Reduction Techniques:</strong></p>
<p>Noisy gradients can slow down convergence and lead to suboptimal solutions. Variance reduction techniques aim to reduce the variance of the gradient estimates.</p>
<ul>
<li><p><strong>Mini-Batch Gradient Descent:</strong> Using mini-batches (instead of single samples) to estimate the gradient reduces the variance of the estimate. The optimal batch size depends on the dataset and model architecture.</p></li>
<li><p><strong>Gradient Clipping:</strong> Clipping the gradients to a certain range prevents exploding gradients, a common issue in deep neural networks.</p>
<p><span class="math display">\[
g_t' = \begin{cases}
g_t, &amp; \text{if } ||g_t|| \leq \text{threshold} \\
\frac{g_t}{||g_t||} \cdot \text{threshold}, &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(g_t\)</span> is the gradient at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(||g_t||\)</span> is the norm of the gradient.</p></li>
<li><p><strong>Batch Normalization:</strong> Batch Normalization normalizes the activations of each layer within a mini-batch, reducing internal covariate shift and stabilizing training. It also has a regularizing effect.</p>
<p><span class="math display">\[
\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i \\
\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2 \\
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\
y_i = \gamma \hat{x}_i + \beta
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> are the activations in a mini-batch <span class="math inline">\(B\)</span>, <span class="math inline">\(\mu_B\)</span> and <span class="math inline">\(\sigma_B^2\)</span> are the mean and variance of the activations, <span class="math inline">\(\hat{x}_i\)</span> are the normalized activations, and <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learnable parameters.</p></li>
</ul>
<p><strong>4. Distributed SGD and Scalability:</strong></p>
<p>For very large datasets, distributing the training process across multiple machines can significantly reduce training time.</p>
<ul>
<li><strong>Data Parallelism:</strong> Distributing the data across multiple machines and computing the gradients independently on each machine. The gradients are then aggregated (e.g., using All-Reduce) to update the model parameters.</li>
<li><strong>Model Parallelism:</strong> Partitioning the model across multiple machines, where each machine is responsible for training a part of the model. This is useful for extremely large models that cannot fit on a single machine.</li>
<li><strong>Asynchronous SGD:</strong> Allowing workers to update the model parameters asynchronously, without waiting for all workers to finish their computations. This can improve training speed but may lead to staleness issues. Techniques like gradient compression or staleness-aware updates can mitigate these issues.</li>
</ul>
<p><strong>5. Convergence Monitoring and Robustness:</strong></p>
<p>Monitoring convergence and ensuring robustness are crucial for practical applications.</p>
<ul>
<li><p><strong>Early Stopping:</strong> Monitoring the performance on a validation set and stopping training when the performance starts to degrade.</p></li>
<li><p><strong>Gradient Norm Monitoring:</strong> Tracking the norm of the gradients to detect exploding or vanishing gradients.</p></li>
<li><p><strong>Regularization Techniques:</strong> L1 or L2 regularization to prevent overfitting and improve generalization. L1 regularization promotes sparsity, while L2 regularization penalizes large weights.</p>
<p><em>L1 Regularization</em>: <span class="math display">\[L(\theta) + \lambda ||\theta||_1\]</span> <em>L2 Regularization</em>: <span class="math display">\[L(\theta) + \frac{\lambda}{2} ||\theta||_2^2\]</span></p>
<p>where <span class="math inline">\(L(\theta)\)</span> is the loss function, <span class="math inline">\(\theta\)</span> are the model parameters, and <span class="math inline">\(\lambda\)</span> is the regularization strength.</p></li>
<li><p><strong>Robust Loss Functions:</strong> Using loss functions that are less sensitive to outliers, such as Huber loss or Tukey’s biweight loss. Huber loss is a combination of squared error loss and absolute error loss, making it less sensitive to outliers.</p></li>
</ul>
<p><strong>6. Algorithmic Modifications for Robustness:</strong></p>
<ul>
<li><p><strong>SWA (Stochastic Weight Averaging)</strong> Instead of using the final weights of a trained network, SWA averages the weights traversed during training with SGD. This often leads to better generalization and robustness. The SWA weights are calculated as:</p>
<p><span class="math display">\[
  \theta_{SWA} = \frac{1}{T} \sum_{t=1}^{T} \theta_t
  \]</span></p>
<p>where <span class="math inline">\(\theta_t\)</span> are the model weights at step <span class="math inline">\(t\)</span> and <span class="math inline">\(T\)</span> is the number of steps to average over.</p></li>
<li><p><strong>SAM (Sharpness-Aware Minimization)</strong> SAM seeks parameters that lie in a neighborhood with uniformly low loss, which corresponds to a flatter and more generalizable minimum. SAM perturbs the weights to find a “worse-case” neighborhood and then minimizes the loss in this neighborhood. This leads to improved robustness.</p></li>
</ul>
<p><strong>In Summary:</strong></p>
<p>Addressing the challenges of high-dimensional, noisy, and imbalanced data with SGD requires a combination of data preprocessing, adaptive optimization algorithms, variance reduction techniques, distributed training strategies, and robust convergence monitoring. The specific techniques used will depend on the specific characteristics of the dataset and the model architecture. Furthermore, experimentation and careful tuning are essential to achieve optimal performance in practice.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s how to present this information in an interview:</p>
<ol type="1">
<li><p><strong>Start with a high-level overview:</strong> “When dealing with high-dimensional, noisy, and imbalanced data with SGD, a multi-faceted approach is necessary. Traditional SGD often struggles, so we need to consider data preprocessing, advanced optimization techniques, and strategies for robustness.”</p></li>
<li><p><strong>Address Data Preprocessing (Emphasize Importance):</strong> “First, data preprocessing is crucial. High dimensionality can lead to sparsity, noise obscures patterns, and class imbalance biases the model. We can use techniques like PCA or feature selection for dimensionality reduction, normalization/standardization to scale features, and resampling or cost-sensitive learning to handle imbalance.” Give a concise example, such as: “For instance, SMOTE can be used to oversample the minority class by creating synthetic samples.”</p></li>
<li><p><strong>Explain Adaptive Learning Rate Methods (Focus on Intuition):</strong> “Next, adaptive learning rate methods like Adam, RMSProp, and AdaGrad are essential. These methods adjust the learning rate for each parameter individually, leading to faster convergence. Adam, for example, combines the benefits of AdaGrad and RMSProp by maintaining estimates of both the first and second moments of the gradients.” Briefly explain the intuition behind Adam without diving into the equations unless asked. “The key idea is to adapt the learning rate based on the gradient’s history.”</p></li>
<li><p><strong>Discuss Variance Reduction (Highlight Practicality):</strong> “Variance reduction techniques are also important. Mini-batch gradient descent reduces the variance of the gradient estimates compared to using single samples. Gradient clipping prevents exploding gradients, and Batch Normalization stabilizes training by normalizing activations within each mini-batch.”</p></li>
<li><p><strong>Mention Distributed SGD (Indicate Scalability Awareness):</strong> “For very large datasets, we can use distributed SGD. Data parallelism involves distributing the data across multiple machines, and the gradients are then aggregated. Model parallelism involves partitioning the model itself. Asynchronous SGD can further speed up training, but requires careful handling of staleness.”</p></li>
<li><p><strong>Address Convergence Monitoring and Robustness (Show Real-World Considerations):</strong> “Finally, monitoring convergence and ensuring robustness are critical. We can use early stopping by monitoring performance on a validation set. Regularization techniques like L1 or L2 regularization prevent overfitting. Also, using robust loss functions is less sensitive to outliers.”</p></li>
<li><p><strong>Handle Equations Strategically:</strong> “I can provide the equations for these methods if you’d like. For example, the update rule for Adam involves calculating the first and second moments of the gradients and then using these to adapt the learning rate.” <em>Only provide the equations if the interviewer asks for them.</em> If you do, walk through the main components step-by-step, explaining the purpose of each term without overwhelming them with detail. Focus on the intuition behind the equations.</p></li>
<li><p><strong>Conclude with a Summary:</strong> “In summary, tackling these challenges requires a combination of data preprocessing, adaptive optimization, variance reduction, distributed training, and robust monitoring. The specific choice of techniques depends on the particular problem, and experimentation is key.”</p></li>
<li><p><strong>Robustness Algorithms like SWA and SAM can be mentioned as add-ons, if the interviewer asks specifically about improving the model’s generalization ability.</strong></p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use clear and concise language:</strong> Avoid jargon unless necessary.</li>
<li><strong>Emphasize the “why”:</strong> Explain the reasoning behind each technique.</li>
<li><strong>Check for understanding:</strong> Ask the interviewer if they have any questions.</li>
<li><strong>Show enthusiasm:</strong> Demonstrate your interest in the topic.</li>
<li><strong>Be prepared to elaborate:</strong> Have a deeper understanding of each technique in case the interviewer asks for more details.</li>
<li><strong>Be honest about your knowledge:</strong> If you’re not sure about something, admit it. It’s better to be honest than to give incorrect information.</li>
<li><strong>Adapt to the interviewer’s level:</strong> Adjust the level of detail based on the interviewer’s background.</li>
</ul>
<p>By following these guidelines, you can effectively communicate your expertise and demonstrate your ability to handle complex real-world data science challenges.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>