<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>naive_bayes_2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-3.-what-are-the-key-differences-between-gaussian-multinomial-and-bernoulli-naive-bayes-classifiers-in-which-scenarios-might-each-variant-be-most-appropriate" class="level2">
<h2 class="anchored" data-anchor-id="question-3.-what-are-the-key-differences-between-gaussian-multinomial-and-bernoulli-naive-bayes-classifiers-in-which-scenarios-might-each-variant-be-most-appropriate">Question: 3. What are the key differences between Gaussian, Multinomial, and Bernoulli Naive Bayes classifiers? In which scenarios might each variant be most appropriate?</h2>
<p><strong>Best Answer</strong></p>
<p>Naive Bayes classifiers are a family of probabilistic classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between the features. Despite their simplicity, they can be surprisingly effective in practice, especially for high-dimensional data. The core differences between Gaussian, Multinomial, and Bernoulli Naive Bayes lie in the <em>assumed distribution of the features</em>.</p>
<p><strong>1. Core Principles &amp; Bayes’ Theorem</strong></p>
<p>At the heart of Naive Bayes is Bayes’ theorem, which provides a way to update our belief about a hypothesis (the class) given some evidence (the features). Mathematically, Bayes’ theorem is expressed as:</p>
<p><span class="math display">\[P(y|X) = \frac{P(X|y) P(y)}{P(X)}\]</span></p>
<p>Where: * <span class="math inline">\(P(y|X)\)</span> is the posterior probability of class <span class="math inline">\(y\)</span> given features <span class="math inline">\(X\)</span>. * <span class="math inline">\(P(X|y)\)</span> is the likelihood of features <span class="math inline">\(X\)</span> given class <span class="math inline">\(y\)</span>. * <span class="math inline">\(P(y)\)</span> is the prior probability of class <span class="math inline">\(y\)</span>. * <span class="math inline">\(P(X)\)</span> is the evidence (probability of features <span class="math inline">\(X\)</span>), often considered a normalizing constant.</p>
<p>The “naive” part comes from the assumption that features are conditionally independent given the class. This simplifies the calculation of <span class="math inline">\(P(X|y)\)</span> to:</p>
<p><span class="math display">\[P(X|y) = \prod_{i=1}^{n} P(x_i|y)\]</span></p>
<p>Where: * <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>-th feature. * <span class="math inline">\(n\)</span> is the number of features.</p>
<p><strong>2. Gaussian Naive Bayes</strong></p>
<ul>
<li><p><strong>Assumption:</strong> Assumes that the continuous features follow a Gaussian (normal) distribution within each class.</p></li>
<li><p><strong>Likelihood:</strong> The likelihood of a feature <span class="math inline">\(x_i\)</span> given class <span class="math inline">\(y\)</span> is modeled as a Gaussian distribution:</p>
<p><span class="math display">\[P(x_i|y) = \frac{1}{\sqrt{2\pi\sigma_y^2}} exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma_y^2}\right)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\mu_y\)</span> is the mean of feature <span class="math inline">\(x_i\)</span> for class <span class="math inline">\(y\)</span>.</li>
<li><span class="math inline">\(\sigma_y^2\)</span> is the variance of feature <span class="math inline">\(x_i\)</span> for class <span class="math inline">\(y\)</span>.</li>
</ul></li>
<li><p><strong>Parameter Estimation:</strong> The mean (<span class="math inline">\(\mu_y\)</span>) and variance (<span class="math inline">\(\sigma_y^2\)</span>) are estimated from the training data for each feature and class. Specifically:</p>
<p><span class="math display">\[\mu_y = \frac{1}{N_y}\sum_{x_i \in y} x_i\]</span></p>
<p><span class="math display">\[\sigma_y^2 = \frac{1}{N_y-1}\sum_{x_i \in y} (x_i - \mu_y)^2\]</span></p>
<p>Where <span class="math inline">\(N_y\)</span> is the number of instances belonging to class <span class="math inline">\(y\)</span>.</p></li>
<li><p><strong>Appropriate Scenarios:</strong> Suitable when features are continuous and approximately normally distributed. Examples include:</p>
<ul>
<li>Classification based on sensor readings (e.g., temperature, pressure).</li>
<li>Predicting customer behavior based on continuous metrics (e.g., income, age).</li>
<li>Image classification where features are continuous pixel intensities or descriptors.</li>
</ul></li>
<li><p><strong>Advantages:</strong> Simple and fast to train. Works well when the Gaussian assumption is reasonably met.</p></li>
<li><p><strong>Disadvantages:</strong> Performs poorly if the Gaussian assumption is severely violated. Feature scaling can sometimes help.</p></li>
</ul>
<p><strong>3. Multinomial Naive Bayes</strong></p>
<ul>
<li><p><strong>Assumption:</strong> Assumes that the features represent counts or frequencies of discrete events (e.g., word counts in a document).</p></li>
<li><p><strong>Likelihood:</strong> The likelihood of features <span class="math inline">\(X = (x_1, x_2, ..., x_n)\)</span> given class <span class="math inline">\(y\)</span> is modeled using a multinomial distribution:</p>
<p><span class="math display">\[P(X|y) = \frac{(\sum_i x_i)!}{\prod_i x_i!} \prod_i p_{yi}^{x_i}\]</span></p>
<p>However, in practice, we usually work with the logarithm of the probability to avoid underflow issues:</p>
<p><span class="math display">\[log \, P(X|y) = log\left(\frac{(\sum_i x_i)!}{\prod_i x_i!}\right) + \sum_i x_i \, log(p_{yi})\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x_i\)</span> is the count of feature <span class="math inline">\(i\)</span> in the sample.</li>
<li><span class="math inline">\(p_{yi}\)</span> is the probability of feature <span class="math inline">\(i\)</span> occurring given class <span class="math inline">\(y\)</span>.</li>
</ul></li>
<li><p><strong>Parameter Estimation:</strong> The probability <span class="math inline">\(p_{yi}\)</span> is estimated from the training data using maximum likelihood estimation with Laplace smoothing (also known as add-one smoothing) to avoid zero probabilities:</p>
<p><span class="math display">\[p_{yi} = \frac{N_{yi} + \alpha}{N_y + \alpha n}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N_{yi}\)</span> is the number of times feature <span class="math inline">\(i\)</span> appears in class <span class="math inline">\(y\)</span> in the training data.</li>
<li><span class="math inline">\(N_y\)</span> is the total number of features appearing in class <span class="math inline">\(y\)</span> in the training data.</li>
<li><span class="math inline">\(n\)</span> is the total number of features (vocabulary size).</li>
<li><span class="math inline">\(\alpha\)</span> is the smoothing parameter (usually 1 for Laplace smoothing).</li>
</ul></li>
<li><p><strong>Appropriate Scenarios:</strong> Well-suited for text classification tasks:</p>
<ul>
<li>Spam detection (classifying emails as spam or not spam).</li>
<li>Sentiment analysis (determining the sentiment of a text document).</li>
<li>Topic classification (assigning documents to predefined categories).</li>
</ul></li>
<li><p><strong>Advantages:</strong> Effective for discrete data, particularly text. Robust to irrelevant features.</p></li>
<li><p><strong>Disadvantages:</strong> Can be sensitive to the choice of smoothing parameter. Not suitable for continuous data.</p></li>
</ul>
<p><strong>4. Bernoulli Naive Bayes</strong></p>
<ul>
<li><p><strong>Assumption:</strong> Assumes that the features are binary (boolean) variables indicating the presence or absence of a particular attribute.</p></li>
<li><p><strong>Likelihood:</strong> The likelihood of features <span class="math inline">\(X = (x_1, x_2, ..., x_n)\)</span> given class <span class="math inline">\(y\)</span> is modeled using a Bernoulli distribution:</p>
<p><span class="math display">\[P(X|y) = \prod_{i=1}^{n} p_{yi}^{x_i} (1-p_{yi})^{(1-x_i)}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x_i\)</span> is a binary variable (0 or 1) indicating the absence or presence of feature <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(p_{yi}\)</span> is the probability of feature <span class="math inline">\(i\)</span> being present given class <span class="math inline">\(y\)</span>.</li>
</ul>
<p>Again, in practice, we often use the logarithm of the probability:</p>
<p><span class="math display">\[log \, P(X|y) = \sum_{i=1}^{n} x_i \, log(p_{yi}) + (1-x_i) \, log(1-p_{yi})\]</span></p></li>
<li><p><strong>Parameter Estimation:</strong> The probability <span class="math inline">\(p_{yi}\)</span> is estimated from the training data using maximum likelihood estimation with Laplace smoothing:</p>
<p><span class="math display">\[p_{yi} = \frac{N_{yi} + \alpha}{N_y + 2\alpha}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N_{yi}\)</span> is the number of samples in class <span class="math inline">\(y\)</span> where feature <span class="math inline">\(i\)</span> is present.</li>
<li><span class="math inline">\(N_y\)</span> is the total number of samples in class <span class="math inline">\(y\)</span>.</li>
<li><span class="math inline">\(\alpha\)</span> is the smoothing parameter (usually 1). The ‘2’ in the denominator accounts for both possible values (0 and 1).</li>
</ul></li>
<li><p><strong>Appropriate Scenarios:</strong> Suitable for binary feature data:</p>
<ul>
<li>Document classification with a binary bag-of-words representation (presence/absence of words).</li>
<li>Medical diagnosis based on the presence or absence of symptoms.</li>
<li>Spam detection using binary indicators for specific words or phrases.</li>
</ul></li>
<li><p><strong>Advantages:</strong> Works well with binary data. Simple and computationally efficient.</p></li>
<li><p><strong>Disadvantages:</strong> Less informative than Multinomial Naive Bayes when feature frequencies are important. Requires features to be binarized.</p></li>
</ul>
<p><strong>5. Summary Table</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 26%">
<col style="width: 30%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Gaussian</th>
<th>Multinomial</th>
<th>Bernoulli</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Feature Type</td>
<td>Continuous</td>
<td>Discrete (Counts/Frequencies)</td>
<td>Binary</td>
</tr>
<tr class="even">
<td>Distribution</td>
<td>Gaussian (Normal)</td>
<td>Multinomial</td>
<td>Bernoulli</td>
</tr>
<tr class="odd">
<td>Parameter Est.</td>
<td>Mean, Variance</td>
<td>Probability of each feature</td>
<td>Probability of feature presence</td>
</tr>
<tr class="even">
<td>Laplace Smoothing</td>
<td>Not Directly Applicable</td>
<td>Commonly Used</td>
<td>Commonly Used</td>
</tr>
<tr class="odd">
<td>Common Use Cases</td>
<td>Continuous data classification</td>
<td>Text classification (word counts)</td>
<td>Binary feature classification</td>
</tr>
</tbody>
</table>
<p><strong>6. Important Considerations &amp; Extensions</strong></p>
<ul>
<li><strong>Feature Scaling:</strong> While Naive Bayes is generally robust, feature scaling can sometimes improve performance, especially for Gaussian Naive Bayes, as it assumes normally distributed data.</li>
<li><strong>Independence Assumption:</strong> The naive independence assumption is often violated in practice. However, Naive Bayes can still perform surprisingly well, even with correlated features.</li>
<li><strong>Model Calibration:</strong> Naive Bayes classifiers are often poorly calibrated, meaning that the predicted probabilities are not accurate estimates of the true probabilities. Techniques like Platt scaling or isotonic regression can be used to calibrate the output probabilities.</li>
<li><strong>Hybrid Approaches:</strong> It’s possible to combine different Naive Bayes variants for different feature sets. For instance, using Gaussian Naive Bayes for continuous features and Multinomial Naive Bayes for discrete features in the same classification problem.</li>
</ul>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to present this information in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Basics:</strong> Begin by defining Naive Bayes as a probabilistic classifier based on Bayes’ theorem and the “naive” independence assumption. Briefly explain Bayes’ theorem, highlighting the prior, likelihood, and posterior.</p></li>
<li><p><strong>Explain the Key Difference:</strong> Emphasize that the main difference between the three variants lies in the assumed distribution of the features.</p>
<ul>
<li><em>“The core distinction between Gaussian, Multinomial, and Bernoulli Naive Bayes lies in their assumptions about the underlying data distribution. Gaussian assumes features are normally distributed, Multinomial deals with count data, and Bernoulli handles binary features.”</em></li>
</ul></li>
<li><p><strong>Gaussian Naive Bayes:</strong></p>
<ul>
<li>State the Gaussian assumption and explain the likelihood function using the Gaussian distribution formula.</li>
<li>Briefly mention how the mean and variance are estimated from the training data. You don’t need to derive the estimators unless specifically asked.</li>
<li>Provide a few real-world examples where Gaussian Naive Bayes is suitable (e.g., sensor data, continuous metrics).</li>
<li><em>“Gaussian Naive Bayes is suitable when you believe your features are continuous and approximately follow a normal distribution. Imagine classifying data from temperature sensors; that’s a good use case.”</em></li>
</ul></li>
<li><p><strong>Multinomial Naive Bayes:</strong></p>
<ul>
<li>State the multinomial assumption and explain that it’s used for count data, particularly in text classification.</li>
<li>Explain the likelihood function and the importance of Laplace smoothing to avoid zero probabilities. Mention the formula for calculating the smoothed probabilities.</li>
<li>Provide examples of text classification tasks (e.g., spam detection, sentiment analysis).</li>
<li><em>“Multinomial Naive Bayes excels with count data, making it ideal for text classification. We use Laplace smoothing to avoid zero probabilities, which is crucial when dealing with vocabulary.”</em></li>
</ul></li>
<li><p><strong>Bernoulli Naive Bayes:</strong></p>
<ul>
<li>State the Bernoulli assumption and explain that it’s used for binary features.</li>
<li>Explain the likelihood function and how the probabilities are estimated with Laplace smoothing.</li>
<li>Provide examples of scenarios with binary features (e.g., document classification with a binary bag-of-words).</li>
<li><em>“Bernoulli Naive Bayes is tailored for binary data, like the presence or absence of words in a document. It’s a simplified approach compared to Multinomial when only the presence matters, not the frequency.”</em></li>
</ul></li>
<li><p><strong>Summarize and Compare:</strong></p>
<ul>
<li>Use the summary table to highlight the key differences in a concise manner. This helps the interviewer quickly grasp the core distinctions.</li>
</ul></li>
<li><p><strong>Discuss Considerations:</strong></p>
<ul>
<li>Mention the limitations of the naive independence assumption and the potential need for model calibration.</li>
<li>Briefly discuss hybrid approaches and the role of feature scaling.</li>
<li><em>“It’s important to remember the ‘naive’ assumption. While often violated, Naive Bayes can still be surprisingly effective. Also, consider calibrating the probabilities if you need accurate confidence scores.”</em></li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Allow the interviewer time to process the information.</li>
<li><strong>Use Clear and Concise Language:</strong> Avoid jargon unless necessary.</li>
<li><strong>Provide Real-World Examples:</strong> Examples make the concepts more concrete and demonstrate your understanding of practical applications.</li>
<li><strong>Check for Understanding:</strong> Periodically ask the interviewer if they have any questions.</li>
<li><strong>Don’t Be Afraid to Simplify:</strong> If the interviewer seems overwhelmed, simplify your explanation without sacrificing accuracy.</li>
<li><strong>Highlight Trade-offs:</strong> Discuss the advantages and disadvantages of each variant to show a balanced understanding.</li>
<li><strong>Mathematical Notation:</strong> When presenting equations, explain each term clearly and avoid getting bogged down in excessive mathematical detail unless prompted. Focus on the intuition behind the equations. If writing on a whiteboard, make sure your notation is clear and well-organized.</li>
</ul>
<p>By following this structure and these communication tips, you can effectively demonstrate your understanding of Naive Bayes classifiers and their applications in an interview setting.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>