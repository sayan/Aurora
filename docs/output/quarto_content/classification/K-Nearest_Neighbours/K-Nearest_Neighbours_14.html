<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>k-nearest_neighbours_14</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-can-you-explain-how-knn-could-be-adapted-for-regression-tasks-what-are-the-differences-compared-to-classification" class="level2">
<h2 class="anchored" data-anchor-id="question-can-you-explain-how-knn-could-be-adapted-for-regression-tasks-what-are-the-differences-compared-to-classification">Question: Can you explain how KNN could be adapted for regression tasks? What are the differences compared to classification?</h2>
<p><strong>Best Answer</strong></p>
<p>K-Nearest Neighbors (KNN) is a versatile algorithm primarily known for classification tasks. However, it can be readily adapted for regression problems. The core principle remains the same: predict the value of a new data point based on the values of its ‘k’ nearest neighbors in the training dataset. The key difference lies in how the prediction is made based on those neighbors and the evaluation metrics used.</p>
<p><strong>KNN for Regression: The Mechanics</strong></p>
<p>In KNN regression, instead of predicting a class label, we predict a continuous value. The prediction is typically obtained by:</p>
<ol type="1">
<li><p><strong>Finding the K-Nearest Neighbors:</strong> Using a distance metric (e.g., Euclidean distance, Manhattan distance, Minkowski distance), identify the ‘k’ data points in the training set that are closest to the new data point for which we want to make a prediction. The choice of distance metric can influence the results, and the optimal metric often depends on the characteristics of the data.</p>
<ul>
<li><p><strong>Euclidean Distance:</strong> <span class="math display">\[d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}\]</span></p></li>
<li><p><strong>Manhattan Distance:</strong> <span class="math display">\[d(x, y) = \sum_{i=1}^{n}|x_i - y_i|\]</span></p></li>
<li><p><strong>Minkowski Distance:</strong> <span class="math display">\[d(x, y) = (\sum_{i=1}^{n}|x_i - y_i|^p)^{\frac{1}{p}}\]</span> (where p=1 is Manhattan, and p=2 is Euclidean)</p></li>
</ul></li>
<li><p><strong>Aggregating the Target Values:</strong> Once the k-nearest neighbors are found, their corresponding target values (the continuous values we are trying to predict) are aggregated to produce the prediction for the new data point. The most common aggregation methods are:</p>
<ul>
<li><p><strong>Simple Averaging:</strong> The prediction is the average of the target values of the k-nearest neighbors.</p>
<p><span class="math display">\[\hat{y} = \frac{1}{k}\sum_{i \in N(x)} y_i\]</span> where <span class="math inline">\(N(x)\)</span> is the set of k-nearest neighbors of point <em>x</em>, and <span class="math inline">\(y_i\)</span> is the target value of the <span class="math inline">\(i^{th}\)</span> neighbor.</p></li>
<li><p><strong>Weighted Averaging:</strong> Neighbors are weighted based on their distance to the new data point. Closer neighbors have a higher weight, contributing more to the final prediction. This can be implemented in various ways, such as using the inverse of the distance as the weight.</p>
<p><span class="math display">\[\hat{y} = \frac{\sum_{i \in N(x)} w_i y_i}{\sum_{i \in N(x)} w_i}\]</span> where <span class="math inline">\(w_i\)</span> is the weight assigned to the <span class="math inline">\(i^{th}\)</span> neighbor, and <span class="math inline">\(w_i = \frac{1}{d(x, x_i)}\)</span> is a common choice, with <span class="math inline">\(d(x, x_i)\)</span> being the distance between <em>x</em> and its <span class="math inline">\(i^{th}\)</span> neighbor.</p></li>
</ul></li>
</ol>
<p><strong>Differences Between KNN Regression and Classification</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 42%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>KNN Classification</th>
<th>KNN Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Target Variable</td>
<td>Categorical/Discrete</td>
<td>Continuous</td>
</tr>
<tr class="even">
<td>Prediction</td>
<td>Class label (e.g., “cat,” “dog”)</td>
<td>Continuous value (e.g., temperature, price)</td>
</tr>
<tr class="odd">
<td>Aggregation</td>
<td>Majority voting among neighbors</td>
<td>Averaging (simple or weighted) of neighbor values</td>
</tr>
<tr class="even">
<td>Evaluation Metrics</td>
<td>Accuracy, Precision, Recall, F1-score, AUC-ROC</td>
<td>Mean Squared Error (MSE), R-squared, MAE</td>
</tr>
</tbody>
</table>
<p><strong>Evaluation Metrics for KNN Regression:</strong></p>
<p>Since KNN regression predicts continuous values, different evaluation metrics are used compared to classification. Common metrics include:</p>
<ul>
<li><p><strong>Mean Squared Error (MSE):</strong> The average of the squared differences between the predicted and actual values.</p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i - y_i)^2\]</span></p></li>
<li><p><strong>R-squared:</strong> Represents the proportion of variance in the dependent variable that can be predicted from the independent variables. Higher R-squared values indicate a better fit.</p>
<p><span class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^{n}(\hat{y}_i - y_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\]</span></p></li>
<li><p><strong>Mean Absolute Error (MAE):</strong> The average of the absolute differences between the predicted and actual values. More robust to outliers than MSE.</p>
<p><span class="math display">\[MAE = \frac{1}{n}\sum_{i=1}^{n}|\hat{y}_i - y_i|\]</span></p></li>
</ul>
<p><strong>Considerations and Advantages:</strong></p>
<ul>
<li><p><strong>Choice of K:</strong> Selecting the optimal value for ‘k’ is crucial in both KNN classification and regression. Small values of ‘k’ can lead to noisy predictions (high variance), while large values can lead to overly smoothed predictions (high bias). Cross-validation techniques are typically used to determine the best ‘k’.</p></li>
<li><p><strong>Distance Metric:</strong> The choice of distance metric significantly impacts the performance of KNN. Consider the nature of the data and experiment with different metrics. Standardization or normalization of features is often necessary, especially when using distance metrics sensitive to feature scaling (like Euclidean distance).</p></li>
<li><p><strong>Data Preprocessing:</strong> As with any machine learning algorithm, data preprocessing is essential. Feature scaling (e.g., standardization or normalization) is particularly important for KNN, as distance calculations are sensitive to the scale of the features.</p></li>
<li><p><strong>Computational Cost:</strong> KNN can be computationally expensive, especially with large datasets, as it requires calculating distances between the new data point and all points in the training set. Approximate nearest neighbor search algorithms (e.g., using KD-trees or Ball-trees) can help mitigate this issue.</p></li>
<li><p><strong>Interpretability:</strong> KNN is relatively easy to understand and interpret. The predictions are based directly on the observed values of the nearest neighbors.</p></li>
</ul>
<p><strong>Advanced Considerations:</strong></p>
<ul>
<li><p><strong>Kernel Regression:</strong> KNN regression can be viewed as a simple form of kernel regression, where the kernel function assigns equal weight to the k-nearest neighbors and zero weight to all other points. More sophisticated kernel functions can be used to improve performance.</p></li>
<li><p><strong>Curse of Dimensionality:</strong> KNN’s performance can degrade in high-dimensional spaces due to the “curse of dimensionality.” Feature selection or dimensionality reduction techniques (e.g., PCA) can help address this issue.</p></li>
</ul>
<p>In summary, KNN can be effectively used for regression tasks by adapting the prediction mechanism to aggregate continuous target values of the nearest neighbors. The key differences compared to classification lie in the type of target variable, the aggregation method, and the evaluation metrics used. Careful consideration of the choice of ‘k’, the distance metric, and data preprocessing is essential for achieving optimal performance.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guideline for explaining KNN for regression in an interview:</p>
<ol type="1">
<li><strong>Start with the Basics:</strong>
<ul>
<li>“KNN is a non-parametric algorithm that can be used for both classification and regression. The fundamental idea is to predict the value of a new data point based on the values of its ‘k’ nearest neighbors in the training data.”</li>
</ul></li>
<li><strong>Explain KNN Regression Specifically:</strong>
<ul>
<li>“For regression, instead of predicting a class, we predict a continuous value. We find the ‘k’ nearest neighbors using a distance metric like Euclidean or Manhattan distance.” (Mention the formulas, but don’t dwell on them unless specifically asked. If you write them on a whiteboard, do so clearly.)</li>
<li>“The prediction is then obtained by averaging the target values of these neighbors. This can be a simple average, or a weighted average where closer neighbors have a greater influence on the prediction.”</li>
</ul></li>
<li><strong>Highlight the Differences from Classification:</strong>
<ul>
<li>“The main difference between KNN classification and regression lies in the type of target variable and how we aggregate the neighbors’ values. Classification deals with discrete labels, using majority voting. Regression handles continuous values, using averaging.”</li>
<li>“Consequently, we use different evaluation metrics. For classification, we use metrics like accuracy and F1-score. For regression, we use Mean Squared Error (MSE), R-squared, or Mean Absolute Error (MAE).” (Briefly explain one or two of these metrics. MSE is a good one to start with.)</li>
</ul></li>
<li><strong>Discuss Key Considerations:</strong>
<ul>
<li>“Choosing the right ‘k’ is crucial. A small ‘k’ can lead to overfitting, while a large ‘k’ can lead to underfitting. We typically use cross-validation to find the optimal ‘k’.”</li>
<li>“The choice of distance metric is also important and depends on the data. Feature scaling is usually necessary to prevent features with larger scales from dominating the distance calculations.”</li>
<li>“KNN can be computationally expensive for large datasets, as we need to calculate distances to all training points. Approximate nearest neighbor search methods can help with this.”</li>
</ul></li>
<li><strong>Advanced Points (Only if time permits or if asked):</strong>
<ul>
<li>“KNN regression can be viewed as a simple form of kernel regression. More sophisticated kernel functions could improve performance. Furthermore, KNN can suffer from the curse of dimensionality in high-dimensional spaces, so dimensionality reduction techniques can be useful.”</li>
</ul></li>
<li><strong>Engage with the Interviewer:</strong>
<ul>
<li>“Does that make sense? Would you like me to elaborate on any of these points?”</li>
<li>“Have you seen KNN used in any specific applications where you work?” (This encourages a conversation.)</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush. Explain the concepts clearly and concisely.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you have a whiteboard, draw a simple diagram to illustrate the concept of nearest neighbors and how averaging works.</li>
<li><strong>Check for Understanding:</strong> Pause periodically to ask if the interviewer has any questions.</li>
<li><strong>Focus on the Key Differences:</strong> Emphasize the differences between KNN classification and regression to demonstrate a clear understanding.</li>
<li><strong>Be Prepared to Elaborate:</strong> Be ready to dive deeper into any specific aspect of the algorithm, such as the choice of distance metric or the impact of ‘k’. If asked about dealing with very large datasets, mention KD-trees, Ball-trees or approximate nearest neighbors algorithm.</li>
<li><strong>Maintain Eye Contact:</strong> Show confidence and engagement.</li>
<li><strong>Equations:</strong> Mention equations with context, and explain the meaning of the components. This will show deep understanding without making the answer all about math. For instance, rather than just presenting the MSE formula, say, “We can evaluate the performance using Mean Squared Error, which measures the average squared difference between predicted and actual values. The formula is…(then present the formula and explain its components)”.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>