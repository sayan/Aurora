<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>gradient_boosting_6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-7.-can-you-compare-and-contrast-gradient-boosting-with-adaboost-and-random-forests-what-are-the-key-differences-in-how-these-ensemble-methods-build-and-combine-their-models" class="level2">
<h2 class="anchored" data-anchor-id="question-7.-can-you-compare-and-contrast-gradient-boosting-with-adaboost-and-random-forests-what-are-the-key-differences-in-how-these-ensemble-methods-build-and-combine-their-models">Question: 7. Can you compare and contrast gradient boosting with AdaBoost and Random Forests? What are the key differences in how these ensemble methods build and combine their models?</h2>
<p><strong>Best Answer</strong></p>
<p>Ensemble methods are powerful machine learning techniques that combine multiple base models to create a stronger, more accurate model. Gradient Boosting, AdaBoost, and Random Forests are all ensemble methods, but they differ significantly in how they build and combine their individual models. Here’s a comparison:</p>
<p><strong>1. Building the Ensemble:</strong></p>
<ul>
<li><strong>AdaBoost (Adaptive Boosting):</strong>
<ul>
<li><strong>Sequential Learning:</strong> AdaBoost builds the ensemble sequentially. Each subsequent model attempts to correct the errors of the previous models.</li>
<li><strong>Weighted Instances:</strong> It assigns weights to each training instance. Instances that are misclassified by previous models receive higher weights, forcing subsequent models to focus on these difficult instances.</li>
<li><strong>Model Weights:</strong> AdaBoost assigns weights to each model in the ensemble based on its performance on the weighted training data. Better-performing models get higher weights.</li>
<li><strong>Focus on Misclassifications:</strong> At each iteration <span class="math inline">\(t\)</span>, a weak learner <span class="math inline">\(h_t(x)\)</span> is trained on data weighted by <span class="math inline">\(w_i^{(t)}\)</span>, where <span class="math inline">\(w_i^{(t)}\)</span> is the weight of instance <span class="math inline">\(i\)</span> at iteration <span class="math inline">\(t\)</span>. The goal is to minimize the weighted error: <span class="math display">\[
\epsilon_t = \sum_{i=1}^{N} w_i^{(t)} \mathbb{I}(h_t(x_i) \neq y_i)
\]</span> where <span class="math inline">\(\mathbb{I}\)</span> is the indicator function.</li>
<li>The model’s weight <span class="math inline">\(\alpha_t\)</span> is calculated as: <span class="math display">\[
\alpha_t = \frac{1}{2} \ln \left( \frac{1 - \epsilon_t}{\epsilon_t} \right)
\]</span> The instance weights are updated as follows: <span class="math display">\[
w_i^{(t+1)} = \frac{w_i^{(t)} \exp(-\alpha_t y_i h_t(x_i))}{Z_t}
\]</span> where <span class="math inline">\(Z_t\)</span> is a normalization factor to ensure that the weights sum to 1.</li>
</ul></li>
<li><strong>Gradient Boosting:</strong>
<ul>
<li><strong>Sequential Learning:</strong> Similar to AdaBoost, Gradient Boosting also builds the ensemble sequentially.</li>
<li><strong>Gradient Descent Optimization:</strong> Instead of weighting instances, Gradient Boosting focuses on minimizing a loss function using gradient descent. Each model predicts the <em>residual errors</em> (negative gradients) made by the previous models.</li>
<li><strong>Loss Function:</strong> Gradient Boosting can optimize any differentiable loss function, making it more flexible than AdaBoost (which is typically used with exponential loss). Common loss functions include mean squared error (MSE) for regression and log loss for classification.</li>
<li><strong>Additive Model:</strong> At each stage <span class="math inline">\(t\)</span>, a weak learner <span class="math inline">\(h_t(x)\)</span> is trained to predict the negative gradient of the loss function <span class="math inline">\(L\)</span> with respect to the current model <span class="math inline">\(F_{t-1}(x)\)</span>: <span class="math display">\[
h_t(x) = \arg\min_{h} \sum_{i=1}^{N} \left[ -\frac{\partial L(y_i, F_{t-1}(x_i))}{\partial F_{t-1}(x_i)} - h(x_i) \right]^2
\]</span> The model is then updated additively: <span class="math display">\[
F_t(x) = F_{t-1}(x) + \eta h_t(x)
\]</span> where <span class="math inline">\(\eta\)</span> is the learning rate (a shrinkage factor) that controls the step size.</li>
</ul></li>
<li><strong>Random Forests:</strong>
<ul>
<li><strong>Parallel Learning:</strong> Random Forests builds multiple decision trees <em>independently</em> and in parallel.</li>
<li><strong>Bagging (Bootstrap Aggregating):</strong> It uses bagging to create multiple subsets of the training data by sampling with replacement. Each tree is trained on a different bootstrap sample.</li>
<li><strong>Random Subspace:</strong> In addition to bagging, Random Forests also uses the random subspace method (feature bagging). When building each tree, only a random subset of features is considered at each split. This further decorrelates the trees and reduces overfitting.</li>
<li>For each tree <span class="math inline">\(T_b\)</span>, a bootstrap sample <span class="math inline">\(Z^*_b\)</span> is drawn from the training data <span class="math inline">\(Z\)</span>. Each tree is grown using the CART algorithm but, at each split, only <span class="math inline">\(m\)</span> of the <span class="math inline">\(p\)</span> features are considered. The prediction for a new point <span class="math inline">\(x\)</span> is the average of the predictions of all trees: <span class="math display">\[
\hat{f}(x) = \frac{1}{B} \sum_{b=1}^{B} T_b(x)
\]</span></li>
</ul></li>
</ul>
<p><strong>2. Combining the Models:</strong></p>
<ul>
<li><strong>AdaBoost:</strong>
<ul>
<li><strong>Weighted Sum:</strong> AdaBoost combines the predictions of the weak learners through a weighted sum, where the weights are determined by the performance of each model.</li>
<li>The final prediction is: <span class="math display">\[
F(x) = \sum_{t=1}^{T} \alpha_t h_t(x)
\]</span> where <span class="math inline">\(T\)</span> is the number of weak learners.</li>
</ul></li>
<li><strong>Gradient Boosting:</strong>
<ul>
<li><strong>Additive Combination:</strong> Gradient Boosting combines the predictions of the weak learners in an additive manner, with each model contributing to the overall prediction based on the residuals it is trained to predict.</li>
<li>The final prediction is: <span class="math display">\[
F(x) = \sum_{t=1}^{T} \eta h_t(x)
\]</span> where <span class="math inline">\(\eta\)</span> is the learning rate, and <span class="math inline">\(T\)</span> is the number of weak learners.</li>
</ul></li>
<li><strong>Random Forests:</strong>
<ul>
<li><strong>Averaging (Regression) / Voting (Classification):</strong> Random Forests combines the predictions of the individual trees by averaging their outputs in regression tasks, or by taking a majority vote in classification tasks.</li>
</ul></li>
</ul>
<p><strong>3. Key Differences and Considerations:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 27%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>AdaBoost</th>
<th>Gradient Boosting</th>
<th>Random Forests</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Learning</td>
<td>Sequential, weighted instances</td>
<td>Sequential, gradient descent on residuals</td>
<td>Parallel, bagging</td>
</tr>
<tr class="even">
<td>Loss Function</td>
<td>Exponential loss (typically)</td>
<td>Flexible, any differentiable loss</td>
<td>N/A (Each tree is independent)</td>
</tr>
<tr class="odd">
<td>Model Combination</td>
<td>Weighted sum</td>
<td>Additive combination</td>
<td>Averaging (regression) / Voting (classif.)</td>
</tr>
<tr class="even">
<td>Overfitting</td>
<td>Prone to overfitting with noisy data</td>
<td>Less prone (with regularization)</td>
<td>Less prone (due to decorrelation)</td>
</tr>
<tr class="odd">
<td>Interpretability</td>
<td>Relatively interpretable (few models)</td>
<td>Less interpretable (more complex)</td>
<td>Relatively interpretable (feature importances)</td>
</tr>
<tr class="even">
<td>Robustness to Noise</td>
<td>Sensitive</td>
<td>More robust</td>
<td>More robust</td>
</tr>
<tr class="odd">
<td>Computation</td>
<td>Faster</td>
<td>Can be slower (depending on loss)</td>
<td>Faster (parallel)</td>
</tr>
</tbody>
</table>
<p><strong>4. Sensitivity to Noisy Data and Outliers:</strong></p>
<ul>
<li><strong>AdaBoost:</strong> Highly sensitive to noisy data and outliers because it tries to perfectly classify all instances, potentially leading to overfitting. The reweighting mechanism amplifies the impact of noisy instances.</li>
<li><strong>Gradient Boosting:</strong> More robust to noisy data compared to AdaBoost, especially with regularization techniques like shrinkage (learning rate) and tree pruning.</li>
<li><strong>Random Forests:</strong> Also robust due to the bagging and random subspace methods. Outliers in one bootstrap sample are less likely to significantly impact the overall ensemble.</li>
</ul>
<p><strong>5. Overfitting Tendencies:</strong></p>
<ul>
<li><strong>AdaBoost:</strong> Can overfit if the weak learners are too complex or if the number of boosting rounds is too high.</li>
<li><strong>Gradient Boosting:</strong> Less prone to overfitting than AdaBoost due to regularization techniques. Techniques like limiting tree depth, using a learning rate, and subsampling can help prevent overfitting.</li>
<li><strong>Random Forests:</strong> Less prone to overfitting than individual decision trees due to the decorrelation of the trees. The random subspace method further reduces overfitting.</li>
</ul>
<p><strong>6. Interpretability:</strong></p>
<ul>
<li><strong>AdaBoost:</strong> Easier to interpret than Gradient Boosting, especially if the number of weak learners is small. The weights assigned to each model provide some insight into their importance.</li>
<li><strong>Gradient Boosting:</strong> Can be less interpretable due to the complexity of the ensemble and the interaction between models. However, feature importance can still be estimated.</li>
<li><strong>Random Forests:</strong> Relatively interpretable. Feature importance can be easily calculated based on how much each feature reduces the impurity (e.g., Gini impurity or entropy) across all trees.</li>
</ul>
<p><strong>7. Real-World Considerations:</strong></p>
<ul>
<li><strong>Implementation Details:</strong> Libraries like scikit-learn, XGBoost, LightGBM, and CatBoost provide efficient implementations of these algorithms. XGBoost, LightGBM, and CatBoost often offer significant performance improvements over the scikit-learn implementations, especially for large datasets.</li>
<li><strong>Corner Cases:</strong>
<ul>
<li><strong>High-Dimensional Data:</strong> Random Forests often perform well in high-dimensional data due to feature bagging.</li>
<li><strong>Imbalanced Data:</strong> All three algorithms can struggle with imbalanced data. Techniques like oversampling the minority class, undersampling the majority class, or using cost-sensitive learning can help.</li>
<li><strong>Missing Data:</strong> Some implementations (e.g., XGBoost, LightGBM) can handle missing data directly.</li>
</ul></li>
</ul>
<p>In summary, AdaBoost focuses on weighting instances and models, Gradient Boosting optimizes a loss function via gradient descent, and Random Forests leverages bagging and random subspace to build a diverse set of trees. Each algorithm has its strengths and weaknesses, and the choice depends on the specific dataset and problem. Gradient Boosting, with its flexibility and regularization options, often provides the best performance in practice, while Random Forests offer a good balance of speed, accuracy, and interpretability.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><strong>Start with a brief overview (30 seconds):</strong>
<ul>
<li>“All three – Gradient Boosting, AdaBoost, and Random Forests – are powerful ensemble methods, but they differ significantly in how they build and combine their individual models. AdaBoost uses weighted instances sequentially, Gradient Boosting optimizes a loss function using gradient descent also sequentially, and Random Forests uses bagging and random subspaces in parallel.”</li>
</ul></li>
<li><strong>Explain AdaBoost (1-2 minutes):</strong>
<ul>
<li>“AdaBoost is a sequential learning algorithm. It assigns weights to training instances, focusing on misclassified instances from previous models. Each model is weighted by its performance.”</li>
<li>“Mathematically, the instance weights are updated after each iteration based on the model’s error rate. The model’s weight reflects its accuracy.”</li>
<li>“A key limitation is its sensitivity to noisy data, as it tries to perfectly fit the weighted instances.”</li>
</ul></li>
<li><strong>Explain Gradient Boosting (2-3 minutes):</strong>
<ul>
<li>“Gradient Boosting, like AdaBoost, is sequential, but it takes a different approach. It minimizes a loss function by iteratively predicting the <em>residuals</em> or negative gradients of the loss. This makes it more flexible as it can use different loss functions.”</li>
<li>“Essentially, each model learns from the errors of the previous models, correcting them step by step. We can express this process mathematically, showing how a learner fits the negative gradient and how the ensemble model is additively updated.”</li>
<li>“Gradient Boosting is generally more robust to noise than AdaBoost, especially when using regularization techniques such as learning rate (shrinkage).”</li>
</ul></li>
<li><strong>Explain Random Forests (1-2 minutes):</strong>
<ul>
<li>“Random Forests takes a completely different approach. It builds multiple decision trees <em>in parallel</em>, using bagging and random subspaces. Bagging creates different subsets of the data, and random subspaces select random subsets of features for each split.”</li>
<li>“This decorrelation of the trees reduces overfitting and makes Random Forests very robust. The predictions are then combined through averaging (regression) or voting (classification).”</li>
<li>“Random Forests are less prone to overfitting than individual trees and offer good interpretability through feature importance scores.”</li>
</ul></li>
<li><strong>Compare and Contrast (2-3 minutes):</strong>
<ul>
<li>Use the table format provided in the “Best Answer” section to summarize the key differences.</li>
<li>“In summary, AdaBoost focuses on reweighting, Gradient Boosting on optimizing a loss function via gradient descent, and Random Forests on decorrelation through bagging. AdaBoost can be sensitive to noisy data, while Gradient Boosting and Random Forests are more robust. Gradient Boosting with regularization often yields best performance, while Random Forests balances speed, accuracy and interpretability.”</li>
<li>“Also worth mentioning is that Random Forest is naturally parallel, while Gradient Boosting and AdaBoost are sequential. This difference makes Random Forest more suitable for very large datasets when combined with distributed computation.”</li>
</ul></li>
<li><strong>Discuss Real-World Considerations (1-2 minutes):</strong>
<ul>
<li>“Libraries like scikit-learn provide implementations, but XGBoost, LightGBM, and CatBoost often offer better performance, especially for larger datasets.”</li>
<li>“Consider corner cases like high-dimensional data, imbalanced data, or missing data. Each algorithm has its own strengths and weaknesses in these scenarios. Some of the popular libraries have built-in functions for imbalanced data and missing data.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to process the information.</li>
<li><strong>Check for Understanding:</strong> Pause periodically and ask if they have any questions. This shows that you are engaged and want to ensure they understand your explanation.</li>
<li><strong>Explain the Math Concisely:</strong> When presenting mathematical formulas, explain the meaning of each term and why it’s important. Avoid getting bogged down in complex derivations unless specifically asked.</li>
<li><strong>Use Visual Aids (if possible):</strong> If you have the option to use a whiteboard or share your screen, use it to illustrate the concepts and relationships between the algorithms.</li>
<li><strong>Tailor to the Audience:</strong> Be mindful of the interviewer’s background. If they are less technical, focus on the high-level concepts and avoid getting too deep into the mathematical details. If they are very technical, you can delve into more detail.</li>
<li><strong>Be Confident, but Humble:</strong> Show confidence in your knowledge, but avoid sounding arrogant. Acknowledge that these are complex topics and that there is always more to learn.</li>
<li><strong>Connect to Practical Experience:</strong> If possible, relate your explanation to your own experience using these algorithms in real-world projects. This will make your answer more engaging and demonstrate your practical skills.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>