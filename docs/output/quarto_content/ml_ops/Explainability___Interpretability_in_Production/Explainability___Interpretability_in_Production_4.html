<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>explainability___interpretability_in_production_4</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-5.-what-challenges-do-you-foresee-in-scaling-interpretability-explainability-solutions-across-a-large-complex-production-system-with-diverse-models-and-how-would-you-approach-these-challenges" class="level2">
<h2 class="anchored" data-anchor-id="question-5.-what-challenges-do-you-foresee-in-scaling-interpretability-explainability-solutions-across-a-large-complex-production-system-with-diverse-models-and-how-would-you-approach-these-challenges">Question: 5. What challenges do you foresee in scaling interpretability/ explainability solutions across a large, complex production system with diverse models, and how would you approach these challenges?</h2>
<p><strong>Best Answer</strong></p>
<p>Scaling interpretability and explainability solutions (XAI) across a large, complex production system presents a multifaceted challenge. The complexities arise from the inherent diversity of models, the computational overhead of explanation methods, and the need to maintain consistency and reliability in explanations across the entire system. Here’s a breakdown of the key challenges and a proposed approach:</p>
<p><strong>1. Challenges:</strong></p>
<ul>
<li><p><strong>Model Diversity &amp; Integration Complexity:</strong></p>
<ul>
<li>Different models (e.g., neural networks, tree-based models, linear models) require different interpretability techniques. A one-size-fits-all approach won’t work. Integrating these different techniques into a unified framework adds significant complexity.</li>
<li>Some models (e.g., black-box deep learning models) are inherently harder to interpret than others. Applying techniques like LIME or SHAP can be computationally expensive, especially in real-time settings.</li>
<li>Consider the complexity of integrating with legacy systems versus newer microservices.</li>
</ul></li>
<li><p><strong>Computational Overhead:</strong></p>
<ul>
<li>Generating explanations, especially complex ones, can add significant latency to prediction pipelines. This is a critical concern for real-time applications where low latency is essential.</li>
<li>Techniques like SHAP, which require multiple model evaluations, can become prohibitively expensive for large models or high-volume predictions.</li>
<li>The computational cost scales with the number of features and the complexity of the model. For example, calculating Shapley values has a complexity of <span class="math inline">\(O(ML)\)</span> where <span class="math inline">\(M\)</span> is the number of features and <span class="math inline">\(L\)</span> is the length of the input sample.</li>
</ul></li>
<li><p><strong>Consistency and Standardization:</strong></p>
<ul>
<li>Ensuring that explanations are consistent and reliable across different models and over time is crucial for building trust and avoiding confusion.</li>
<li>Defining standard metrics and evaluation procedures for explanation quality is essential for monitoring and improving the performance of XAI solutions.</li>
</ul></li>
<li><p><strong>Explanation Quality and Fidelity:</strong></p>
<ul>
<li>Striking a balance between explanation simplicity and fidelity to the underlying model is challenging. Overly simplified explanations may be misleading, while overly complex explanations may be incomprehensible to end-users.</li>
<li>It’s important to consider the target audience for explanations (e.g., data scientists, business users, regulators) and tailor the level of detail accordingly.</li>
</ul></li>
<li><p><strong>Data Governance and Privacy:</strong></p>
<ul>
<li>Explanations may reveal sensitive information about the data used to train the models, raising privacy concerns.</li>
<li>It’s crucial to implement appropriate data masking and anonymization techniques to protect sensitive data while still providing meaningful explanations.</li>
</ul></li>
<li><p><strong>Monitoring and Maintenance:</strong></p>
<ul>
<li>Monitoring the performance of XAI solutions over time is essential to detect and address issues such as explanation drift or degradation in quality.</li>
<li>As models evolve, explanations may need to be updated or re-trained to remain accurate and relevant.</li>
</ul></li>
<li><p><strong>Storage and Logging:</strong></p>
<ul>
<li>Storing and managing explanation logs can be challenging, especially for high-volume applications. The sheer amount of data can become unwieldy.</li>
<li>Designing an efficient storage and retrieval system for explanations is essential for auditing and debugging purposes.</li>
</ul></li>
</ul>
<p><strong>2. Proposed Approach:</strong></p>
<p>To address these challenges, I would advocate for a multi-pronged approach:</p>
<ul>
<li><p><strong>Centralized Explanation Service:</strong></p>
<ul>
<li>Develop a centralized explanation service that acts as an intermediary between the models and the users/applications requiring explanations. This service should be model-agnostic and support multiple explanation techniques.</li>
<li>This service can handle tasks such as:
<ul>
<li><strong>Explanation generation:</strong> Based on the model type and the desired explanation granularity.</li>
<li><strong>Explanation storage and retrieval:</strong> Using a dedicated explanation store (e.g., a document database or a graph database) optimized for efficient querying.</li>
<li><strong>Explanation monitoring and alerting:</strong> Tracking explanation quality metrics and alerting when issues arise.</li>
<li><strong>Access control and authorization:</strong> Ensuring that explanations are only accessible to authorized users/applications.</li>
</ul></li>
<li>The service should be designed with scalability and fault tolerance in mind, using technologies like Kubernetes and message queues.</li>
<li>Communication with the central explanation service can be done through APIs. The model simply sends the input and prediction to the explanation service which then returns the explanation.</li>
</ul></li>
<li><p><strong>Model-Specific Explanation Adapters:</strong></p>
<ul>
<li>Create model-specific adapters that translate the model’s input and output into a format that the centralized explanation service can understand.</li>
<li>These adapters would also be responsible for invoking the appropriate explanation techniques for each model type.</li>
<li>For example, an adapter for a tree-based model might use feature importance scores, while an adapter for a neural network might use LIME or SHAP.</li>
</ul></li>
<li><p><strong>Automated Explanation Logging and Auditing:</strong></p>
<ul>
<li>Implement automated logging of explanations along with relevant metadata (e.g., model version, input data, prediction).</li>
<li>This logging should be comprehensive enough to support auditing and debugging purposes.</li>
<li>Use a distributed logging system (e.g., Elasticsearch, Fluentd, Kibana) to efficiently store and analyze explanation logs.</li>
</ul></li>
<li><p><strong>Explanation Quality Monitoring and Evaluation:</strong></p>
<ul>
<li>Define metrics to quantify explanation quality (e.g., fidelity, stability, comprehensibility).</li>
<li>Implement automated monitoring of these metrics and alert when explanation quality degrades.</li>
<li>Regularly evaluate the performance of XAI solutions using A/B testing or other evaluation methods.</li>
</ul></li>
<li><p><strong>Federated Learning for Explainability:</strong></p>
<ul>
<li>Consider using federated learning principles for explainability models. This involves training explainability models on decentralized data sources without directly accessing the data.</li>
<li>It can help address data privacy concerns and enhance scalability.</li>
</ul></li>
<li><p><strong>Prioritization of Explanation Requests:</strong></p>
<ul>
<li>Implement a prioritization scheme for explanation requests to ensure that the most critical requests are processed first.</li>
<li>For example, requests for explanations of high-stakes decisions (e.g., loan applications, medical diagnoses) might be given higher priority than requests for explanations of routine decisions.</li>
</ul></li>
<li><p><strong>Asynchronous Explanation Generation:</strong></p>
<ul>
<li>For applications where latency is critical, consider generating explanations asynchronously.</li>
<li>The prediction can be returned immediately, and the explanation can be generated in the background and delivered separately.</li>
<li>This approach can significantly reduce the impact of explanation generation on prediction latency.</li>
</ul></li>
<li><p><strong>Explanation Simplification and Abstraction:</strong></p>
<ul>
<li>Develop techniques to simplify and abstract explanations to make them more understandable to end-users.</li>
<li>For example, feature importance scores can be aggregated into higher-level categories or presented visually.</li>
<li>Consider using natural language explanations to describe the reasoning behind the model’s predictions.</li>
</ul></li>
<li><p><strong>Resource Allocation and Optimization:</strong></p>
<ul>
<li>Dynamically allocate resources to the explanation service based on demand.</li>
<li>Optimize the performance of explanation techniques to reduce computational overhead.</li>
<li>Consider using hardware acceleration (e.g., GPUs) to speed up explanation generation.</li>
</ul></li>
<li><p><strong>Continuous Learning and Improvement:</strong></p>
<ul>
<li>Continuously monitor the performance of XAI solutions and identify areas for improvement.</li>
<li>Experiment with new explanation techniques and evaluate their effectiveness.</li>
<li>Incorporate feedback from users to improve the quality and usability of explanations.</li>
</ul></li>
</ul>
<p><strong>Mathematical Notation and Formulas</strong></p>
<ul>
<li><p><strong>SHAP (SHapley Additive exPlanations):</strong> SHAP values decompose a prediction to show the impact of each feature. The Shapley value for a feature <span class="math inline">\(i\)</span> is calculated as:</p>
<p><span class="math display">\[\phi_i = \sum_{S \subseteq M \setminus \{i\}} \frac{|S|!(|M| - |S| - 1)!}{|M|!} [f_x(S \cup \{i\}) - f_x(S)]\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(M\)</span> is the set of all features.</li>
<li><span class="math inline">\(S\)</span> is a subset of features excluding feature <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(|S|\)</span> is the number of features in the subset <span class="math inline">\(S\)</span>.</li>
<li><span class="math inline">\(f_x(S)\)</span> is the prediction of the model using only the features in subset <span class="math inline">\(S\)</span> (setting the other features to a baseline value).</li>
<li><span class="math inline">\(f_x(S \cup \{i\})\)</span> is the prediction of the model using the features in subset <span class="math inline">\(S\)</span> and feature <span class="math inline">\(i\)</span>.</li>
</ul></li>
<li><p><strong>LIME (Local Interpretable Model-agnostic Explanations):</strong> LIME approximates the model locally with a linear model. The explanation is the feature weights of this linear model. The objective function to minimize is:</p>
<p><span class="math display">\[\xi(x) = \arg\min_{g \in G} \mathcal{L}(f, g, \pi_x) + \Omega(g)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x\)</span> is the instance to be explained.</li>
<li><span class="math inline">\(f\)</span> is the original model.</li>
<li><span class="math inline">\(g\)</span> is the interpretable model (e.g., a linear model).</li>
<li><span class="math inline">\(G\)</span> is the space of interpretable models.</li>
<li><span class="math inline">\(\mathcal{L}\)</span> is a loss function measuring how well <span class="math inline">\(g\)</span> approximates <span class="math inline">\(f\)</span> in the neighborhood of <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(\pi_x\)</span> is a proximity measure defining the neighborhood around <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(\Omega(g)\)</span> is a complexity penalty for the interpretable model <span class="math inline">\(g\)</span>.</li>
</ul></li>
<li><p><strong>Fidelity:</strong> Fidelity measures how well the explanation aligns with the model’s prediction. A common measure is the <span class="math inline">\(R^2\)</span> score between the original model’s output and the local approximation’s output: <span class="math display">\[Fidelity = R^2(f(x), g(x))\]</span> Where:</p>
<ul>
<li><span class="math inline">\(f(x)\)</span> is the output of the original model for input <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(g(x)\)</span> is the output of the explanation model for input <span class="math inline">\(x\)</span>.</li>
</ul></li>
</ul>
<p><strong>Real-World Considerations:</strong></p>
<ul>
<li><strong>Regulatory Compliance:</strong> In regulated industries (e.g., finance, healthcare), XAI is often a legal requirement. Ensure that the chosen XAI solutions meet the specific regulatory requirements for the relevant industry.</li>
<li><strong>User Interface/User Experience (UI/UX):</strong> The explanations should be presented in a clear and intuitive way to end-users. Consider the target audience and tailor the UI/UX accordingly.</li>
<li><strong>Security:</strong> Secure the explanation service and protect it from unauthorized access. Implement appropriate authentication and authorization mechanisms.</li>
<li><strong>Cost:</strong> Consider the cost of implementing and maintaining XAI solutions. Balance the benefits of XAI with the associated costs.</li>
<li><strong>Experimentation:</strong> Run A/B tests to determine the effectiveness of different explanation methods and presentation styles.</li>
</ul>
<p>By addressing these challenges and implementing the proposed approach, it is possible to scale interpretability and explainability solutions effectively across a large, complex production system, leading to more transparent, trustworthy, and accountable AI.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong></p>
<ul>
<li>“Scaling XAI in a large production system is complex, stemming from model diversity, computational costs, the need for consistency, data privacy concerns, and ongoing maintenance. A comprehensive approach is needed.”</li>
</ul></li>
<li><p><strong>Discuss the Challenges (Prioritize Key Ones):</strong></p>
<ul>
<li>“One key challenge is <em>model diversity</em>. Different models require different XAI techniques. A centralized service is needed.” (Emphasize the problem and hint at the solution).</li>
<li>“Another significant hurdle is <em>computational overhead</em>. Techniques like SHAP can be expensive, so asynchronous processing and resource optimization are crucial.” (Mention specific techniques and their limitations).</li>
<li>” <em>Data governance and privacy</em> are paramount. We need to ensure that explanations don’t reveal sensitive information.”</li>
<li>“Finally, we need to continuously <em>monitor and maintain</em> our XAI solutions to ensure their accuracy and reliability.”</li>
</ul></li>
<li><p><strong>Introduce the Centralized Explanation Service:</strong></p>
<ul>
<li>“To address these challenges, my proposed solution centers around a centralized explanation service. This service acts as a model-agnostic layer, decoupling models from specific explanation techniques.” (Clearly state the core solution).</li>
</ul></li>
<li><p><strong>Explain the Components:</strong></p>
<ul>
<li>“<em>Model-Specific Adapters</em>: These translate model inputs/outputs into a standardized format for the service. Think of them as the ‘glue’ between the models and the explanation service.”</li>
<li>“<em>Automated Logging</em>: We need robust logging of explanations and metadata for auditing and debugging.”</li>
<li>“<em>Quality Monitoring</em>: Defining metrics to track explanation quality, such as Fidelity, is essential.”</li>
</ul></li>
<li><p><strong>Handle Mathematical Notation Carefully:</strong></p>
<ul>
<li>“Techniques like SHAP and LIME are powerful but can be computationally intensive. For example, SHAP values involve calculating the marginal contribution of each feature across all possible feature subsets…” (Mention the concept without diving into the full equation unless prompted. Have a simplified explanation ready).</li>
<li>“The computational complexity of Shapley values is O(ML), where M is the number of features and L is the length of the input sample. This is important for when you have a high dimensional dataset”</li>
<li>“LIME approximates the model locally with a linear model. The objective is to minimize the loss between the original model and the local linear approximation.”</li>
</ul></li>
<li><p><strong>Highlight Real-World Considerations:</strong></p>
<ul>
<li>“Regulatory compliance is crucial in many industries. We need to ensure our XAI solutions meet legal requirements.”</li>
<li>“UI/UX is also critical. The explanations must be understandable and actionable for the target audience.”</li>
<li>“Security is non-negotiable. We need to protect the explanation service and the data it processes.”</li>
</ul></li>
<li><p><strong>Conclude with Impact:</strong></p>
<ul>
<li>“By implementing this approach, we can scale XAI effectively, fostering transparency, trust, and accountability in our AI systems.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush. Speak clearly and deliberately.</li>
<li><strong>Use Visual Aids (if allowed):</strong> A diagram illustrating the centralized service architecture would be beneficial.</li>
<li><strong>Check for Understanding:</strong> Pause after explaining a complex concept and ask, “Does that make sense?”</li>
<li><strong>Be Prepared to Simplify:</strong> If the interviewer seems lost, offer a simpler explanation. For example, “SHAP essentially tells us how much each feature contributed to the final prediction.”</li>
<li><strong>Enthusiasm:</strong> Show genuine interest in the topic. Your passion will be contagious.</li>
<li><strong>Adapt to the Interviewer:</strong> Pay attention to their cues and tailor your response accordingly. If they interrupt with a question, address it directly and then return to your planned answer.</li>
<li><strong>Confidence:</strong> Even if you’re not 100% sure about something, present your answer with confidence. It is better to convey understanding even if something is not 100% accurate.</li>
</ul>
<p>By following these guidelines, you can effectively articulate your knowledge of scaling XAI in a large production system and impress the interviewer with your expertise.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>