<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>explainability___interpretability_in_production_3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-4.-in-a-real-world-production-environment-data-can-be-messy-or-evolving.-how-would-you-ensure-that-the-explainability-tools-remain-reliable-and-robust-in-the-face-of-data-quality-issues-and-distributional-shifts" class="level2">
<h2 class="anchored" data-anchor-id="question-4.-in-a-real-world-production-environment-data-can-be-messy-or-evolving.-how-would-you-ensure-that-the-explainability-tools-remain-reliable-and-robust-in-the-face-of-data-quality-issues-and-distributional-shifts">Question: 4. In a real-world production environment, data can be messy or evolving. How would you ensure that the explainability tools remain reliable and robust in the face of data quality issues and distributional shifts?</h2>
<p><strong>Best Answer</strong></p>
<p>Explainability and interpretability are critical for deploying machine learning models in production, especially in regulated industries or high-stakes decision-making scenarios. However, the reliability of these tools can be significantly compromised by data quality issues and distributional shifts, both of which are common in real-world settings. Here’s a comprehensive approach to ensure that explainability tools remain robust under such conditions:</p>
<p><strong>1. Data Quality Monitoring and Preprocessing:</strong></p>
<ul>
<li><strong>Comprehensive Data Validation:</strong> Implement rigorous data validation checks at the ingestion stage. This includes:
<ul>
<li><strong>Type Checking:</strong> Ensuring that data types match the expected schema (e.g., numerical columns contain numbers, categorical columns contain valid categories).</li>
<li><strong>Range Checks:</strong> Verifying that numerical values fall within acceptable ranges. For example, age should be positive and within a plausible limit.</li>
<li><strong>Missing Value Analysis:</strong> Monitoring the proportion of missing values for each feature and flagging anomalies.</li>
<li><strong>Cardinality Checks:</strong> Tracking the number of unique values in categorical features to detect unexpected changes.</li>
<li><strong>Custom Rules:</strong> Enforcing business-specific rules to ensure data integrity. For instance, “transaction amount cannot be negative.”</li>
</ul></li>
<li><strong>Data Cleaning and Imputation:</strong> Employ robust data cleaning techniques to handle missing values, outliers, and inconsistencies.
<ul>
<li><strong>Missing Value Imputation:</strong> Choose appropriate imputation methods based on the nature of the missing data (e.g., mean/median imputation for numerical data, mode imputation for categorical data, or more sophisticated methods like k-NN imputation). Document the chosen methods and rationale.</li>
<li><strong>Outlier Handling:</strong> Implement outlier detection techniques (e.g., Z-score, IQR-based methods, clustering-based methods) and apply appropriate transformations or removal strategies.</li>
<li><strong>Data Transformation:</strong> Use transformations like scaling (e.g., StandardScaler, MinMaxScaler) and encoding (e.g., OneHotEncoder, OrdinalEncoder) to ensure that data is in a suitable format for both the model and the explainability tools.</li>
</ul></li>
<li><strong>Data Profiling:</strong> Use data profiling tools to automatically analyze data characteristics and detect anomalies. This can help identify unexpected changes in data distributions or data quality issues.</li>
</ul>
<p><strong>2. Monitoring for Distributional Shifts:</strong></p>
<ul>
<li><strong>Statistical Distance Metrics:</strong> Continuously monitor for distributional shifts using statistical distance metrics like:
<ul>
<li><strong>Kolmogorov-Smirnov (KS) Test:</strong> For comparing the distributions of numerical features. The KS statistic, <span class="math inline">\(D\)</span>, quantifies the maximum distance between the cumulative distribution functions (CDFs) of two samples: <span class="math display">\[D = \sup_x |CDF_1(x) - CDF_2(x)|\]</span></li>
<li><strong>Chi-squared Test:</strong> For comparing the distributions of categorical features. The chi-squared statistic is calculated as: <span class="math display">\[\chi^2 = \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i}\]</span> where <span class="math inline">\(O_i\)</span> is the observed frequency and <span class="math inline">\(E_i\)</span> is the expected frequency for category <span class="math inline">\(i\)</span>.</li>
<li><strong>Population Stability Index (PSI):</strong> A commonly used metric in credit risk to measure the shift in the distribution of a variable. It’s calculated as: <span class="math display">\[PSI = \sum_{i=1}^{N} (Actual\%_i - Expected\%_i) * ln(\frac{Actual\%_i}{Expected\%_i})\]</span> where <span class="math inline">\(Actual\%_i\)</span> is the percentage of observations in bucket <span class="math inline">\(i\)</span> in the new dataset, and <span class="math inline">\(Expected\%_i\)</span> is the percentage of observations in bucket <span class="math inline">\(i\)</span> in the original dataset.</li>
</ul></li>
<li><strong>Drift Detection Algorithms:</strong> Implement drift detection algorithms (e.g., ADWIN, Page-Hinkley) to automatically detect changes in the model’s input data distribution or output predictions. ADWIN (Adaptive Windowing) maintains a sliding window of data and detects change by comparing the means of two sub-windows. The Page-Hinkley test detects changes in the mean of a distribution by monitoring the cumulative sum of deviations from an expected value.</li>
<li><strong>Monitoring Model Performance:</strong> Track key performance metrics (e.g., accuracy, precision, recall, AUC) and flag significant drops in performance, as this can be an indicator of distributional shifts.</li>
</ul>
<p><strong>3. Robust Explainability Techniques:</strong></p>
<ul>
<li><strong>Model-Agnostic Methods:</strong> Prefer model-agnostic explainability methods (e.g., SHAP, LIME) over model-specific methods when possible, as they are generally more robust to changes in the underlying model architecture.</li>
<li><strong>SHAP (SHapley Additive exPlanations):</strong> SHAP values assign each feature an importance value for a particular prediction. They are based on game-theoretic Shapley values and provide a consistent and locally accurate explanation. For a feature <span class="math inline">\(i\)</span>, the SHAP value <span class="math inline">\(\phi_i\)</span> is calculated as: <span class="math display">\[\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F| - |S| - 1)!}{|F|!} (f(S \cup \{i\}) - f(S))\]</span> where <span class="math inline">\(F\)</span> is the set of all features, and <span class="math inline">\(f(S)\)</span> is the model’s prediction using only the features in the subset <span class="math inline">\(S\)</span>.</li>
<li><strong>LIME (Local Interpretable Model-Agnostic Explanations):</strong> LIME approximates the model locally with a simpler, interpretable model (e.g., a linear model). For a given instance <span class="math inline">\(x\)</span>, LIME generates a set of perturbed instances around <span class="math inline">\(x\)</span>, obtains predictions for these instances, and then learns a weighted linear model that approximates the original model’s behavior in the neighborhood of <span class="math inline">\(x\)</span>.</li>
<li><strong>Feature Importance Stability:</strong> Monitor the stability of feature importances over time. Significant fluctuations in feature importances may indicate that the explainability tool is being influenced by data quality issues or distributional shifts.</li>
</ul>
<p><strong>4. Recalibration and Retraining of Explainability Models:</strong></p>
<ul>
<li><strong>Periodic Recalibration:</strong> Regularly recalibrate the explainability models using recent data to ensure that they are aligned with the current data distribution.</li>
<li><strong>Retraining Triggers:</strong> Define triggers for retraining the explainability models based on distributional shift detection or performance degradation.</li>
<li><strong>A/B Testing:</strong> When recalibrating or retraining explainability models, conduct A/B tests to compare the performance and stability of the new models against the existing models.</li>
</ul>
<p><strong>5. Monitoring Explainability Outputs:</strong></p>
<ul>
<li><strong>Explanation Quality Metrics:</strong> Develop metrics to assess the quality and consistency of the explanations generated by the explainability tools. These metrics can include:
<ul>
<li><strong>Explanation Stability:</strong> Measuring how consistent the explanations are for similar instances.</li>
<li><strong>Explanation Plausibility:</strong> Evaluating whether the explanations align with domain knowledge and human intuition.</li>
<li><strong>Explanation Coverage:</strong> Assessing the proportion of predictions that can be adequately explained.</li>
</ul></li>
<li><strong>Human-in-the-Loop Validation:</strong> Involve domain experts in the validation of the explanations. This can help identify spurious correlations or misleading explanations that may arise due to data quality issues or distributional shifts.</li>
<li><strong>Alerting and Anomaly Detection:</strong> Set up alerts to notify stakeholders when the explanation quality metrics fall below a certain threshold or when anomalies are detected in the explanations.</li>
</ul>
<p><strong>6. Handling Noisy or Incomplete Data within Explainability Methods:</strong></p>
<ul>
<li><strong>Robust Feature Selection:</strong> Use feature selection techniques that are robust to noise and outliers. Techniques like L1 regularization (Lasso) can help identify the most important features while minimizing the impact of noisy features.</li>
<li><strong>Ensemble Methods:</strong> Employ ensemble methods for explainability. By aggregating the explanations from multiple models or multiple runs of the same model, you can reduce the variance and improve the robustness of the explanations.</li>
<li><strong>Regularization:</strong> Apply regularization techniques to the explainability models themselves. This can help prevent overfitting to noisy data and improve the generalization performance of the explanations.</li>
</ul>
<p><strong>7. Communicating Uncertainty:</strong></p>
<ul>
<li><strong>Confidence Intervals:</strong> Provide confidence intervals or uncertainty estimates for the feature importances or other explanation outputs. This can help users understand the reliability of the explanations and avoid over-interpreting them.</li>
<li><strong>Disclaimers:</strong> Clearly communicate the limitations of the explainability tools and the potential impact of data quality issues and distributional shifts.</li>
<li><strong>Transparency:</strong> Be transparent about the data preprocessing steps and the assumptions made by the explainability methods.</li>
</ul>
<p><strong>8. Documentation and Governance:</strong></p>
<ul>
<li><strong>Detailed Documentation:</strong> Maintain detailed documentation of the data quality monitoring procedures, the explainability methods used, and the validation processes.</li>
<li><strong>Governance Framework:</strong> Establish a governance framework to ensure that the explainability tools are used responsibly and ethically. This framework should include guidelines for interpreting the explanations, addressing potential biases, and mitigating the risks associated with relying on the explanations for decision-making.</li>
</ul>
<p>By implementing these strategies, organizations can ensure that their explainability tools remain reliable and robust in the face of data quality issues and distributional shifts, enabling them to make informed decisions based on trustworthy and understandable insights.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested approach for verbally delivering this answer during an interview:</p>
<ol type="1">
<li><p><strong>Start with the Importance of Explainability:</strong></p>
<ul>
<li>“Explainability is crucial for deploying ML models, especially in regulated industries or high-stakes scenarios.”</li>
</ul></li>
<li><p><strong>Acknowledge the Challenges:</strong></p>
<ul>
<li>“However, data quality issues and distributional shifts can severely impact the reliability of explainability tools, which are common in real-world deployments.”</li>
</ul></li>
<li><p><strong>Outline the Key Strategies (High-Level):</strong></p>
<ul>
<li>“To address this, I would focus on several key areas: data quality monitoring, distributional shift detection, robust explainability techniques, recalibration, and continuous monitoring of explainability outputs.”</li>
</ul></li>
<li><p><strong>Dive Deeper into Data Quality Monitoring:</strong></p>
<ul>
<li>“First, a robust data validation process is critical. This includes type checking, range checks, missing value analysis, and custom business rules. For example, we’d ensure numerical columns are numbers, age is within reasonable bounds, and transaction amounts can’t be negative.”</li>
<li>“We’d also employ data cleaning techniques like imputation for missing values and outlier handling. The choice of imputation method, whether mean, median, or k-NN, depends on the nature of the missing data.”</li>
</ul></li>
<li><p><strong>Explain Distributional Shift Monitoring (With Examples):</strong></p>
<ul>
<li>“Next, we need to monitor for distributional shifts. We can use statistical distance metrics such as the Kolmogorov-Smirnov (KS) test for numerical features and the Chi-squared test for categorical features. For example, the KS test measures the maximum difference between the cumulative distribution functions of two datasets.</li>
<li>“Alternatively, the Population Stability Index (PSI) is also very useful. We would also track model performance metrics, as a significant drop could indicate a shift.”</li>
<li>“Drift detection algorithms like ADWIN can also be implemented to automatically detect changes.”</li>
</ul></li>
<li><p><strong>Discuss Robust Explainability Techniques:</strong></p>
<ul>
<li>“For explainability, I prefer model-agnostic methods like SHAP and LIME. SHAP values, based on Shapley values from game theory, assign each feature an importance value for a specific prediction. LIME, on the other hand, approximates the model locally with a simpler, interpretable model.”</li>
<li>“We would also monitor the stability of feature importances over time, looking for significant fluctuations that might indicate issues.”</li>
</ul></li>
<li><p><strong>Address Recalibration and Monitoring of Explainability Outputs:</strong></p>
<ul>
<li>“Recalibrating the explainability models periodically with recent data is essential. We’d define triggers for retraining based on distributional shift detection or performance degradation.”</li>
<li>“We’d also develop metrics to assess the quality and consistency of explanations, involving human experts in the validation process to catch any spurious correlations.”</li>
</ul></li>
<li><p><strong>Mention Handling Noisy Data:</strong></p>
<ul>
<li>“Within the explainability methods, we can use robust feature selection techniques like L1 regularization to minimize the impact of noisy features. Ensemble methods for explainability can also help reduce variance and improve robustness.”</li>
</ul></li>
<li><p><strong>Highlight Communication of Uncertainty:</strong></p>
<ul>
<li>“Finally, it’s crucial to communicate the uncertainty associated with the explanations. Providing confidence intervals or disclaimers can help users understand the limitations and avoid over-interpreting the results.”</li>
</ul></li>
<li><p><strong>Emphasize Documentation and Governance:</strong></p>
<ul>
<li>“All these procedures need to be documented and governed to ensure responsible and ethical use of the explainability tools.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Explain complex concepts like KS test or SHAP values at a moderate pace, ensuring the interviewer can follow along.</li>
<li><strong>Use Examples:</strong> Illustrate the techniques with real-world examples to make the concepts more relatable.</li>
<li><strong>Check for Understanding:</strong> Pause periodically to ask if the interviewer has any questions.</li>
<li><strong>Be Confident:</strong> Project confidence in your knowledge, but also acknowledge the limitations of explainability tools.</li>
<li><strong>Summarize:</strong> Briefly summarize the key strategies at the end to reinforce the main points.</li>
<li><strong>Offer to Elaborate:</strong> Invite the interviewer to delve deeper into any specific area they find interesting. For example, “I’ve briefly covered distributional shift detection; I can elaborate on specific algorithms like ADWIN if you’d like.”</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>