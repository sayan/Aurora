<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>explainability___interpretability_in_production_0</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-1.-can-you-explain-the-difference-between-explainability-and-interpretability-in-the-context-of-machine-learning-models-deployed-in-production" class="level2">
<h2 class="anchored" data-anchor-id="question-1.-can-you-explain-the-difference-between-explainability-and-interpretability-in-the-context-of-machine-learning-models-deployed-in-production">Question: 1. Can you explain the difference between explainability and interpretability in the context of machine learning models deployed in production?</h2>
<p><strong>Best Answer</strong></p>
<p>In the realm of machine learning, particularly when deploying models in production environments, understanding the distinction between explainability and interpretability is crucial for building trust, ensuring accountability, and complying with regulations. While the terms are often used interchangeably, they represent distinct concepts.</p>
<ul>
<li><p><strong>Interpretability:</strong></p>
<ul>
<li><strong>Definition:</strong> Interpretability refers to the degree to which a human can understand the cause-and-effect relationships captured by a machine learning model. It is an intrinsic property of the model itself. A model is interpretable if its decision-making process is transparent and easily understood by humans.</li>
<li><strong>Characteristics:</strong> High interpretability often comes from using simpler models, such as linear regression, logistic regression, decision trees (with limited depth), or rule-based systems. The inherent structure of these models allows for direct inspection and comprehension of how input features influence predictions.</li>
<li><strong>Example:</strong> In a linear regression model, <span class="math inline">\(y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n\)</span>, the coefficients <span class="math inline">\(\beta_i\)</span> directly indicate the impact of each feature <span class="math inline">\(x_i\)</span> on the predicted outcome <span class="math inline">\(y\)</span>. A positive <span class="math inline">\(\beta_i\)</span> implies a positive relationship, and the magnitude of <span class="math inline">\(\beta_i\)</span> reflects the strength of that relationship.</li>
<li><strong>Mathematical Representation:</strong> For instance, consider a logistic regression model: <span class="math display">\[P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2)}}\]</span> Here, the log-odds are a linear combination of the input features, and the coefficients <span class="math inline">\(\beta_i\)</span> can be interpreted as the change in the log-odds for a one-unit change in <span class="math inline">\(X_i\)</span>.</li>
</ul></li>
<li><p><strong>Explainability:</strong></p>
<ul>
<li><strong>Definition:</strong> Explainability, on the other hand, is the extent to which the reasons behind a model’s decision can be understood. It focuses on providing post-hoc explanations for specific predictions or behaviors of a model, even if the model itself is a black box. Explainability techniques are often used to shed light on the decision-making process of complex models that lack inherent interpretability.</li>
<li><strong>Characteristics:</strong> Explainability techniques are model-agnostic or model-specific methods used to approximate or interpret the model’s behavior. Examples include LIME (Local Interpretable Model-agnostic Explanations), SHAP (SHapley Additive exPlanations), attention mechanisms in neural networks, and rule extraction methods.</li>
<li><strong>Example:</strong> SHAP values quantify the contribution of each feature to a particular prediction compared to the average prediction. If a model predicts a high credit risk for a customer, SHAP values can identify which features (e.g., income, credit history) contributed most to that prediction.</li>
<li><strong>Mathematical Representation:</strong> SHAP values are based on game theory. The SHAP value for feature <span class="math inline">\(i\)</span> is calculated as the average marginal contribution of feature <span class="math inline">\(i\)</span> across all possible feature coalitions: <span class="math display">\[\phi_i = \sum_{S \subseteq N\setminus\{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} (f_{S \cup \{i\}}(x_{S \cup \{i\}}) - f_S(x_S))\]</span> where:
<ul>
<li><span class="math inline">\(N\)</span> is the set of all features.</li>
<li><span class="math inline">\(S\)</span> is a subset of features not including feature <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(f_{S \cup \{i\}}(x_{S \cup \{i\}})\)</span> is the model’s prediction with feature <span class="math inline">\(i\)</span> and the features in <span class="math inline">\(S\)</span>.</li>
<li><span class="math inline">\(f_S(x_S)\)</span> is the model’s prediction with only the features in <span class="math inline">\(S\)</span>.</li>
<li><span class="math inline">\(\phi_i\)</span> is the Shapley value of feature <span class="math inline">\(i\)</span></li>
</ul></li>
</ul></li>
<li><p><strong>Key Differences &amp; Trade-offs:</strong></p>
<ul>
<li><strong>Intrinsic vs.&nbsp;Post-hoc:</strong> Interpretability is an intrinsic property, whereas explainability is achieved through post-hoc methods.</li>
<li><strong>Model Complexity vs.&nbsp;Transparency:</strong> There’s often a trade-off between model complexity (and potentially accuracy) and interpretability. Complex models like deep neural networks often achieve higher accuracy but are inherently less interpretable than simpler models.</li>
<li><strong>Scope:</strong> Interpretability provides a global understanding of the model, while explainability can provide both global (model-level) and local (instance-level) insights.</li>
<li><strong>Use Cases:</strong>
<ul>
<li><strong>High-stakes decisions:</strong> In applications like medical diagnosis or loan approvals, where transparency is crucial, interpretable models might be preferred, even if they sacrifice some accuracy.</li>
<li><strong>Model debugging:</strong> Explainability techniques are useful for debugging and identifying biases in complex models, even if the models themselves are not inherently interpretable.</li>
<li><strong>Regulatory compliance:</strong> Regulations like GDPR often require explanations for automated decisions, making explainability techniques essential.</li>
</ul></li>
</ul></li>
<li><p><strong>Examples in Production:</strong></p>
<ul>
<li><strong>Fraud Detection:</strong> A simple decision tree might be used for initial fraud detection due to its interpretability, allowing analysts to easily understand the rules triggering flags. However, a more complex model like a Gradient Boosted Machine might be employed in conjunction with SHAP values to explain individual fraud alerts, providing justification for further investigation.</li>
<li><strong>Credit Risk Assessment:</strong> Logistic regression is often used due to its interpretability. The coefficients associated with each feature (e.g., income, credit history) directly indicate their influence on the credit risk score. Explainability methods like LIME can provide individual explanations, showing which factors most influenced a particular credit decision.</li>
<li><strong>Recommender Systems:</strong> While collaborative filtering models can be highly accurate, they can also be black boxes. Explainability methods like feature importance or rule extraction can help explain why a particular item was recommended to a user, improving user trust.</li>
</ul></li>
</ul>
<p>In summary, interpretability and explainability are distinct but complementary concepts. Interpretability is the inherent ability of a model to be understood, while explainability is the ability to provide reasons for a model’s decisions. The choice between prioritizing interpretability or explainability (or both) depends on the specific application, the complexity of the model, and the need for transparency, accountability, and regulatory compliance. <strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to deliver this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a clear distinction:</strong> “The terms explainability and interpretability are often used interchangeably in machine learning, but they represent distinct concepts, especially when we consider deploying models in production.”</p></li>
<li><p><strong>Define Interpretability:</strong> “Interpretability refers to the degree to which a human can understand the cause-and-effect relationships learned by a model. It’s an intrinsic property of the model itself. Think of it as how transparent and understandable the model’s decision-making process is.” Provide examples of interpretable models: “For example, linear regression or a shallow decision tree are inherently interpretable.”</p></li>
<li><p><strong>Provide a simple equation example (Linear Regression):</strong> “Consider a simple linear regression: <span class="math inline">\(y = \beta_0 + \beta_1x_1 + \beta_2x_2\)</span>. Each <span class="math inline">\(\beta\)</span> coefficient directly tells you the impact of its corresponding feature. I can easily see the influence of each feature on the output.”</p></li>
<li><p><strong>Define Explainability:</strong> “Explainability, on the other hand, focuses on providing reasons <em>after</em> the model has made a prediction. It’s about understanding <em>why</em> a model made a specific decision, even if the model is complex or a ‘black box’.”</p></li>
<li><p><strong>Give examples of Explainability techniques:</strong> “Techniques like SHAP values or LIME are used to explain the predictions of complex models. These methods help us understand which features were most important for a specific prediction.”</p></li>
<li><p><strong>Explain SHAP (at a high level, don’t dive too deep into the math unless prompted):</strong> “SHAP values, for instance, quantify the contribution of each feature to a prediction relative to the average prediction. So, if a customer is denied a loan, SHAP values can tell us which factors like low income or bad credit history contributed the most to that decision.”</p></li>
<li><p><strong>Highlight Key Differences/Tradeoffs:</strong> “The key difference is that interpretability is an inherent model property, while explainability is achieved through post-hoc methods. There’s often a trade-off between model complexity and interpretability. More complex models might be more accurate but harder to understand.”</p></li>
<li><p><strong>Mention the importance of both based on the task:</strong> “The right choice between these will depend on what you are trying to accomplish. For example, regulatory requirements might force you to chose a more interpretable model that satisfies legal constraints over a less interpretable model that performs better. Sometimes you can combine the two. Using a highly performant black box model, combined with explainability methods to understand and debug the model, is a common approach.”</p></li>
<li><p><strong>Discuss examples from production (1-2 max):</strong> “In a real-world fraud detection system, you might use a simple decision tree for initial screening because its rules are easy to understand. Then, you could use a more complex model with SHAP values to explain individual fraud alerts, justifying further investigation.”</p></li>
<li><p><strong>Regulatory Example (GDPR):</strong> “A good example of the importance of this in the real world are regulations like GDPR. These often require automated decisions to provide explanations, making the field of explainability critical.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush. Explain each concept clearly and concisely.</li>
<li><strong>Use analogies:</strong> Compare the concepts to real-world scenarios to make them more relatable.</li>
<li><strong>Check for understanding:</strong> Pause occasionally and ask the interviewer if they have any questions. “Does that distinction make sense?”</li>
<li><strong>Avoid jargon:</strong> While you should demonstrate technical expertise, avoid overly complex jargon that might confuse the interviewer.</li>
<li><strong>Focus on practical applications:</strong> Emphasize how these concepts apply to real-world problems and production systems.</li>
<li><strong>If they ask you to dive into the mathematics:</strong> If the interviewer asks for more detail on the mathematics behind SHAP values, briefly explain the concept of Shapley values from game theory and how they are used to fairly distribute the contribution of each feature. However, avoid getting bogged down in the complex calculations unless specifically requested.</li>
</ul>
<p>By following these steps and communicating clearly, you can effectively demonstrate your understanding of interpretability and explainability and impress the interviewer with your expertise.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>