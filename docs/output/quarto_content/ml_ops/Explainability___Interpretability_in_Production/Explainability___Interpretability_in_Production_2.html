<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>explainability___interpretability_in_production_2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-3.-explain-how-you-would-incorporate-interpretability-and-explainability-considerations-during-the-development-and-deployment-stages-of-a-production-ml-system.-what-metrics-or-tools-would-you-monitor" class="level2">
<h2 class="anchored" data-anchor-id="question-3.-explain-how-you-would-incorporate-interpretability-and-explainability-considerations-during-the-development-and-deployment-stages-of-a-production-ml-system.-what-metrics-or-tools-would-you-monitor">Question: 3. Explain how you would incorporate interpretability and explainability considerations during the development and deployment stages of a production ML system. What metrics or tools would you monitor?</h2>
<p><strong>Best Answer</strong></p>
<p>Interpretability and explainability are crucial for building trust, ensuring fairness, and complying with regulations in production ML systems. They are not just “nice-to-haves” but integral parts of the development lifecycle, from initial design to continuous monitoring. Here’s how I would approach incorporating them:</p>
<p><strong>1. Development Stage:</strong></p>
<ul>
<li><strong>Problem Framing and Requirements Gathering:</strong>
<ul>
<li><strong>Define Explainability Goals:</strong> Before any model building begins, explicitly define <em>why</em> explainability is important for this specific use case. Are we trying to increase user trust? Comply with regulations? Debug model behavior? This drives the choice of techniques.</li>
<li><strong>Stakeholder Alignment:</strong> Involve stakeholders (product managers, legal, end-users) early on to understand their interpretability needs. What kind of explanations are most useful to them? Do they need global explanations of the model’s behavior, or local explanations for individual predictions? Understanding these requirements is key.</li>
</ul></li>
<li><strong>Data Understanding and Feature Engineering:</strong>
<ul>
<li><strong>Prioritize Interpretable Features:</strong> Favor features that have inherent meaning and are easily understood. For example, using a customer’s age directly is more interpretable than a complex interaction term involving age and income.</li>
<li><strong>Careful Feature Engineering:</strong> If complex feature transformations are necessary, document them thoroughly and consider providing “inverse” transformations that map transformed feature values back to their original meaning.</li>
<li><strong>Feature Importance Analysis (pre-modeling):</strong> Use simple techniques like correlation analysis or univariate feature importance to understand the relationship between input features and the target variable <em>before</em> training complex models. This provides a baseline understanding.</li>
</ul></li>
<li><strong>Model Selection and Training:</strong>
<ul>
<li><strong>Consider Inherently Interpretable Models:</strong> Linear models (logistic regression, linear regression), decision trees, and rule-based systems are often easier to interpret than complex deep learning models. If interpretability is paramount, start with these.</li>
<li><strong>Regularization for Sparsity:</strong> Use L1 regularization (Lasso) in linear models or tree-based models to encourage feature selection and simplify the model. L1 regularization forces some coefficients to be exactly zero, effectively removing features from the model. The Lasso objective function is: <span class="math display">\[
\min_{\beta} ||y - X\beta||_2^2 + \lambda ||\beta||_1
\]</span> where <span class="math inline">\(\lambda\)</span> is the regularization parameter.</li>
<li><strong>Explainable Deep Learning Techniques:</strong> If deep learning is necessary, explore techniques that enhance interpretability, such as:
<ul>
<li><strong>Attention Mechanisms:</strong> These highlight which parts of the input the model is focusing on.</li>
<li><strong>Concept Activation Vectors (CAVs):</strong> Identify directions in the latent space that correspond to human-understandable concepts.</li>
<li><strong>Prototypical Part Networks (ProtoPNet):</strong> Learns to classify images based on the presence of learned prototypes.</li>
<li><strong>SHAP and LIME:</strong> Apply these post-hoc explanation methods to understand feature importance for individual predictions (more details below).</li>
</ul></li>
</ul></li>
<li><strong>Model Evaluation and Validation:</strong>
<ul>
<li><strong>Beyond Accuracy:</strong> Evaluate models not just on performance metrics (accuracy, F1-score, AUC) but also on interpretability metrics (e.g., number of features used, complexity of decision rules).</li>
<li><strong>Qualitative Evaluation:</strong> Manually review explanations for a sample of predictions to ensure they make sense and are aligned with domain knowledge. Involve domain experts in this process.</li>
<li><strong>Adversarial Example Analysis:</strong> Test the model’s robustness to adversarial examples. If small perturbations in the input significantly change the explanation, it indicates instability and potential interpretability issues.</li>
<li><strong>Fairness Assessment:</strong> Use explainability techniques to identify potential biases in the model. Are certain features disproportionately influencing predictions for specific demographic groups?</li>
</ul></li>
</ul>
<p><strong>2. Deployment Stage:</strong></p>
<ul>
<li><strong>Explanation Generation and Storage:</strong>
<ul>
<li><strong>Consistent Explanation Generation:</strong> Implement a robust and reproducible pipeline for generating explanations alongside predictions.</li>
<li><strong>Explanation Storage:</strong> Store explanations along with the corresponding predictions and input data. This allows for auditing, debugging, and retrospective analysis. Consider using a dedicated explanation store or a feature store that supports explanation metadata.</li>
<li><strong>Version Control for Explanations:</strong> Treat explanations as first-class citizens and use version control to track changes in explanation algorithms or model versions.</li>
</ul></li>
<li><strong>Monitoring and Alerting:</strong>
<ul>
<li><strong>Model Performance Monitoring:</strong> Continuously monitor standard performance metrics (accuracy, F1-score, AUC) for signs of model drift.</li>
<li><strong>Explanation Drift Monitoring:</strong> Track changes in explanation patterns over time. Are feature importances shifting? Are certain features becoming more or less influential? Use metrics like:
<ul>
<li><strong>Distribution Distance:</strong> Measure the distance between the distributions of feature importances over time (e.g., using Kullback-Leibler divergence or Jensen-Shannon divergence).</li>
<li><strong>Explanation Stability:</strong> Quantify how much the explanations change for similar input instances over time.</li>
</ul></li>
<li><strong>Anomaly Detection on Explanations:</strong> Use anomaly detection techniques to identify unusual or unexpected explanations. This could indicate data quality issues, adversarial attacks, or model degradation.</li>
<li><strong>Qualitative Feedback Loop:</strong> Establish a feedback loop with end-users to gather qualitative feedback on the usefulness and accuracy of explanations.</li>
<li><strong>Alerting:</strong> Set up alerts to notify the team when model performance degrades significantly, explanation patterns change drastically, or anomalies are detected in explanations.</li>
</ul></li>
<li><strong>Tools and Techniques:</strong>
<ul>
<li><p><strong>SHAP (SHapley Additive exPlanations):</strong> A game-theoretic approach to explain the output of any machine learning model. It assigns each feature an importance value for a particular prediction. SHAP values represent the average marginal contribution of a feature across all possible feature combinations. <span class="math display">\[
\phi_i(f, x) = \sum_{S \subseteq M \setminus \{i\}} \frac{|S|!(M - |S| - 1)!}{M!} [f(S \cup \{i\}) - f(S)]
\]</span> where:</p>
<ul>
<li><span class="math inline">\(\phi_i(f, x)\)</span> is the SHAP value for feature <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(f\)</span> is the model.</li>
<li><span class="math inline">\(x\)</span> is the input instance.</li>
<li><span class="math inline">\(M\)</span> is the set of all features.</li>
<li><span class="math inline">\(S\)</span> is a subset of features excluding feature <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(f(S)\)</span> is the prediction of the model using only the features in set <span class="math inline">\(S\)</span>.</li>
</ul></li>
<li><p><strong>LIME (Local Interpretable Model-agnostic Explanations):</strong> Approximates the model locally around a specific prediction with a simpler, interpretable model (e.g., a linear model). <span class="math display">\[
\xi(x) = \arg\min_{g \in G} \mathcal{L}(f, g, \pi_x) + \Omega(g)
\]</span> where:</p>
<ul>
<li><span class="math inline">\(x\)</span> is the instance to be explained.</li>
<li><span class="math inline">\(f\)</span> is the original model.</li>
<li><span class="math inline">\(g\)</span> is the interpretable model.</li>
<li><span class="math inline">\(G\)</span> is the space of interpretable models.</li>
<li><span class="math inline">\(\mathcal{L}\)</span> is the loss function.</li>
<li><span class="math inline">\(\pi_x\)</span> is a proximity measure defining the locality around <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(\Omega(g)\)</span> is a complexity measure for the interpretable model.</li>
</ul></li>
<li><p><strong>Integrated Gradients:</strong> Computes the integral of the gradients of the model’s output with respect to the input features along a path from a baseline input to the actual input. It attributes the change in prediction to the input features.</p></li>
<li><p><strong>Explanation Toolkits:</strong> Leverage dedicated explainability toolkits such as:</p>
<ul>
<li><strong>InterpretML:</strong> A Microsoft toolkit with various interpretability techniques.</li>
<li><strong>AI Explainability 360 (AIX360):</strong> An IBM toolkit with a comprehensive set of explanation algorithms and evaluation metrics.</li>
<li><strong>TensorBoard:</strong> TensorBoard’s “What-If Tool” allows for interactive exploration of model behavior and explanations.</li>
</ul></li>
</ul></li>
<li><strong>Human-in-the-Loop:</strong>
<ul>
<li><strong>Subject Matter Expert Review:</strong> Regularly involve subject matter experts in reviewing explanations and validating their accuracy and relevance.</li>
<li><strong>User Feedback Mechanisms:</strong> Provide users with a way to provide feedback on the explanations they receive. This could be a simple “thumbs up/thumbs down” rating or a more detailed feedback form.</li>
<li><strong>Continuous Improvement:</strong> Use the feedback gathered from subject matter experts and users to continuously improve the model, the explanation algorithms, and the overall interpretability of the system.</li>
</ul></li>
</ul>
<p><strong>Real-World Considerations:</strong></p>
<ul>
<li><strong>Computational Cost:</strong> Generating explanations can be computationally expensive, especially for complex models. Optimize the explanation pipeline to minimize latency. Consider using techniques like caching or approximation methods to reduce the cost.</li>
<li><strong>Explanation Complexity:</strong> Explanations can be complex and difficult for non-technical users to understand. Tailor the explanations to the target audience and provide different levels of detail.</li>
<li><strong>Legal and Regulatory Compliance:</strong> Ensure that the explanations comply with relevant legal and regulatory requirements, such as GDPR’s “right to explanation.”</li>
<li><strong>Trade-offs:</strong> There is often a trade-off between accuracy and interpretability. Choose the model and explanation techniques that best balance these two factors for the specific use case.</li>
</ul>
<p>By integrating interpretability and explainability into the entire ML lifecycle, we can build more trustworthy, reliable, and responsible AI systems.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a step-by-step guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with the Importance:</strong> “Interpretability and explainability are paramount in production ML for trust, fairness, and compliance. It’s not an afterthought, but integral from design to deployment.”</p></li>
<li><p><strong>Development Stage (Walkthrough):</strong> “In the development stage, I’d focus on these aspects…”</p>
<ul>
<li><strong>Problem Framing:</strong> “First, I’d explicitly define <em>why</em> explainability matters for the use case and align with stakeholders on their needs – what explanations do <em>they</em> need?” (Pause for interviewer acknowledgement).</li>
<li><strong>Data &amp; Features:</strong> “I’d prioritize inherently interpretable features and document complex transformations carefully.”</li>
<li><strong>Model Selection:</strong> “I’d start with inherently interpretable models like linear models or decision trees. If deep learning is necessary, I’d use techniques like attention mechanisms or Prototype networks to enhance explainability and compensate.” Explain why and what this means.</li>
<li><strong>Evaluation:</strong> “I’d evaluate models not just on accuracy, but also on interpretability metrics and perform qualitative reviews with domain experts. I’d test the model against adversarial examples to gauge instability.”</li>
</ul></li>
<li><p><strong>Deployment Stage (Walkthrough):</strong> “In deployment, I’d ensure…”</p>
<ul>
<li><strong>Explanation Pipeline:</strong> “A consistent, reproducible pipeline for generating explanations alongside predictions, versioning, and then storing these.”</li>
<li><strong>Monitoring:</strong> “I’d monitor model performance <em>and</em> explanation drift using metrics like distribution distance, explanation stability, and anomaly detection. I would also set alerts on any significant changes.” (Mention Kullback-Leibler or Jensen-Shannon Divergence if comfortable).</li>
<li><strong>Tools &amp; Techniques:</strong> “I would utilize tools like SHAP and LIME to generate explanations. SHAP, uses shapley values, computes the contribution of each feature, based on the average marginal contribution of a feature across all possible feature combinations. LIME, computes local, interpretable explanations.”</li>
</ul></li>
<li><p><strong>Real-World Considerations:</strong> “Several real-world issues exist…”</p>
<ul>
<li><strong>Computational Cost:</strong> “Generating explanations can be expensive. I would optimize the pipeline with caching or approximations.”</li>
<li><strong>Explanation Complexity:</strong> “Explanations must be tailored for the audience. Simple, clear, and at different levels of detail.”</li>
<li><strong>Compliance:</strong> “Ensure compliance with regulations like GDPR.”</li>
<li><strong>Trade-offs:</strong> “Remember the accuracy/interpretability trade-off; balance them based on the problem.”</li>
</ul></li>
<li><p><strong>Wrap Up:</strong> “By integrating these aspects, we can build AI systems that are not only performant but also trustworthy, reliable, and responsible.”</p></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pause and Gauge:</strong> After major sections (Development, Deployment), pause briefly to allow the interviewer to ask questions.</li>
<li><strong>Avoid Jargon Overload:</strong> Explain complex terms in plain language. For example, when mentioning SHAP, say, “SHAP, which uses game theory, calculates each feature’s contribution to the prediction.”</li>
<li><strong>Focus on Practicality:</strong> Emphasize how you would <em>actually</em> implement these techniques in a real-world setting.</li>
<li><strong>Enthusiasm:</strong> Convey your passion for building responsible and explainable AI systems.</li>
<li><strong>Mathematical Comfort (But Don’t Overdo It):</strong> If you are comfortable with the math behind SHAP or LIME, briefly mention the underlying principles, but don’t dive into excessive detail unless prompted. The key is to demonstrate understanding, not to recite equations.</li>
<li><strong>Be Honest About Limitations:</strong> Acknowledge the limitations of current explainability techniques and the ongoing research in this area. For example, “While SHAP and LIME are powerful, they can be computationally expensive and may not always provide perfectly accurate explanations.”</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>