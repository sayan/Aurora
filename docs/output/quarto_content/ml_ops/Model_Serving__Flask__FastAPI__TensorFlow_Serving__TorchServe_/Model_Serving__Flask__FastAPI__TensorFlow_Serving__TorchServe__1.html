<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>model_serving__flask__fastapi__tensorflow_serving__torchserve__1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-a37d0bf9d509de95c1ba4621f20add8c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-2.-tensorflow-serving-and-torchserve-are-popular-frameworks-for-model-deployment.-how-do-their-designs-differ-in-terms-of-supporting-their-respective-frameworks-tensorflow-and-pytorch-identify-potential-advantages-and-limitations-each-one-presents-when-handling-model-versioning-and-scaling." class="level2">
<h2 class="anchored" data-anchor-id="question-2.-tensorflow-serving-and-torchserve-are-popular-frameworks-for-model-deployment.-how-do-their-designs-differ-in-terms-of-supporting-their-respective-frameworks-tensorflow-and-pytorch-identify-potential-advantages-and-limitations-each-one-presents-when-handling-model-versioning-and-scaling.">Question: 2. TensorFlow Serving and TorchServe are popular frameworks for model deployment. How do their designs differ in terms of supporting their respective frameworks (TensorFlow and PyTorch)? Identify potential advantages and limitations each one presents when handling model versioning and scaling.</h2>
<p><strong>Best Answer</strong></p>
<p>TensorFlow Serving and TorchServe are both designed to serve machine learning models in production, but they have distinct architectures and features tailored to their respective frameworks, TensorFlow and PyTorch. Here’s a breakdown of their differences, advantages, and limitations:</p>
<p><strong>1. Design and Architecture:</strong></p>
<ul>
<li><strong>TensorFlow Serving:</strong>
<ul>
<li>Designed for TensorFlow models.</li>
<li>Written in C++ for performance.</li>
<li>Uses gRPC and RESTful APIs for serving predictions.</li>
<li>Employs a highly optimized and production-ready serving infrastructure.</li>
<li>Relies on a core concept called “Servables” - which are versioned machine learning models, vocabularies, or lookup tables.</li>
<li>Features advanced batching and dynamic model loading/unloading.</li>
</ul></li>
<li><strong>TorchServe:</strong>
<ul>
<li>Designed specifically for PyTorch models.</li>
<li>Written in Java.</li>
<li>Provides RESTful API endpoints for serving.</li>
<li>Supports custom handlers, allowing users to define pre- and post-processing logic in Python.</li>
<li>Utilizes a model archive (.mar) format for packaging models and related artifacts.</li>
<li>Designed for ease of use and seamless integration with the PyTorch ecosystem.</li>
</ul></li>
</ul>
<p><strong>2. Framework Support:</strong></p>
<ul>
<li><strong>TensorFlow Serving:</strong>
<ul>
<li>Native support for TensorFlow models (SavedModel format).</li>
<li>Requires models to be exported in a specific TensorFlow format (e.g., SavedModel).</li>
<li>Tight integration with the TensorFlow ecosystem.</li>
<li>Extensive support for various TensorFlow model types (classification, regression, object detection, etc.).</li>
</ul></li>
<li><strong>TorchServe:</strong>
<ul>
<li>Native support for PyTorch models.</li>
<li>Supports models defined using <code>torch.nn.Module</code>.</li>
<li>Simplifies the deployment of PyTorch models with minimal code changes.</li>
<li>Allows custom handlers to be written in Python for pre-processing, post-processing, and other custom logic.</li>
</ul></li>
</ul>
<p><strong>3. Model Versioning:</strong></p>
<ul>
<li><strong>TensorFlow Serving:</strong>
<ul>
<li>Excellent support for model versioning.</li>
<li>Can serve multiple versions of a model simultaneously.</li>
<li>Supports gradual rollouts and A/B testing.</li>
<li>Allows specifying which version(s) to load and serve.</li>
<li>Versioning is integral to its architecture.</li>
</ul></li>
<li><strong>TorchServe:</strong>
<ul>
<li>Supports model versioning through the model archive (.mar) format.</li>
<li>Can deploy multiple versions of a model.</li>
<li>Easier to manage model versions due to the self-contained model archive.</li>
<li>Less mature than TensorFlow Serving’s versioning capabilities, especially for complex deployment scenarios.</li>
</ul></li>
</ul>
<p><strong>4. Scaling:</strong></p>
<ul>
<li><strong>TensorFlow Serving:</strong>
<ul>
<li>Designed for scalability and high performance.</li>
<li>Supports horizontal scaling by deploying multiple instances of the serving application.</li>
<li>Can handle high request volumes with low latency.</li>
<li>Supports GPU acceleration for improved performance.</li>
<li>Integrates with container orchestration systems like Kubernetes for automated scaling.</li>
</ul></li>
<li><strong>TorchServe:</strong>
<ul>
<li>Supports scaling through multiple worker processes.</li>
<li>Can be deployed on Kubernetes for horizontal scaling.</li>
<li>GPU acceleration is supported.</li>
<li>Achieving true scale and low-latency can sometimes require more tuning and optimization compared to TensorFlow Serving.</li>
</ul></li>
</ul>
<p><strong>5. Advantages and Limitations:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 42%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>TensorFlow Serving</th>
<th>TorchServe</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Advantages</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Framework</td>
<td>Optimized for TensorFlow</td>
<td>Optimized for PyTorch</td>
</tr>
<tr class="odd">
<td>Performance</td>
<td>High performance, written in C++</td>
<td>Good performance, written in Java</td>
</tr>
<tr class="even">
<td>Maturity</td>
<td>Mature and widely adopted</td>
<td>Relatively newer, but rapidly improving</td>
</tr>
<tr class="odd">
<td>Versioning</td>
<td>Excellent versioning capabilities</td>
<td>Good versioning capabilities, simpler model packaging</td>
</tr>
<tr class="even">
<td>Scaling</td>
<td>Excellent scaling capabilities</td>
<td>Good scaling capabilities, but might require more tuning</td>
</tr>
<tr class="odd">
<td><strong>Limitations</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Framework</td>
<td>Requires TensorFlow models (SavedModel format)</td>
<td>Requires PyTorch models</td>
</tr>
<tr class="odd">
<td>Complexity</td>
<td>Can be more complex to set up initially</td>
<td>Easier to get started with</td>
</tr>
<tr class="even">
<td>Customization</td>
<td>Less flexible for custom pre/post-processing logic</td>
<td>More flexible with custom handlers written in Python</td>
</tr>
<tr class="odd">
<td>Language</td>
<td>Primarily C++</td>
<td>Java and Python for custom handlers</td>
</tr>
</tbody>
</table>
<p><strong>6. Mathematical Underpinnings (Batching and Latency Considerations):</strong></p>
<ul>
<li><p><strong>Batching:</strong> Both frameworks support batching of incoming requests to improve throughput. The key idea is to amortize the overhead of model inference across multiple requests. Let <span class="math inline">\(B\)</span> be the batch size, <span class="math inline">\(T_{inference}\)</span> be the time to perform inference on a single example, and <span class="math inline">\(T_{overhead}\)</span> be the fixed overhead per batch. The throughput (examples per second) with batching is:</p>
<p><span class="math display">\[Throughput = \frac{B}{T_{overhead} + T_{inference} \cdot B}\]</span></p>
<p>As <span class="math inline">\(B\)</span> increases, the <span class="math inline">\(T_{overhead}\)</span> becomes less significant, improving throughput. However, this comes at the cost of increased latency for individual requests.</p></li>
<li><p><strong>Latency:</strong> Latency is a critical metric for real-time applications. Let <span class="math inline">\(L\)</span> be the latency, then:</p>
<p><span class="math display">\[L = T_{overhead} + T_{inference} \cdot B\]</span></p>
<p>For low-latency requirements, smaller batch sizes are preferred. Balancing throughput and latency requires careful tuning of batch size.</p></li>
</ul>
<p><strong>7. Real-world Considerations:</strong></p>
<ul>
<li><p><strong>Choosing the Right Framework:</strong> Select the serving framework that aligns with the model framework used (TensorFlow or PyTorch). Consider factors such as performance requirements, scaling needs, and the complexity of pre- and post-processing logic.</p></li>
<li><p><strong>Containerization and Orchestration:</strong> Both frameworks can be containerized using Docker and deployed on orchestration platforms like Kubernetes. This simplifies deployment, scaling, and management.</p></li>
<li><p><strong>Monitoring:</strong> Implement robust monitoring to track key metrics like throughput, latency, error rates, and resource utilization. This helps identify and address performance bottlenecks.</p></li>
<li><p><strong>Security:</strong> Secure the serving endpoints using authentication and authorization mechanisms. Protect models from unauthorized access.</p></li>
<li><p><strong>Model Optimization:</strong> Optimize models for inference by quantizing weights, pruning connections, and using efficient operators. Tools like TensorFlow Lite and TorchScript can help with model optimization.</p></li>
<li><p><strong>Hardware Acceleration:</strong> Utilize GPUs or specialized hardware accelerators to accelerate inference. Ensure the serving framework and model are properly configured to leverage these resources.</p></li>
</ul>
<p><strong>In summary:</strong> TensorFlow Serving is a mature, high-performance serving framework optimized for TensorFlow models, with excellent versioning and scaling capabilities. TorchServe is a more recent framework designed specifically for PyTorch, offering ease of use and flexibility through custom handlers. The choice between the two depends on the specific requirements of the project and the underlying machine learning framework.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a suggested way to present this answer in an interview:</p>
<ol type="1">
<li><strong>Start with a brief overview:</strong>
<ul>
<li>“TensorFlow Serving and TorchServe are both frameworks for deploying machine learning models, but they’re tailored to TensorFlow and PyTorch, respectively. Their designs differ in several key aspects.”</li>
</ul></li>
<li><strong>Discuss the design and architecture differences:</strong>
<ul>
<li>“TensorFlow Serving, written in C++, is known for its performance and uses gRPC and REST APIs. TorchServe, written in Java, prioritizes ease of use and customizability with Python handlers.”</li>
</ul></li>
<li><strong>Explain framework support:</strong>
<ul>
<li>“TensorFlow Serving natively supports the SavedModel format, requiring models to be exported in that format. TorchServe supports PyTorch models directly and allows custom pre- and post-processing using Python handlers, which gives it greater flexibility.”</li>
</ul></li>
<li><strong>Elaborate on model versioning:</strong>
<ul>
<li>“TensorFlow Serving has robust versioning capabilities, allowing simultaneous serving of multiple versions for A/B testing. TorchServe also supports versioning using its model archive format, but it’s generally considered less mature in complex deployment scenarios.”</li>
</ul></li>
<li><strong>Address scaling aspects:</strong>
<ul>
<li>“Both frameworks support horizontal scaling, often using Kubernetes. TensorFlow Serving is designed for high scalability and performance. While TorchServe can also scale, it may require more tuning to achieve similar levels of performance.”</li>
</ul></li>
<li><strong>Summarize the advantages and limitations using the table (optional, but impressive):</strong>
<ul>
<li>“To summarize, TensorFlow Serving offers high performance and mature features, but it can be complex to set up. TorchServe is easier to use and offers more flexibility, but it might not be as performant out-of-the-box.”</li>
<li><em>You can quickly highlight key rows from the table.</em></li>
</ul></li>
<li><strong>Introduce the mathematical aspect (selectively):</strong>
<ul>
<li>“Batching is a key optimization in both frameworks. By processing multiple requests together, we can improve throughput. <quickly explain="" the="" throughput="" formula="">. However, this can increase latency. It’s all about balancing these two.”</quickly></li>
</ul></li>
<li><strong>Mention real-world considerations:</strong>
<ul>
<li>“When choosing a framework, consider the model framework, performance needs, and the complexity of your pre- and post-processing. Containerization, monitoring, and security are also important.”</li>
</ul></li>
<li><strong>Conclude with a concise summary:</strong>
<ul>
<li>“In essence, TensorFlow Serving is a robust choice for TensorFlow models where performance is paramount, while TorchServe provides a more user-friendly experience for PyTorch deployments, especially when custom logic is needed.”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace yourself:</strong> Don’t rush through the explanation. Give the interviewer time to digest the information.</li>
<li><strong>Use analogies:</strong> If possible, relate the concepts to real-world examples to make them easier to understand.</li>
<li><strong>Highlight key differences:</strong> Focus on the most important distinctions between the two frameworks.</li>
<li><strong>Quantify where possible:</strong> Use terms like “high performance” and “mature,” but provide context to support those claims.</li>
<li><strong>Acknowledge trade-offs:</strong> Show awareness of the advantages and disadvantages of each framework.</li>
<li><strong>Pause for questions:</strong> Give the interviewer opportunities to ask questions and clarify any points.</li>
<li><strong>Be prepared to dive deeper:</strong> Have a strong understanding of the underlying concepts so you can answer follow-up questions effectively.</li>
<li><strong>For the math, you might say:</strong> “Shall I briefly touch upon the mathematical factors? The throughput and latency equations help us understand how batching impacts performance.” <em>Then explain the equations.</em> Don’t assume the interviewer wants a full derivation unless they explicitly ask for it.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>