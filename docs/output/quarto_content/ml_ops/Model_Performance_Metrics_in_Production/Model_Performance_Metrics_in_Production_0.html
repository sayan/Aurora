<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>model_performance_metrics_in_production_0</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-5d3fd86dc4559d58e199c8cc4a79ed5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="question-1.-what-are-the-key-performance-metrics-commonly-used-for-classification-and-regression-models-in-production-and-what-are-the-trade-offs-associated-with-each-metric" class="level2">
<h2 class="anchored" data-anchor-id="question-1.-what-are-the-key-performance-metrics-commonly-used-for-classification-and-regression-models-in-production-and-what-are-the-trade-offs-associated-with-each-metric">Question: 1. What are the key performance metrics commonly used for classification and regression models in production, and what are the trade-offs associated with each metric?</h2>
<p><strong>Best Answer</strong></p>
<p>When deploying classification and regression models in a production environment, selecting the right performance metrics is crucial for monitoring model health, identifying potential issues, and ensuring alignment with business objectives. The choice of metric heavily depends on the specific problem, the class distribution (in classification), and the cost associated with different types of errors.</p>
<section id="classification-metrics" class="level3">
<h3 class="anchored" data-anchor-id="classification-metrics">Classification Metrics</h3>
<ol type="1">
<li><p><strong>Accuracy:</strong></p>
<ul>
<li><p><strong>Definition:</strong> The ratio of correctly classified instances to the total number of instances. <span class="math display">\[
\text{Accuracy} = \frac{\text{True Positives (TP) + True Negatives (TN)}}{\text{Total Instances (TP + TN + FP + FN)}}
\]</span> where:</p></li>
<li><p>TP (True Positives): Instances correctly predicted as positive.</p></li>
<li><p>TN (True Negatives): Instances correctly predicted as negative.</p></li>
<li><p>FP (False Positives): Instances incorrectly predicted as positive.</p></li>
<li><p>FN (False Negatives): Instances incorrectly predicted as negative.</p></li>
<li><p><strong>Pros:</strong> Easy to understand and interpret.</p></li>
<li><p><strong>Cons:</strong> Can be misleading with imbalanced datasets. For example, if 95% of the data belongs to one class, a model that always predicts that class will have 95% accuracy, but it’s not useful.</p></li>
</ul></li>
<li><p><strong>Precision:</strong></p>
<ul>
<li><strong>Definition:</strong> The ratio of correctly predicted positive instances to the total number of instances predicted as positive. It answers the question: “Of all the instances predicted as positive, how many were actually positive?” <span class="math display">\[
\text{Precision} = \frac{\text{TP}}{\text{TP + FP}}
\]</span></li>
<li><strong>Pros:</strong> Useful when the cost of false positives is high.</li>
<li><strong>Cons:</strong> Ignores false negatives. A model can achieve high precision by only predicting positive when it’s very certain, but it might miss many actual positive instances.</li>
</ul></li>
<li><p><strong>Recall (Sensitivity or True Positive Rate):</strong></p>
<ul>
<li><strong>Definition:</strong> The ratio of correctly predicted positive instances to the total number of actual positive instances. It answers the question: “Of all the actual positive instances, how many were correctly predicted?” <span class="math display">\[
\text{Recall} = \frac{\text{TP}}{\text{TP + FN}}
\]</span></li>
<li><strong>Pros:</strong> Useful when the cost of false negatives is high.</li>
<li><strong>Cons:</strong> Ignores false positives. A model can achieve high recall by predicting positive for almost every instance, but it might include many false positives.</li>
</ul></li>
<li><p><strong>F1-Score:</strong></p>
<ul>
<li><strong>Definition:</strong> The harmonic mean of precision and recall. It provides a balanced measure that considers both false positives and false negatives. <span class="math display">\[
\text{F1-Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision + Recall}}
\]</span></li>
<li><strong>Pros:</strong> Balances precision and recall, making it useful when there’s an uneven class distribution.</li>
<li><strong>Cons:</strong> Doesn’t perform well if one seeks to optimize for precision or recall at the expense of the other. An F-beta score can be used to weigh precision vs recall more heavily. <span class="math display">\[
F_\beta = (1 + \beta^2) \cdot \frac{\text{precision} \cdot \text{recall}}{(\beta^2 \cdot \text{precision}) + \text{recall}}
\]</span> If you set <span class="math inline">\(\beta &lt; 1\)</span>, you weigh precision higher, and if you set <span class="math inline">\(\beta &gt; 1\)</span>, you weigh recall higher. When <span class="math inline">\(\beta = 1\)</span> the F-beta score is equal to the F1-score.</li>
</ul></li>
<li><p><strong>ROC AUC (Area Under the Receiver Operating Characteristic Curve):</strong></p>
<ul>
<li><p><strong>Definition:</strong> The ROC curve plots the true positive rate (recall) against the false positive rate at various threshold settings. AUC measures the area under this curve. It represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. <span class="math display">\[
\text{TPR} = \frac{\text{TP}}{\text{TP + FN}}
\]</span> <span class="math display">\[
\text{FPR} = \frac{\text{FP}}{\text{FP + TN}}
\]</span></p></li>
<li><p><strong>Pros:</strong> Provides a good measure of the model’s ability to discriminate between classes, regardless of class distribution.</p></li>
<li><p><strong>Cons:</strong> Can be less interpretable than other metrics. Sensitive to imbalances in the dataset. Can sometimes give an optimistic view of model performance if there is a region of the ROC space that is not relevant.</p></li>
</ul></li>
<li><p><strong>Log Loss (Cross-Entropy Loss):</strong></p>
<ul>
<li><strong>Definition:</strong> Measures the performance of a classification model where the prediction input is a probability value between 0 and 1. It quantifies the uncertainty of the predicted probabilities. <span class="math display">\[
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
\]</span> Where:
<ul>
<li><span class="math inline">\(N\)</span> is the number of data points.</li>
<li><span class="math inline">\(y_i\)</span> is the actual label (0 or 1) for the <span class="math inline">\(i\)</span>-th data point.</li>
<li><span class="math inline">\(p_i\)</span> is the predicted probability of the label being 1 for the <span class="math inline">\(i\)</span>-th data point.</li>
</ul></li>
<li><strong>Pros:</strong> Penalizes confident and wrong predictions heavily. Good for optimizing probabilistic classifiers.</li>
<li><strong>Cons:</strong> Not easily interpretable. Requires well-calibrated probability estimates.</li>
</ul></li>
</ol>
</section>
<section id="regression-metrics" class="level3">
<h3 class="anchored" data-anchor-id="regression-metrics">Regression Metrics</h3>
<ol type="1">
<li><p><strong>Mean Squared Error (MSE):</strong></p>
<ul>
<li><strong>Definition:</strong> The average of the squared differences between the predicted and actual values. <span class="math display">\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]</span> where:
<ul>
<li><span class="math inline">\(y_i\)</span> is the actual value.</li>
<li><span class="math inline">\(\hat{y}_i\)</span> is the predicted value.</li>
<li><span class="math inline">\(N\)</span> is the number of data points.</li>
</ul></li>
<li><strong>Pros:</strong> Easy to compute and mathematically tractable. Penalizes larger errors more heavily.</li>
<li><strong>Cons:</strong> Sensitive to outliers due to the squared term. Not on the same scale as the original data.</li>
</ul></li>
<li><p><strong>Root Mean Squared Error (RMSE):</strong></p>
<ul>
<li><strong>Definition:</strong> The square root of the MSE. <span class="math display">\[
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2}
\]</span></li>
<li><strong>Pros:</strong> Same advantages as MSE, but on the same scale as the original data, making it easier to interpret. Still penalizes larger errors more heavily.</li>
<li><strong>Cons:</strong> Also sensitive to outliers.</li>
</ul></li>
<li><p><strong>Mean Absolute Error (MAE):</strong></p>
<ul>
<li><strong>Definition:</strong> The average of the absolute differences between the predicted and actual values. <span class="math display">\[
\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|
\]</span></li>
<li><strong>Pros:</strong> Robust to outliers. Easy to understand and interpret.</li>
<li><strong>Cons:</strong> Doesn’t penalize large errors as heavily as MSE/RMSE. Can be less mathematically tractable than MSE.</li>
</ul></li>
<li><p><strong>R-squared (Coefficient of Determination):</strong></p>
<ul>
<li><strong>Definition:</strong> Represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Ranges from 0 to 1 (or can be negative if the model is very poor). <span class="math display">\[
R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}
\]</span> where:
<ul>
<li><span class="math inline">\(\bar{y}\)</span> is the mean of the actual values.</li>
</ul></li>
<li><strong>Pros:</strong> Provides a measure of how well the model fits the data. Easy to interpret.</li>
<li><strong>Cons:</strong> Can be misleading if the model is overfitting. Doesn’t indicate whether the model is biased. Can increase artificially with the addition of irrelevant features.</li>
</ul></li>
<li><p><strong>Mean Absolute Percentage Error (MAPE):</strong></p>
<ul>
<li><strong>Definition:</strong> The average percentage difference between the predicted and actual values. <span class="math display">\[
\text{MAPE} = \frac{100\%}{N} \sum_{i=1}^{N} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\]</span></li>
<li><strong>Pros:</strong> Easy to understand and interpret as a percentage error. Scale-independent.</li>
<li><strong>Cons:</strong> Can be infinite or undefined if any actual value is zero. Sensitive to small actual values, leading to disproportionately large percentage errors. Can be biased; tends to penalize under-forecasting more than over-forecasting.</li>
</ul></li>
</ol>
</section>
<section id="trade-offs-and-considerations" class="level3">
<h3 class="anchored" data-anchor-id="trade-offs-and-considerations">Trade-offs and Considerations</h3>
<ul>
<li><p><strong>Business Objectives:</strong> The most important consideration is aligning the metric with the business goals. For example, in fraud detection, minimizing false negatives (i.e., catching as much fraud as possible) might be more important than minimizing false positives. In medical diagnosis, recall is generally favored over precision to avoid missing any positive cases of a disease.</p></li>
<li><p><strong>Class Imbalance:</strong> In imbalanced datasets, accuracy can be misleading. Precision, recall, F1-score, and ROC AUC are generally better choices. Consider using techniques like class weighting or oversampling/undersampling to address the imbalance.</p></li>
<li><p><strong>Outliers:</strong> MSE and RMSE are sensitive to outliers. MAE is more robust. Consider using data preprocessing techniques to handle outliers.</p></li>
<li><p><strong>Interpretability:</strong> Some metrics (e.g., accuracy, MAE, R-squared) are easier to understand than others (e.g., log loss, ROC AUC). If interpretability is important, choose metrics that can be easily explained to stakeholders.</p></li>
<li><p><strong>Threshold Selection:</strong> For classification, the choice of the classification threshold affects precision and recall. ROC AUC is threshold-independent, but you still need to choose a threshold for making predictions in production.</p></li>
<li><p><strong>Model Complexity:</strong> Overly complex models might achieve high performance on training data but generalize poorly to new data. Use techniques like cross-validation to estimate the model’s performance on unseen data.</p></li>
<li><p><strong>Data Distribution:</strong> Ensure that the data used for evaluation is representative of the data the model will encounter in production. Monitor for data drift, where the distribution of the input data changes over time, which can degrade model performance.</p></li>
<li><p><strong>Error Analysis:</strong> Go beyond overall metrics and perform detailed error analysis to understand <em>why</em> the model is making mistakes. This can help you identify areas for improvement. For example, Confusion Matrices are useful to classify the ways in which the model can err.</p></li>
</ul>
<p>By carefully considering these factors and selecting the most appropriate metrics, you can ensure that your models are performing well and delivering value in a production environment. Continuous monitoring of these metrics is crucial for maintaining model health and addressing any issues that may arise.</p>
<p><strong>How to Narrate</strong></p>
<p>Here’s a guide on how to articulate this answer in an interview:</p>
<ol type="1">
<li><p><strong>Start with a High-Level Overview:</strong></p>
<ul>
<li>“Choosing the right performance metrics is critical for deploying models to production. The choice depends on several factors, including the problem type (classification vs.&nbsp;regression), class distribution, and, most importantly, the business objectives.”</li>
</ul></li>
<li><p><strong>Classification Metrics:</strong></p>
<ul>
<li>“For classification problems, common metrics include accuracy, precision, recall, F1-score, ROC AUC, and log loss. I’ll explain each one and discuss their trade-offs.”</li>
<li><strong>Accuracy:</strong> “Accuracy is straightforward – the percentage of correct predictions. However, it’s misleading for imbalanced datasets…” (Explain with the 95% example.)</li>
<li><strong>Precision and Recall:</strong> “Precision focuses on minimizing false positives, while recall focuses on minimizing false negatives. These are often used together…” (Explain the formulas). “For example, in fraud detection, we might prioritize recall to catch as much fraudulent activity as possible.”</li>
<li><strong>F1-Score:</strong> “The F1-score is the harmonic mean of precision and recall, providing a balanced measure. It’s useful when you want to balance false positives and false negatives.”</li>
<li><strong>ROC AUC:</strong> “ROC AUC measures the model’s ability to discriminate between classes across different thresholds. It’s less sensitive to class imbalance than accuracy.”</li>
<li><strong>Log Loss:</strong> “Log Loss measures the uncertainty of your model’s probabilities. Lower values represent better calibrated predictions.”</li>
<li><strong>Pause for Questions:</strong> “Before I move on to regression metrics, do you have any questions about these classification metrics?”</li>
</ul></li>
<li><p><strong>Regression Metrics:</strong></p>
<ul>
<li>“For regression problems, common metrics include MSE, RMSE, MAE, R-squared, and MAPE.”</li>
<li><strong>MSE and RMSE:</strong> “MSE calculates the average squared error. RMSE is just the square root of MSE, making it more interpretable. Both penalize large errors heavily but are sensitive to outliers.”</li>
<li><strong>MAE:</strong> “MAE calculates the average absolute error. It’s more robust to outliers than MSE/RMSE but doesn’t penalize large errors as much.”</li>
<li><strong>R-squared:</strong> “R-squared represents the proportion of variance explained by the model. It’s easy to interpret but can be misleading if the model is overfitting.”</li>
<li><strong>MAPE:</strong> “MAPE expresses error as a percentage, which is intuitive. However, it’s undefined if actual values are zero and can be skewed by small values.”</li>
</ul></li>
<li><p><strong>Trade-offs and Considerations:</strong></p>
<ul>
<li>“The choice of metric involves trade-offs. You need to consider the business objectives, class imbalance, the impact of outliers, and the interpretability of the metric.”</li>
<li>“For example, if minimizing false negatives is critical, you’d prioritize recall. If you’re dealing with outliers, MAE might be a better choice than MSE.”</li>
<li>“It’s also important to monitor for data drift in production, where the distribution of the input data changes over time, which can affect model performance.”</li>
<li>“Finally, error analysis is key. Understanding <em>why</em> the model is making mistakes can help you identify areas for improvement.”</li>
</ul></li>
<li><p><strong>Conclude and Encourage Questions:</strong></p>
<ul>
<li>“So, in summary, selecting the right performance metrics is a nuanced process that depends on the specific problem and business goals. Continuous monitoring is essential to ensure model health in production. Do you have any questions about any of these points?”</li>
</ul></li>
</ol>
<p><strong>Communication Tips:</strong></p>
<ul>
<li><strong>Pace Yourself:</strong> Don’t rush through the explanation. Give the interviewer time to process the information.</li>
<li><strong>Use Visual Aids (if available):</strong> If you’re in a virtual interview, consider sharing your screen and using a whiteboard tool to draw diagrams or write down formulas.</li>
<li><strong>Check for Understanding:</strong> Periodically ask the interviewer if they have any questions or if they’d like you to elaborate on a particular point.</li>
<li><strong>Connect to Real-World Examples:</strong> Use real-world examples to illustrate the importance of different metrics and their trade-offs.</li>
<li><strong>Show Enthusiasm:</strong> Your passion for the topic will make the answer more engaging and memorable.</li>
<li><strong>Don’t Be Afraid to Say “It Depends”:</strong> The best answer is often, “It depends on the specific context.” This shows that you’re a thoughtful and experienced data scientist.</li>
<li><strong>For Equations:</strong> When presenting equations, explain each symbol and its meaning. Walk through the logic of the equation step by step. Avoid simply reciting the equation without context.</li>
</ul>
<p>By following these guidelines, you can effectively demonstrate your expertise in model performance metrics and leave a lasting impression on the interviewer.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>